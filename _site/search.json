[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "July Fourth by Grandma Moses\n\n\nI am not a data scientist. Others have coined good terms like “Data Nerd” and “Citizen Data Scientist.” I’ll coin another: “Outsider Data Scientist.” I would style myself in the likeness of an “outsider” artist, Grandma Moses. She was an American artist who didn’t pick up a brush between childhood and old age, and had no formal training. The works she produced would never be mistaken for the old masters’ but they had a certain charm. Perhaps I might strive for that. I am also getting on in years.\nI play around with R for fun. I enjoy thinking up ways to present complex information in a simple, compelling way. I attended an R conference where an axiom was presented by Dave Robertson that the value of information “still” on your computer is approximately zero and the value of information out in the world is infinitely more. Even if it is small that’s infinitely more than zero, right? That emboldened me to put stuff “out there.” Perhaps someone else might find it interesting. At a minimum this blog forces more rigor in my own thinking.\n“Outsider” is a bit of a misnomer. I’m sure Grandma Moses saw an another painting or two in her life. Painting was pretty mature before Ms. Moses picked up her brush, but this whole data science thing has exploded in the last few years. It is very exciting to even be on the periphery of the event horizon. I am indebted to the R community for all the examples they have shared through R-Bloggers, Stack Overflow and Twitter. The tools provided by Posit (formerly RStudio) are the bomb!\nThis is a personal side project in no way associated with any organization I am affiliated with. My opinions here are mine alone and any data I present here is neither proprietary nor is it warranted to be correct or accurate. Nothing I say here should be construed as investment advice.\nI used to work at OppenheimerFunds Inc. before it was acquired by Invesco, first as a portfolio manager of global macro fixed income and ultimately as CEO. I am the former board chair at the National Museum of Mathematics, MoMath.org, where I rubbed shoulders (though not in creepy way) with people who are really, really smart. Visit the museum when you are in NYC. We are making math cool! Finally, I am on the board of “Rock the Street, Wall Street” which brings financial literacy programs into high school classrooms to encourage girls to become interested in finance. We need more diversity in finance.\n\nArt Steinmetz\n\nemail: apsteinmetz@yahoo.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "outsider_blog",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nLive Fast, Die Young, Stay Pretty?\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2018\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nPlumbing the Depths of My Soul (in Facebook)\n\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2017\n\n\nArt Steinmetz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence.html",
    "href": "posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence.html",
    "title": "Plumbing the Depths of My Soul (in Facebook)",
    "section": "",
    "text": "First post! Let’s start out nice and easy. No big data machine learning or heavy stats. This post will merely explore the depths of my soul through a meta-analysis of every one of my Facebook posts. Meta-navel gazing, if you will.\nPerhaps you are not all that interested in the plumbing the depths of my soul. Still, you may be interested in seeing how you can do an analyis of your own Facebook life in the comfort of your own home. If so, read on!\nWe will (lightly) cover web scraping, sentiment analysis, tests of significance and visualize it with a generous helping of ggplot. Note I use the tidyverse/dplyr vernacular. This is fast becoming a dialect of R. I quite like it but its syntax is different than traditional R. It produces sometimes slower, but much more readable, code. Basically, you “pipe” data tables through action verbs using the pipe operator (“%>%”).\nLet’s go do some outsider data science!\nStart by loading needed packages.\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(wordcloud)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggplot2)\nlibrary(zoo)\nlibrary(reshape2)\nlibrary(lubridate)\n\n\n#make explicit so kableExtra doesn't complain later\noptions(knitr.table.format = \"html\") \n\n\nFetch and clean all the words in my Facebook posts\nFacebook lets you download a log of all your activity at https://Facebook.com/settings. Look toward the bottom of the page for the download link. You will get an email with a link to a zipped set of html files. These are what I’ll be using for the analysis.\nFirst let’s get all my comments since the dawn of my Facebook existence.\n\npath='data/'\nraw_timeline<- read_html(paste0(path,\"timeline.htm\"),encoding=\"UTC-8\")\n\nNow that we have the raw data we need to extract the just the text of the comments. Visually inspecting the raw html file reveals that all of the comments I wrote have the tag <div class=\"comment\"> so I construct an xpath selector to grab those nodes then get the text in them. This is what the raw html looks like:\n</p><p><div class=\"meta\">Thursday, November 16, 2017 at 1:17pm EST</div> <div class=\"comment\">I’m sure you are all thinking “what does this mean for Al Franken?”</div> </p><p> <div class=\"meta\">Thursday, November 16, 2017 at 10:44am EST</div> Art Steinmetz shared a link. </p><p>\nThe challenge here is that we want to get the date also which appears BEFORE the comment and has the tag <div class=\"meta\">. Unfortunately, as we see above, merely sharing a link generates this tag without any comment or a different tag class so there are more meta classes than comment classes. Facebook should create a separate XML record for each log activity, but they don’t.\nThe code below seems inelegant to me. for loops in R are non-idiomatic and indicate somebody was steeped in a non vectorized language (like me). I tried without success to craft an xpath expression that would walk backwards when it sees a comment class to get the date. In the end I resorted to the devil I know, a loop.\n\ntimeline_post_nodes <- raw_timeline %>% \n  html_nodes(xpath=\"//div[@class ='comment'] | //div[@class='meta']\")\n\ntimeline_posts1<-NULL\n#the bit below is the slowest part of our project. \n#If you post multiple times a day over years it could take a while.\nfor (n in 1:length(timeline_post_nodes)){\n  if ( html_attr(timeline_post_nodes[n],\"class\")==\"comment\"){\n    post= html_text(timeline_post_nodes[n])\n    date= html_text(timeline_post_nodes[n-1])\n    timeline_posts1<-timeline_posts1 %>% bind_rows(tibble(date,post))\n  }\n}\n\nThe time stamps we extracted are just character strings with no quantitative meaning. Let’s convert the dates in the form of “Saturday November 18 2017 11:12am EST” to a day of the week and a POSIX date/time format that other R functions will understand. First we pull out the day of the week using the comma as a separator but this also separates the month and day from the year, which we don’t want, so we put those back together.\nThis begs the question of whether we should have used a tricker “regular expression” to accomplish this in one step. RegExes are a dark art that I have a lot of admiration for, even if I am a rank neophyte. In this exercise I didn’t think it was worth the time to figure out a “proper” solution when a “simple” one sufficed. Other times I like the puzzle challenge of coming up with a powerful RegEx. There are web sites that are a great help in building them. Try http://regex101.com, for one.\nWith a good date string in hand we can use parse_date() to convert it. Notice the format string we use to accomplish this.\n\ntimeline_posts<-timeline_posts1 %>%\n  mutate(date=sub(\"at \",\"\",date)) %>%\n  separate(date,c(\"doy\",\"date\",\"yeartime\"),sep=\", \") %>%\n  transmute(doy=doy,date=paste(date,yeartime),post=post)\n\n# Now that we've pulled out the day of the week, let's make sure they show in order in plots\n# by making doy and ordered factor.\nday_order<-c(\"Monday\",\"Tuesday\",\"Wednesday\",\n            \"Thursday\",\"Friday\",\"Saturday\", \n            \"Sunday\")\n\ntimeline_posts$doy<-factor(timeline_posts$doy,levels = day_order)\n\ntimeline_posts<-timeline_posts %>% \n  mutate(date = str_remove(date,\" EST| EDT\")) |> \n  mutate(date = parse_datetime(date,\n                               format=\"%B %d %Y %I:%M%p\",\n                               locale = locale(tz = \"US/Eastern\")))\n\nkable(head(timeline_posts[1:2,])) %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    doy \n    date \n    post \n  \n \n\n  \n    Saturday \n    2017-11-18 11:12:00 \n    I feel cheated. When I read the fine print I see these guys haven't won the \"Uniformity of Granulation\" award since 1894.  I want the oatmeal that won last year! \n  \n  \n    Saturday \n    2017-11-18 10:41:00 \n    I had a chance to visit Shenzhen this year.  The hardware scene is reminiscent of Blade Runner as you'll see.  This guy prowls the markets to make his own iPhone from scratch. \n  \n\n\n\n\n\nWe now have over 2000 text strings, each representing one post. Since we are working at the word level we need to break up each post into its constituent words.\nFor much of this analysis I am following the example shown at https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html.\nThe unnest_tokens function from the ‘tidytext’ package lets us convert a dataframe with a text column to be one-word-per-row dataframe. How many words are there?\n\nmy_post_words<-  timeline_posts %>%\n  unnest_tokens(word, post)\n\nnrow(my_post_words)\n\n[1] 51347\n\n\nSo we have over fifty thousand words. A lot of them are going to be uninteresting. Although, given that Facebook posts are an excercise in narcissim, you might say all of them are uninteresting to anybody but me.\nAnyway, lets press on. We can use the stop_words data set included with tidytext to to strip out the superfluous words. Note this includes words like ‘accordingly’ which convey little meaning but might be useful in revealing idiosyncratic writting patterns, much like people punctuate their speech with vocal pauses like “like” and “right.” How many words are left after that?\n\ndata(\"stop_words\")\ncleaned_post_words <- my_post_words %>%\n  anti_join(stop_words,by='word')\n\nnrow(cleaned_post_words)\n\n[1] 22465\n\n\n\n\nLook at the most common words\nSo now our data set is clean and tidy. Let’s answer some questions. What are the most common words I use in posts.\n\npopular_words<-cleaned_post_words %>%\n  count(word, sort = TRUE)\nkable(popular_words[1:10,]) %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    word \n    n \n  \n \n\n  \n    day \n    111 \n  \n  \n    â \n    101 \n  \n  \n    people \n    97 \n  \n  \n    time \n    92 \n  \n  \n    kids \n    84 \n  \n  \n    love \n    69 \n  \n  \n    carrie \n    60 \n  \n  \n    guy \n    59 \n  \n  \n    friends \n    56 \n  \n  \n    art \n    48 \n  \n\n\n\n\n\nI don’t know where that “a-hat” character comes from but let’s get rid of it.\n\ncleaned_post_words<- cleaned_post_words%>% \n  mutate(word=str_replace(word,\"â\",\"\")) %>% \n  filter(str_length(word)>0)\npopular_words<-cleaned_post_words %>%\n  count(word, sort = TRUE)\n\nAfter we strip out stop words we have less then 10,000 “real” words left.\nGood to see that my wife’s name is one of my most used words. “Kids,” “friends,” and “love” are no surprise. What’s a good way to visualize this? Word cloud!\nI love word clouds! We can easily display the most used words this way using the wordcloud package.\n\n# We love wordclouds!\n#scalefactor magnifies differences for wordcloud\nscaleFactor=1.3\nmaxWords = 200\n\n\nwordcloud(words = popular_words$word, \n          freq = popular_words$n^scaleFactor,\n          max.words=maxWords, \n          random.order=FALSE,rot.per=0.35, \n          colors=brewer.pal(8, \"Dark2\"),\n          scale = c(3,.3))\n\n\n\n\nI mentioned “Obama” about as often as I mentioned “beer.”\n\n\nDo some sentiment analysis\nI used to be jerk. But, given my age, I am entitled to call myself a curmudgeon instead. That sounds nicer somehow, and excuses my negative reaction to everything. However, given how internet discourse easily sinks into a tit-for-tat of profane hatred, I try to go against type, accentuate the positive and say nothing if I can’t say something nice. That’s the idea. How does my sour nature interact with my better intentions? We can use sentiment analysis to find out. The tidytext package also has serveral lexicons with thousands of words coded by their sentiment. Refer to http://tidytextmining.com for an excellent tutorial on this. Obviously, the isolated word approach has limitations. Context matters and by taking one word at a time we don’t capture that. So, with that caveat, how much of a downer am I?\nFirst, let’s look at the sentiment of my posts on a binary basis. Is the word positive or negative? The “bing” lexicon scores thousands of words that way. Obviously, not all the words we used are in the data set. About a third are, though.\n\ncleaned_post_words %>% \n  inner_join(get_sentiments('bing'),by=\"word\") %>% \n  group_by(sentiment) %>% \n  summarize(count=n()) %>% \n  kable() %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    sentiment \n    count \n  \n \n\n  \n    negative \n    1678 \n  \n  \n    positive \n    1412 \n  \n\n\n\n\n\nWell, then. So I am a downer, on a net basis, but not terribly so.\nWe can make this into a word cloud, too! Here are the words I used divided by sentiment.\n\ncleaned_post_words %>%\n  inner_join(get_sentiments('bing'),by=\"word\") %>%\n  count(word, sentiment, sort = TRUE) %>%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %>%\n  comparison.cloud(colors = c(\"#F8766D\", \"#00BFC4\"),\n                   max.words = 100)\n\n\n\n\nWait a minute! “Trump” is scored as a positive sentiment word! Is this a hidden statement by the author of the lexicon?! Doubtful. It’s “trump,” as in “spades trumps clubs,” not as a proper name. And why is “funny” a negative word? I guess it’s “funny strange,” not “funny ha-ha.” It shows the limitations of this kind of thing.\nA different lexicon scores each word’s sentiment on a scale of minus to positive five. This seems pretty subjective to me but has the benefit of letting us add up the numbers to get a net score. What is my sentiment score over all words I’ve ever written on Facebook (not all, the log doesn’t include comments to other’s posts).\n\nsentiment_score<-cleaned_post_words %>% \n  inner_join(get_sentiments('afinn'),by=\"word\") %>% \n  pull(value) %>% \n  mean()\nsentiment_score\n\n[1] 0.1610233\n\n\nWell, this draws an slightly different conclusion. The net score of my sentiment is +0.16 out of range of -5 to +5. Just barely happy. While I may use more negative than positive words, my positive words are more positive. I suspect the word “love” which we already saw is frequently used (though it is “only” a “3”) accounts for this.\nWhat were my most negative words?\n\nword_scores<-cleaned_post_words %>% \n  inner_join(get_sentiments('afinn'),by=\"word\") %>% \n  group_by(word,value) %>% summarise(count=n())\n\n`summarise()` has grouped output by 'word'. You can override using the\n`.groups` argument.\n\n\n\nword_scores %>%\n        arrange((value)) %>%\n        ungroup() %>%\n        .[1:10,] %>%\n        kable() %>%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\nThis is a family blog so I comment out the code that displays the worst words. Suffice it to say, they are the usual curse words and forms thereof. I am cringing right now. Did I say those things? Yes, well not often, at least, once or twice is typical for each.\nAs I mentioned above, the limitation of this analysis is that it lacks context. For instance, did I call someone a slut? I was briefly horrified when I saw that word. Here is the word in context from 2014: “Less slut-shaming and more perp-jailing.”\nAll these negative words carry more power for me, an old-geezer, than for kids today (kids today!) who let f-bombs roll off their tongues with uncomfortable (to me) ease. Get off my lawn!\nWhat were my positive words?\n\nword_scores %>% arrange(desc(value)) %>% \n  ungroup() %>%\n  .[1:10,] %>% \n  kable() %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    word \n    value \n    count \n  \n \n\n  \n    breathtaking \n    5 \n    1 \n  \n  \n    outstanding \n    5 \n    1 \n  \n  \n    thrilled \n    5 \n    3 \n  \n  \n    amazing \n    4 \n    19 \n  \n  \n    awesome \n    4 \n    19 \n  \n  \n    brilliant \n    4 \n    5 \n  \n  \n    fabulous \n    4 \n    1 \n  \n  \n    fantastic \n    4 \n    2 \n  \n  \n    fun \n    4 \n    45 \n  \n  \n    funnier \n    4 \n    1 \n  \n\n\n\n\n\nWhew! I feel better now. Everything is awesome!\nDid I get happier or sadder over time? We’ll answer that question in a minute.\n\n\nTime Patterns\nThe foregoing analysis just includes posts on my timeline where I made a comment. If we want to know things like when I’m active on Facebook we need to look at all activity. Again, Facebook doesn’t separately tag different activities. Let’s go back over all the activity to pull out just the timestamps, but all of them this time.\n\nactivity_times <- tibble(date = raw_timeline %>% \n                             html_nodes(xpath=\"//div[@class='meta']\") %>% \n                             html_text()\n                              ) %>%\n  mutate(date=sub(\"at \",\"\",date)) %>%\n  separate(date,c(\"doy\",\"date\",\"yeartime\"),sep=\", \") %>%\n  transmute(doy=doy,date=paste(date,yeartime)) %>%\n  mutate(date = str_remove(date,\" EST| EDT\")) |> \n  mutate(date = parse_datetime(date,\n                               format=\"%B %d %Y %I:%M%p\",\n                               locale = locale(tz = \"US/Eastern\")))\n\nactivity_times$doy<-factor(activity_times$doy,levels = day_order)\n\nLet’s ask a couple questions. What day of the week am I most active on Facebook?\n\n#make sure days of week are in sequential order. Monday first\n\nactivity_times %>% ggplot(aes(doy))+geom_bar()+\n  labs(title='Facebook Activity', x='Weekday',y='Posts')\n\n\n\n\nMonday stands out. I didn’t realize this. Perhaps I come to work Monday morning and catch up with the news which prompts me to post.\nAm I more cranky on different days?\n\n#cleaned_post_words$doy<-factor(cleaned_post_words$doy,levels = day_order)\n\nword_scores_by_weekday<-cleaned_post_words %>% \n  inner_join(get_sentiments('afinn'),by=\"word\") %>% \n  group_by(doy)\n\nword_scores_by_weekday %>%\n  summarise(mood=mean(value)) %>% \n  ggplot(aes(x=doy,y=mood))+geom_col()+labs(x=\"Weekday\",y=\"Mood Score\")\n\n\n\n\nThis is interesting! I am in a relatively good mood on Monday! It’s the middle of the week when I tend to use more negative words. Then I pick up going into the weekend.\nRemember though, these are numbers of small magnitude. Are the variations statistically significant? Let’s compare Tuesday to Sunday and (which have the most extreme differences). First visually then with a t-test to see if the differences are significant. For our hypothesis we assume the the true difference in the average mood on Monday is no different than the average mood on Sunday. Based on the differences we see, can we reject this hypothesis?\n\nsunday_moods<-word_scores_by_weekday %>% \n  filter(doy==\"Sunday\") %>% \n  group_by(doy,date) %>% \n  summarise(mood=mean(value)) %>% \n  select(doy,mood)\n\n`summarise()` has grouped output by 'doy'. You can override using the `.groups`\nargument.\n\ntuesday_moods<-word_scores_by_weekday %>% \n  filter(doy==\"Tuesday\") %>% \n  group_by(doy,date) %>% \n  summarise(mood=mean(value)) %>% \n  select(doy,mood)\n\n`summarise()` has grouped output by 'doy'. You can override using the `.groups`\nargument.\n\nbind_rows(tuesday_moods,sunday_moods) %>% ggplot(aes(mood,fill=doy))+geom_density(alpha=0.7)\n\n\n\n\n\nt.test(tuesday_moods$mood,sunday_moods$mood)\n\n\n    Welch Two Sample t-test\n\ndata:  tuesday_moods$mood and sunday_moods$mood\nt = -0.97824, df = 332.11, p-value = 0.3287\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.6292549  0.2112694\nsample estimates:\n mean of x  mean of y \n0.06088435 0.26987711 \n\n\nRats! It looks like our “interesting” observation is not interesting. The p-value of 0.32 is below 2, so we can’t reject our hypothesis. The difference in mean sentiment for Sunday and Tuesday would have to be beyond the confidence interval to give us acceptable certainty that I am most cranky on Tuesday.\nWe can’t get too excited by the density plot, either. My posts are bi-modally distributed but, given the relatively short length of my posts, chances are I use just one sentiment-loaded word and that skews the distribution. Again, small sample sizes are the problem. Pretty picture, though!\nWhat times am I most active?\n\nhours<-cleaned_post_words %>% mutate(hour=hour(date))\n\nhours %>% ggplot(aes(hour))+geom_bar()+\n  labs(title='Facebook Activity', x='Hour',y='Posts')\n\n\n\n\n#Trends over Time\nIs there any trend to my Facebook activity over time? Let’s bucket the posts by month and look for a pattern.\n\nactivity_times <- activity_times %>% \n  #filter dates before I joined as bad data\n  filter(date>as.Date(\"2008-01-01\")) %>% \n  mutate(month=as.yearmon(date))\n\nactivity_times %>% \n  ggplot(aes(as.Date(month))) + geom_bar() +labs(x=\"Month\",y=\"Posts\")\n\n\n\n\nWhat’s up with the beginning of 2013 and December in 2015? Looking at the raw activity log I see that I briefly let Spotify tell you what I was listening to via Facebook. That generated a lot of activity. I turned it off after a couple weeks. In late 2016 around the election we also see an uptick in activity. Otherwise there have been pretty mild ebbs and flows, averaging about 30 posts per month.\n\nactivity_times %>% \n  group_by(month) %>% \n  summarise(n=n()) %>% \n  summarise(avg_per_month=mean(n)) %>% \n  kable() %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    avg_per_month \n  \n \n\n  \n    29.42105 \n  \n\n\n\n\n\n\n\nDoes my mood change over time?\nWe can repeat the sentiment analysis from above but bucket it by month.\n\nword_scores_by_month<-cleaned_post_words %>% \n  inner_join(get_sentiments('afinn'),by=\"word\") %>% \n  select(date, word,value) %>% \n  mutate(yearmonth=as.yearmon(date)) %>% \n  group_by(yearmonth) %>% summarise(mood=sum(value))\n\nword_scores_by_month %>%  \n  ggplot(aes(x=as.Date(yearmonth),y=mood))+geom_col()+geom_smooth()+labs(x=\"Month\",y=\"Mood Score\")\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\nA trend is not very evident. Month-to-month variation is very high. Is it that we don’t have a good sample size or do my moods swing wildly? The most extreme gyration is around the 2016 presidential election. Optimism followed by despair? Perhaps.\n\n\nPolitics Rears Its Ugly Head\nI try to avoid too much talk about politics on Facebook but, like most of us, it was tough in an election year. This gives us an opportunity to dig into a specific topic within the posting corpus.\nLet’s start by seeing how often I mentioned politicians names.\n\npoliticos=c(\"obama\",\"trump\",\"hillary\",\"clinton\",\"johnson\",\"kasich\",\"bush\",\"sanders\",\"romney\",\"mccain\",\"palin\")\n\ngg<-cleaned_post_words %>% \n  filter(word %in% politicos) %>%\n  mutate(word=str_to_title(word)) %>% \n  count(word, sort = TRUE) %>% \n  \n  ggplot(aes(x=reorder(word,n),y=n))+geom_col()+coord_flip()+labs(y=\"Mentions\",x=\"\")\n\ngg\n\n\n\n\n“Johnson”” is Gary Johnson, in case you forgot. Unlike the the “lamestream media,” I gave much more attention to the Libertarian candidate in my posts. Oddly, that didn’t help his success in the election.\n“Hillary”” is the only first name in the list. Using a woman’s first name when equally familiar men are referred to with their last name is often sexist. In my defense, during the election it was important to distinguish between her and husband Bill so that’s why I used “Hillary.” We are not on a first name basis. On the other hand, “Bill” is a common enough name (and noun) so it’s likely that many posts using it don’t refer to the politician. Just to get an idea let’s look at the words bracketing “bill”:\n\ncleaned_post_words %>% \n  mutate(phrase=paste(lag(word),word,lead(word))) %>% \n  filter(word=='bill') %>% \n  select(date,phrase) %>% \n  kable() %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    date \n    phrase \n  \n \n\n  \n    2017-09-20 08:53:00 \n    morning bill protection \n  \n  \n    2017-07-23 12:27:00 \n    water bill extra \n  \n  \n    2017-03-11 14:15:00 \n    stitch bill doctor \n  \n  \n    2017-03-11 13:51:00 \n    pass bill nancy \n  \n  \n    2017-03-05 10:45:00 \n    front bill maudlin \n  \n  \n    2016-10-18 13:13:00 \n    friends bill fast \n  \n  \n    2016-09-28 21:46:00 \n    johnson bill weld \n  \n  \n    2016-09-28 21:46:00 \n    idealist bill policy \n  \n  \n    2016-07-28 14:28:00 \n    delegates bill clinton \n  \n  \n    2012-08-02 12:59:00 \n    governor bill haslam \n  \n  \n    2012-05-03 18:43:00 \n    jobs bill money \n  \n  \n    2012-05-03 18:43:00 \n    talking bill drummond \n  \n  \n    2012-04-21 17:45:00 \n    xtc bill lot \n  \n  \n    2010-10-20 21:12:00 \n    betty bill bartlett \n  \n  \n    2009-07-02 16:57:00 \n    disco bill nelson \n  \n  \n    2009-03-09 08:36:00 \n    pay bill 20 \n  \n  \n    2009-02-27 18:17:00 \n    kill bill banks \n  \n\n\n\n\n\nIt looks like just one of the mentions of “Bill” is Bill Clinton, so we can ignore him (for this project, anyway).\nWas the uptick we saw above in posting activity around the election due to political activty? We asssume, but let’s look just the posts containing the names of the politcians I identified earlier. This does not include links I shared containing names but did not comment upon, as Facebook won’t tell me what the link was.\n\ncleaned_post_words %>% \n  mutate(month=as.yearmon(date)) %>% \n  filter(word %in% politicos) %>% \n  mutate(word=str_to_title(word)) %>% \n  group_by(month,word) %>% \n  summarize(n=n()) %>% \n  ggplot(aes(as.Date(x=month),y=n,fill=word)) + geom_col() +labs(x=\"Month\",y=\"Mentions\")\n\n`summarise()` has grouped output by 'month'. You can override using the\n`.groups` argument.\n\n\n\n\n\nYup, I guess so. Unlike some of my wonderful Facebook friends I have been able to let go of the emotions around the election, as my posts naming politicians have dropped back to their baseline levels. Naturally, the names have changed!\nCan you tell my political leanings from this data? Well, duh! You could just read the posts but where’s the fun in that? More practically, if we are analyzing big data sets it would be tough to glean the overall sentiment of many people from reading a feasible sample of posts. Let’s try running the sentiment analysis on all of the posts containing the name of a politician to see of anything emerges.\nNote, I often mention more than one name in a single post and this method won’t distinguish which words apply to which name.\n\npol_sentiment<- NULL\npol_sent<- NULL\nfor (pol in politicos) {\n  sent_words<-cleaned_post_words %>% \n    filter(word == pol) %>%\n    select(date) %>% \n    unique() %>% \n    inner_join(cleaned_post_words,by='date') %>%\n    select(word) %>% \n    inner_join(get_sentiments('afinn'),by=\"word\")\n  #did we get anything?\n  if(nrow(sent_words)>0){\n    avg_score<-summarise(sent_words,opinion=mean(value),post_count=n())\n    pol_sentiment<-bind_rows(bind_cols(Politician=str_to_title(pol),avg_score),pol_sentiment)\n    pol_sent<-bind_rows(tibble(Politician=str_to_title(pol),\n                                   opinion=avg_score$opinion,\n                                   post_count=avg_score$post_count,\n                                   sent_words=list(sent_words$value)\n                                   ),\n    pol_sent)\n    \n  }\n\n}\npol_sent[,1:3]\n\n# A tibble: 8 × 3\n  Politician opinion post_count\n  <chr>        <dbl>      <int>\n1 Romney     -1.75            4\n2 Sanders     0.0333         30\n3 Bush       -0.410          39\n4 Johnson     0.226          84\n5 Clinton     0.0345         29\n6 Hillary     0.0108         93\n7 Trump       0.171          82\n8 Obama      -0.112          98\n\n\nFirst off, Palin and McCain don’t appear in this list beacuse I apparently didn’t use any words from the “afinn” lexicon in my posts mentioning them. Second, Romney is only mentioned in four posts so I don’t think we have a valid sample size. Notice that we store the list of sentiment scores for each politician in sent_words. We’ll use that in a minute.\nTaking what’s left, let’s view a chart.\n\npol_sent %>% filter(Politician != \"Romney\") %>% \n  ggplot(aes(x=reorder(Politician,opinion),y=opinion))+geom_col()+\n  coord_flip()+labs(y=\"Sentiment\",x=\"\")\n\n\n\n\nI’m sad to say this is far from how I would rank order my feelings about each of these names. I was a Gary Johnson supporter but, beyond that, this list might as well be random. Sample size is an issue. I am not that prolific a poster and the words I have in common with the sentiment lexicon is fewer still. Also, remember the sentiment ranges run from -5 to +5. We are talking small magnitudes here. Here is the same chart with the maximum allowable range shown.\n\npol_sent %>% filter(Politician != \"Romney\") %>% \n  ggplot(aes(x=reorder(Politician,opinion),y=opinion))+geom_col()+\n  coord_flip()+labs(y=\"Sentiment\",x=\"\")+ylim(c(-5,5))\n\n\n\n\nLet’s subject this to a test of significance. The “null hypothesis” we want to test is that the mean sentiment score expressed for “Hillary” is not statistically different from the “Trump” score.\n\ntrump_opinion<-pol_sent %>% filter(Politician==\"Trump\") %>% pull(sent_words) %>% unlist()\nhillary_opinion<-pol_sent %>% filter(Politician==\"Hillary\") %>% pull(sent_words) %>% unlist()\nt.test(trump_opinion,hillary_opinion)\n\n\n    Welch Two Sample t-test\n\ndata:  trump_opinion and hillary_opinion\nt = 0.46581, df = 170.76, p-value = 0.6419\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5179631  0.8379211\nsample estimates:\n mean of x  mean of y \n0.17073171 0.01075269 \n\n\nLike before, we can’t reject our hypothesis, I liked Donald Trump as much as I liked Hillary Clinton or, rather, you can’t prove a thing!\nThis analysis shows I didn’t really express strong opinions. It was deliberate. During the election things got pretty heated, as you may recall. I have Facebook friends on all sides of the the political divides. Out of respect, I tried very hard not to trash anybody’s candidate and instead tried to accentuate the positive.\n#Bonus Friend Analysis!\nOne more thing. We’ve only looked at the ‘timeline.htm’ file but Facebook’s data dump includes a lot of other stuff including a list of your friends and the date they were added. We can look at a timeline and summary statistics for for this too.\n\nraw_friends<- read_html(paste0(path,\"friends.htm\"),encoding=\"UTC-8\")\n\nHere the <h2> tag separates the labels for the different friend interactions. What are the categories of interactions Facebook gives us?\n\nfriend_nodes <- raw_friends %>% \n  html_nodes(\"h2\") %>% html_text()\n\nfriend_nodes\n\n[1] \"Friends\"                  \"Sent Friend Requests\"    \n[3] \"Received Friend Requests\" \"Deleted Friend Requests\" \n[5] \"Removed Friends\"          \"Friend Peer Group\"       \n\n\nThe actual items are in lists separated by the <ul> tag. Let’s traverse the list, extracting the category, name and date for each. The first and the last lists do not contain relevant info so we’ll take just the middle five. We’ll also rename the “Friends” category to the more descriptive “Friends Added.”\n\nfriend_nodes[1]<-\"Friends Added\"\nlist_nodes <- raw_friends %>% \n  html_nodes(\"ul\") %>% \n  .[2:6]\n\nfriend_table<- NULL\nfor (node in 1:length(list_nodes)){\n  category<-friend_nodes[node]\n  items<- list_nodes[node] %>% \n    html_nodes(\"li\") %>% \n    html_text() %>% \n    tibble(item=.) %>% \n    separate(item,c(\"Name\",\"Date\"),sep=\"\\\\(\",remove=TRUE) %>% \n    mutate(Date=str_replace(Date,\"\\\\)\",\"\"))\n  \n    friend_table<-cbind(Category=category,items,stringsAsFactors=FALSE) %>% \n      bind_rows(friend_table) %>% \n      as_tibble()\n}\n\nHow many Facebook friends do I have? Not many, by Facebook standards. This is, of course, the question that measures our entire self-worth. I’m very discriminating about who I friend . Well, that plus I’m not a very likeable person. Only four people have become so annoying that I unfriended them (both political extremes). There are more that I unfollowed (too many cat videos) but Facebook doesn’t include them in the log for some reason. Facebook also won’t tell me who unfriended me. But I’m sure that no one would do THAT.\n\nfriend_table %>% group_by(Category) %>% summarize(Number=n())\n\n# A tibble: 5 × 2\n  Category                 Number\n  <chr>                     <int>\n1 Deleted Friend Requests      67\n2 Friends Added               167\n3 Received Friend Requests      7\n4 Removed Friends               4\n5 Sent Friend Requests          3\n\n\nOnce again we have a character string for the date which we need to turn into a proper date. The wrinkle here is dates in the current year don’t specify the year in the log. We have to manually add it. The data is at a daily resolution, which is too granular for a clear picture at my level of activity. Let’s make it quarterly.\n\nfriend_table2 <- friend_table %>% \n  mutate(Date=if_else(str_length(Date)<7,paste0(Date,\", \",year(Sys.Date())),Date)) %>%\n  mutate(Date=parse_date(Date,format=\"%b %d, %Y\")) %>% \n  select(Category,Date) %>%\n  mutate(yearquarter=as.yearqtr(Date)) %>%\n  group_by(yearquarter)\ngg<-friend_table2 %>% ggplot(aes(x=as.Date(yearquarter),fill=Category))+ geom_bar(width=70)\ngg<-gg +labs(title=\"Facebook Friending Activity\",y=\"Count\",x=\"Quarterly\")\ngg\n\n\n\n\nNot too surprising. There was a lot of friending happening when I first joined Facebook. Perhaps a bit more curious is the recent renewed uptick in friending. There has been an influx of renewed high school aquaintances.\nSometimes it is useful too look at the balance of opposites. For example, we can see the balance of the number of friendings vs. the number of delete friend requests by assigning a negative number to deletions. There is no simple way to do this with native dplyr functions, though there should be. Base R is actually better at transforming just certain elements in a column based on some condition. Fortunately, I found a super-useful bit of code on Stack Overflow, mutate_cond(), that does exactly what we need.\n\nmutate_cond <- function(.data, condition, ..., envir = parent.frame()) {\n  #change elements of a column based on a condition\n  #https://stackoverflow.com/questions/34096162/dplyr-mutate-replace-on-a-subset-of-rows/34096575#34096575\n  condition <- eval(substitute(condition), .data, envir)\n    condition[is.na(condition)] = FALSE\n    .data[condition, ] <- .data[condition, ] %>% mutate(...)\n    .data\n}\n\n# tabulate sums of categories by quarter\nfriend_table3 <- friend_table %>% \n  mutate(Date=if_else(str_length(Date)<7,paste0(Date,\", \",year(Sys.Date())),Date)) %>%\n  mutate(Date=parse_date(Date,format=\"%b %d, %Y\")) %>% \n  mutate(yearquarter=as.yearqtr(Date)) %>%\n  select(Category,yearquarter) %>%\n  group_by(Category,yearquarter) %>% \n  summarise(count=n())\n\n`summarise()` has grouped output by 'Category'. You can override using the\n`.groups` argument.\n\n#make deleted requests negative\ngg<-friend_table3 %>% \n  mutate_cond(Category==\"Deleted Friend Requests\",count=-count) %>% \n  filter(Category==\"Deleted Friend Requests\" | Category==\"Friends Added\") %>% \n  ggplot(aes(x=as.Date(yearquarter),y=count,fill=Category))+ geom_col() \n\ngg<-gg +labs(title=\"Facebook Friending Activity\",y=\"Count\",x=\"Quarterly\")\ngg\n\n\n\n\nIt seems that adding friends is associated with deleted requests. I’ll surmise that when I show up in a new friend’s network that will spark some friend requests from their network. Some, maybe most, will be from people I don’t actually know and I will reject. There are spikes in rejections because I let them stack up before I notice them.\n\n\nWrapping Up\nWell that was cool. We got to try a lot of things. HTML parsing, date functions, sentiment analysis, text mining and lots of dplyr manipulations. Like a lot of projects, once I got going I thought of many things to try beyond the initial scope. That’s where the fun is when you’re not on deadline and deliverables. Thanks for making it all the way through. Hopefully this gives you some ideas for your own explorations. Now you try!\n#Double Bonus! Making the cool speedometer at the top of this post\nBecause we love cool visualizations, let’s show my mood on a silly gauge. I won’t show the low-level code I used to generate it because it’s basically a copy of what you can find here: http://www.gastonsanchez.com/. Gaston is a visualization guru extrordinaire. This shows how far you can take base R graphics.\nWhat do you think your gauge would look like? If it’s in the red call the suicide prevention hotline.\nThe animation is created using the magick package. I am thrilled that this recent release brings the image magick program inboard to R so we no longer have to run an external program to render animated files like GIFs (which I insist on pronouncing with a hard ‘G’, BTW.)\n\n# create animated mood gif for top of notebook.\nlibrary(magick)\nmy_days_moods<-word_scores_by_weekday %>%\n  summarise(mood=mean(value))\n\n#interpolate to create more points for a smooth animation.\n# the trick is to create a series where the mood stays constant for a number of frames\n# then transitions smoothly to the next mood value. Examine interp_moods to see how.\ninterp_moods<-tibble(doy=unlist(lapply(levels(my_days_moods$doy),rep,10)),\n                         mood_label=round(unlist(lapply(my_days_moods$mood,rep,10)),2),\n                         mood=approx(x=1:14,unlist(lapply(my_days_moods$mood,rep,2)),n=70)$y)\n\n\ninterp_moods$mood_label<- paste(ifelse(interp_moods$mood_label>0,\"+\",\"\"),interp_moods$mood_label)\n\n# I'll spare you the details of the low-level code.\n# see it at http://www.gastonsanchez.com/.\nsource(\"g_speedometer.r\")\n\nimg <- image_graph(600, 400, res = 96)\nfor(n in 1:nrow(interp_moods)){\n  plot_speedometer(label=interp_moods$doy[n],\n                   value=round(interp_moods$mood[n],2),\n                   bottom_label=interp_moods$mood_label[n],\n                   min=-0.5,\n                   max=0.5)\n  text(-0.1,1.0,\"Faceboook Mood-o-Meter\",cex=1.3)\n}\ndev.off()\nimg <- image_background(image_trim(img), 'white')\nanimation <- image_animate(img, fps = 10)\nimage_write(animation, path = \"moods.gif\", format = \"gif\")"
  },
  {
    "objectID": "posts/2023_01_24/index.html",
    "href": "posts/2023_01_24/index.html",
    "title": "test",
    "section": "",
    "text": "here is my test."
  },
  {
    "objectID": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html",
    "href": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html",
    "title": "Live Fast, Die Young, Stay Pretty?",
    "section": "",
    "text": "Live fast, die young, stay pretty? That’s the stereotype for rockers, or it was. We only need to look at Keith Richards, over 70 and going strong, to find a major counterexample. Do rockers die young? What do they die of? How does that compare to the broader population (in the U.S., anyway). It turns out there are some suprising answers to those questions.\n\n\nAlong the way we’ll learn something about web scraping, html parsing and some ggplot2 tricks. We use the tidyverse dialect throughout, just so you know."
  },
  {
    "objectID": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html#a-note-on-pointers.",
    "href": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html#a-note-on-pointers.",
    "title": "Live Fast, Die Young, Stay Pretty?",
    "section": "\nA note on “pointers.”\n",
    "text": "A note on “pointers.”\n\n\nR and the rvest package have some great functions for converting html <table>s into data frames. rvest is a very powerful package but one thing I learned is that it works with pointers to the data rather than the actual data. C programmers and old geezers like me will be familiar with this. I remember pop and push and stacks and all that stuff from the old days. R generally doesn’t pass values “by reference.” It passes “by value.” That’s why when you modify data in the scope of a function it doesn’t affect the value of the data outside unless you assign it with return <data>. Using pointers in rvest functions means modifications to html data happen without an explicit assigment.\n\n\nConsider a trival example:\n\n#usual R behavior\nmy_function<- function(x){return (x+1)}\ndata=3\n`#this doesn't change data but it would if data was passed by reference\nmy_function(data)\n`# [1] 4\ndata\n`# [1] 3\n`#this does change data in the usual R way\ndata<-my_function(data)\ndata\n`# [1] 4\n\nIf we were passing values “by reference” my_function(data) would change data without the need to assign it back to data. That’s how rvest works.\n\n\nWe use this behavior to combine the tables in the two Wikipedia articles into one html page by extracting the tables in the second wiki article and making them xml siblings of the tables in the first.\n\n\nAlternatively, we could load the two pages, extract the tables separately and combine them later but this is trickier!\n\n#join pre-2010 to post 2010\ndeath_page<-death_page1\ndeath_page_child<-death_page1 %>% xml_children() %>% .[2]\ndeath_page2_child<-death_page2 %>% xml_children() %>% .[2]\n#create one big web page by adding all the first level children from the second\n#page to the first.\n#This modifies death_page by changing the list of pointers associated with it.\nxml_add_sibling(death_page_child,death_page2_child)\nwrite_html(death_page,file=\"death_page.html\")"
  }
]