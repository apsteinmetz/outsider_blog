[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Art Steinmetz",
    "section": "",
    "text": "July Fourth by Grandma Moses\n\n\nI am not a data scientist. Others have coined good terms like “Data Nerd” and “Citizen Data Scientist.” I’ll coin another: “Outsider Data Scientist.” I would style myself in the likeness of an “outsider” artist, Grandma Moses. She was an American artist who didn’t pick up a brush between childhood and old age, and had no formal training. The works she produced would never be mistaken for the old masters’ but they had a certain charm. Perhaps I might strive for that. I am also getting on in years.\nI play around with R for fun. I enjoy thinking up ways to present complex information in a simple, compelling way. I attended an R conference where an axiom was presented by Dave Robertson that the value of information “still” on your computer is approximately zero and the value of information out in the world is infinitely more. Even if it is small that’s infinitely more than zero, right? That emboldened me to put stuff “out there.” Perhaps someone else might find it interesting. At a minimum this blog forces more rigor in my own thinking.\n“Outsider” is a bit of a misnomer. I’m sure Grandma Moses saw an another painting or two in her life. Painting was pretty mature before Ms. Moses picked up her brush, but this whole data science thing has exploded in the last few years. It is very exciting to even be on the periphery of the event horizon. I am indebted to the R community for all the examples they have shared through R-Bloggers, Stack Overflow and Twitter. The tools provided by Posit (formerly RStudio) are the bomb!\nThis is a personal side project in no way associated with any organization I am affiliated with. My opinions here are mine alone and any data I present here is neither proprietary nor is it warranted to be correct or accurate. Nothing I say here should be construed as investment advice.\nI used to work at OppenheimerFunds Inc. before it was acquired by Invesco, first as a portfolio manager of global macro fixed income and ultimately as CEO. I am the former board chair at the National Museum of Mathematics, MoMath.org, where I rubbed shoulders (though not in creepy way) with people who are really, really smart. Visit the museum when you are in NYC. We are making math cool! Finally, I am on the board of “Rock the Street, Wall Street” which brings financial literacy programs into high school classrooms to encourage girls to become interested in finance. We need more diversity in finance.\n\nArt Steinmetz"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Outsider Data Science",
    "section": "",
    "text": "Putting what’s in there, out there. With R!\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nSwitching to Quarto from Blogdown\n\n\n\n\n\n\n\nquarto\n\n\nblogging\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2022\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nCovid Cases vs. Deaths\n\n\n\n\n\n\n\nggplot2\n\n\ntidymodels\n\n\ntidyverse\n\n\nCOVID\n\n\n\n\nEstimate the average lag between a positive COVID-19 case and a death.\n\n\n\n\n\n\nDec 6, 2020\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nWhat Do The Ramones Want?\n\n\n\n\n\n\n\nggplot2\n\n\ntidytext\n\n\nmusic\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2020\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nState Taxes: It’s not just about Income\n\n\n\n\n\n\n\ntax\n\n\nplotly\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2019\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nGender Diversity in R and Python Package Contributors\n\n\n\n\n\n\n\ngithub\n\n\ngender\n\n\n\n\n\n\n\n\n\n\n\nJul 16, 2019\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nWhy I migrated from Excel to R\n\n\n\n\n\n\n\nR\n\n\nExcel\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2019\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nSolving the Letterboxed Puzzle in the New York Times\n\n\n\n\n\n\n\npuzzle\n\n\nrecursion\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2019\n\n\nArthur Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nWhere Are The Libertarians?\n\n\n\n\n\n\n\npolitics\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2019\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nRick and Morty Palettes\n\n\n\n\n\n\n\nclustering\n\n\nrick and morty\n\n\npalettes\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2019\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nIs Free Pre-K in NYC Favoring the Rich?\n\n\n\n\n\n\n\nweb scraping\n\n\neducation\n\n\nmaps\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2018\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nNew Winter Sports for New Countries\n\n\n\n\n\n\n\nweb scraping\n\n\nsports\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2018\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nLive Fast, Die Young, Stay Pretty?\n\n\n\n\n\n\n\nmusic\n\n\nweb scraping\n\n\nhealth\n\n\ngapminder\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2018\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nPlumbing the Depths of My Soul (in Facebook)\n\n\n\n\n\n\n\ntext mining\n\n\nfacebook\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2017\n\n\nArt Steinmetz\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence.html",
    "href": "posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence.html",
    "title": "Plumbing the Depths of My Soul (in Facebook)",
    "section": "",
    "text": "First post! Let’s start out nice and easy. No big data machine learning or heavy stats. This post will merely explore the depths of my soul through a meta-analysis of every one of my Facebook posts. Meta-navel gazing, if you will.\nPerhaps you are not all that interested in the plumbing the depths of my soul. Still, you may be interested in seeing how you can do an analyis of your own Facebook life in the comfort of your own home. If so, read on!\nWe will (lightly) cover web scraping, sentiment analysis, tests of significance and visualize it with a generous helping of ggplot. Note I use the tidyverse/dplyr vernacular. This is fast becoming a dialect of R. I quite like it but its syntax is different than traditional R. It produces sometimes slower, but much more readable, code. Basically, you “pipe” data tables through action verbs using the pipe operator (“%>%”).\nLet’s go do some outsider data science!\nStart by loading needed packages.\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(wordcloud)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggplot2)\nlibrary(zoo)\nlibrary(reshape2)\nlibrary(lubridate)\n\n\n#make explicit so kableExtra doesn't complain later\noptions(knitr.table.format = \"html\") \n\n\nFetch and clean all the words in my Facebook posts\nFacebook lets you download a log of all your activity at https://Facebook.com/settings. Look toward the bottom of the page for the download link. You will get an email with a link to a zipped set of html files. These are what I’ll be using for the analysis.\nFirst let’s get all my comments since the dawn of my Facebook existence.\n\npath='data/'\nraw_timeline<- read_html(paste0(path,\"timeline.htm\"),encoding=\"UTC-8\")\n\nNow that we have the raw data we need to extract the just the text of the comments. Visually inspecting the raw html file reveals that all of the comments I wrote have the tag <div class=\"comment\"> so I construct an xpath selector to grab those nodes then get the text in them. This is what the raw html looks like:\n</p><p><div class=\"meta\">Thursday, November 16, 2017 at 1:17pm EST</div> <div class=\"comment\">I’m sure you are all thinking “what does this mean for Al Franken?”</div> </p><p> <div class=\"meta\">Thursday, November 16, 2017 at 10:44am EST</div> Art Steinmetz shared a link. </p><p>\nThe challenge here is that we want to get the date also which appears BEFORE the comment and has the tag <div class=\"meta\">. Unfortunately, as we see above, merely sharing a link generates this tag without any comment or a different tag class so there are more meta classes than comment classes. Facebook should create a separate XML record for each log activity, but they don’t.\nThe code below seems inelegant to me. for loops in R are non-idiomatic and indicate somebody was steeped in a non vectorized language (like me). I tried without success to craft an xpath expression that would walk backwards when it sees a comment class to get the date. In the end I resorted to the devil I know, a loop.\n\ntimeline_post_nodes <- raw_timeline %>% \n  html_nodes(xpath=\"//div[@class ='comment'] | //div[@class='meta']\")\n\ntimeline_posts1<-NULL\n#the bit below is the slowest part of our project. \n#If you post multiple times a day over years it could take a while.\nfor (n in 1:length(timeline_post_nodes)){\n  if ( html_attr(timeline_post_nodes[n],\"class\")==\"comment\"){\n    post= html_text(timeline_post_nodes[n])\n    date= html_text(timeline_post_nodes[n-1])\n    timeline_posts1<-timeline_posts1 %>% bind_rows(tibble(date,post))\n  }\n}\n\nThe time stamps we extracted are just character strings with no quantitative meaning. Let’s convert the dates in the form of “Saturday November 18 2017 11:12am EST” to a day of the week and a POSIX date/time format that other R functions will understand. First we pull out the day of the week using the comma as a separator but this also separates the month and day from the year, which we don’t want, so we put those back together.\nThis begs the question of whether we should have used a tricker “regular expression” to accomplish this in one step. RegExes are a dark art that I have a lot of admiration for, even if I am a rank neophyte. In this exercise I didn’t think it was worth the time to figure out a “proper” solution when a “simple” one sufficed. Other times I like the puzzle challenge of coming up with a powerful RegEx. There are web sites that are a great help in building them. Try http://regex101.com, for one.\nWith a good date string in hand we can use parse_date() to convert it. Notice the format string we use to accomplish this.\n\ntimeline_posts<-timeline_posts1 %>%\n  mutate(date=sub(\"at \",\"\",date)) %>%\n  separate(date,c(\"doy\",\"date\",\"yeartime\"),sep=\", \") %>%\n  transmute(doy=doy,date=paste(date,yeartime),post=post)\n\n# Now that we've pulled out the day of the week, let's make sure they show in order in plots\n# by making doy and ordered factor.\nday_order<-c(\"Monday\",\"Tuesday\",\"Wednesday\",\n            \"Thursday\",\"Friday\",\"Saturday\", \n            \"Sunday\")\n\ntimeline_posts$doy<-factor(timeline_posts$doy,levels = day_order)\n\ntimeline_posts<-timeline_posts %>% \n  mutate(date = str_remove(date,\" EST| EDT\")) |> \n  mutate(date = parse_datetime(date,\n                               format=\"%B %d %Y %I:%M%p\",\n                               locale = locale(tz = \"US/Eastern\")))\n\nkable(head(timeline_posts[1:2,])) %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    doy \n    date \n    post \n  \n \n\n  \n    Saturday \n    2017-11-18 11:12:00 \n    I feel cheated. When I read the fine print I see these guys haven't won the \"Uniformity of Granulation\" award since 1894.  I want the oatmeal that won last year! \n  \n  \n    Saturday \n    2017-11-18 10:41:00 \n    I had a chance to visit Shenzhen this year.  The hardware scene is reminiscent of Blade Runner as you'll see.  This guy prowls the markets to make his own iPhone from scratch. \n  \n\n\n\n\n\nWe now have over 2000 text strings, each representing one post. Since we are working at the word level we need to break up each post into its constituent words.\nFor much of this analysis I am following the example shown at https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html.\nThe unnest_tokens function from the ‘tidytext’ package lets us convert a dataframe with a text column to be one-word-per-row dataframe. How many words are there?\n\nmy_post_words<-  timeline_posts %>%\n  unnest_tokens(word, post)\n\nnrow(my_post_words)\n\n[1] 51347\n\n\nSo we have over fifty thousand words. A lot of them are going to be uninteresting. Although, given that Facebook posts are an excercise in narcissim, you might say all of them are uninteresting to anybody but me.\nAnyway, lets press on. We can use the stop_words data set included with tidytext to to strip out the superfluous words. Note this includes words like ‘accordingly’ which convey little meaning but might be useful in revealing idiosyncratic writting patterns, much like people punctuate their speech with vocal pauses like “like” and “right.” How many words are left after that?\n\ndata(\"stop_words\")\ncleaned_post_words <- my_post_words %>%\n  anti_join(stop_words,by='word')\n\nnrow(cleaned_post_words)\n\n[1] 22465\n\n\n\n\nLook at the most common words\nSo now our data set is clean and tidy. Let’s answer some questions. What are the most common words I use in posts.\n\npopular_words<-cleaned_post_words %>%\n  count(word, sort = TRUE)\nkable(popular_words[1:10,]) %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    word \n    n \n  \n \n\n  \n    day \n    111 \n  \n  \n    â \n    101 \n  \n  \n    people \n    97 \n  \n  \n    time \n    92 \n  \n  \n    kids \n    84 \n  \n  \n    love \n    69 \n  \n  \n    carrie \n    60 \n  \n  \n    guy \n    59 \n  \n  \n    friends \n    56 \n  \n  \n    art \n    48 \n  \n\n\n\n\n\nI don’t know where that “a-hat” character comes from but let’s get rid of it.\n\ncleaned_post_words<- cleaned_post_words%>% \n  mutate(word=str_replace(word,\"â\",\"\")) %>% \n  filter(str_length(word)>0)\npopular_words<-cleaned_post_words %>%\n  count(word, sort = TRUE)\n\nAfter we strip out stop words we have less then 10,000 “real” words left.\nGood to see that my wife’s name is one of my most used words. “Kids,” “friends,” and “love” are no surprise. What’s a good way to visualize this? Word cloud!\nI love word clouds! We can easily display the most used words this way using the wordcloud package.\n\n# We love wordclouds!\n#scalefactor magnifies differences for wordcloud\nscaleFactor=1.3\nmaxWords = 200\n\n\nwordcloud(words = popular_words$word, \n          freq = popular_words$n^scaleFactor,\n          max.words=maxWords, \n          random.order=FALSE,rot.per=0.35, \n          colors=brewer.pal(8, \"Dark2\"),\n          scale = c(3,.3))\n\n\n\n\nI mentioned “Obama” about as often as I mentioned “beer.”\n\n\nDo some sentiment analysis\nI used to be jerk. But, given my age, I am entitled to call myself a curmudgeon instead. That sounds nicer somehow, and excuses my negative reaction to everything. However, given how internet discourse easily sinks into a tit-for-tat of profane hatred, I try to go against type, accentuate the positive and say nothing if I can’t say something nice. That’s the idea. How does my sour nature interact with my better intentions? We can use sentiment analysis to find out. The tidytext package also has serveral lexicons with thousands of words coded by their sentiment. Refer to http://tidytextmining.com for an excellent tutorial on this. Obviously, the isolated word approach has limitations. Context matters and by taking one word at a time we don’t capture that. So, with that caveat, how much of a downer am I?\nFirst, let’s look at the sentiment of my posts on a binary basis. Is the word positive or negative? The “bing” lexicon scores thousands of words that way. Obviously, not all the words we used are in the data set. About a third are, though.\n\ncleaned_post_words %>% \n  inner_join(get_sentiments('bing'),by=\"word\") %>% \n  group_by(sentiment) %>% \n  summarize(count=n()) %>% \n  kable() %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    sentiment \n    count \n  \n \n\n  \n    negative \n    1678 \n  \n  \n    positive \n    1412 \n  \n\n\n\n\n\nWell, then. So I am a downer, on a net basis, but not terribly so.\nWe can make this into a word cloud, too! Here are the words I used divided by sentiment.\n\ncleaned_post_words %>%\n  inner_join(get_sentiments('bing'),by=\"word\") %>%\n  count(word, sentiment, sort = TRUE) %>%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %>%\n  comparison.cloud(colors = c(\"#F8766D\", \"#00BFC4\"),\n                   max.words = 100)\n\n\n\n\nWait a minute! “Trump” is scored as a positive sentiment word! Is this a hidden statement by the author of the lexicon?! Doubtful. It’s “trump,” as in “spades trumps clubs,” not as a proper name. And why is “funny” a negative word? I guess it’s “funny strange,” not “funny ha-ha.” It shows the limitations of this kind of thing.\nA different lexicon scores each word’s sentiment on a scale of minus to positive five. This seems pretty subjective to me but has the benefit of letting us add up the numbers to get a net score. What is my sentiment score over all words I’ve ever written on Facebook (not all, the log doesn’t include comments to other’s posts).\n\nsentiment_score<-cleaned_post_words %>% \n  inner_join(get_sentiments('afinn'),by=\"word\") %>% \n  pull(value) %>% \n  mean()\nsentiment_score\n\n[1] 0.1610233\n\n\nWell, this draws an slightly different conclusion. The net score of my sentiment is +0.16 out of range of -5 to +5. Just barely happy. While I may use more negative than positive words, my positive words are more positive. I suspect the word “love” which we already saw is frequently used (though it is “only” a “3”) accounts for this.\nWhat were my most negative words?\n\nword_scores<-cleaned_post_words %>% \n  inner_join(get_sentiments('afinn'),by=\"word\") %>% \n  group_by(word,value) %>% summarise(count=n())\n\n`summarise()` has grouped output by 'word'. You can override using the\n`.groups` argument.\n\n\n\nword_scores %>%\n        arrange((value)) %>%\n        ungroup() %>%\n        .[1:10,] %>%\n        kable() %>%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\nThis is a family blog so I comment out the code that displays the worst words. Suffice it to say, they are the usual curse words and forms thereof. I am cringing right now. Did I say those things? Yes, well not often, at least, once or twice is typical for each.\nAs I mentioned above, the limitation of this analysis is that it lacks context. For instance, did I call someone a slut? I was briefly horrified when I saw that word. Here is the word in context from 2014: “Less slut-shaming and more perp-jailing.”\nAll these negative words carry more power for me, an old-geezer, than for kids today (kids today!) who let f-bombs roll off their tongues with uncomfortable (to me) ease. Get off my lawn!\nWhat were my positive words?\n\nword_scores %>% arrange(desc(value)) %>% \n  ungroup() %>%\n  .[1:10,] %>% \n  kable() %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    word \n    value \n    count \n  \n \n\n  \n    breathtaking \n    5 \n    1 \n  \n  \n    outstanding \n    5 \n    1 \n  \n  \n    thrilled \n    5 \n    3 \n  \n  \n    amazing \n    4 \n    19 \n  \n  \n    awesome \n    4 \n    19 \n  \n  \n    brilliant \n    4 \n    5 \n  \n  \n    fabulous \n    4 \n    1 \n  \n  \n    fantastic \n    4 \n    2 \n  \n  \n    fun \n    4 \n    45 \n  \n  \n    funnier \n    4 \n    1 \n  \n\n\n\n\n\nWhew! I feel better now. Everything is awesome!\nDid I get happier or sadder over time? We’ll answer that question in a minute.\n\n\nTime Patterns\nThe foregoing analysis just includes posts on my timeline where I made a comment. If we want to know things like when I’m active on Facebook we need to look at all activity. Again, Facebook doesn’t separately tag different activities. Let’s go back over all the activity to pull out just the timestamps, but all of them this time.\n\nactivity_times <- tibble(date = raw_timeline %>% \n                             html_nodes(xpath=\"//div[@class='meta']\") %>% \n                             html_text()\n                              ) %>%\n  mutate(date=sub(\"at \",\"\",date)) %>%\n  separate(date,c(\"doy\",\"date\",\"yeartime\"),sep=\", \") %>%\n  transmute(doy=doy,date=paste(date,yeartime)) %>%\n  mutate(date = str_remove(date,\" EST| EDT\")) |> \n  mutate(date = parse_datetime(date,\n                               format=\"%B %d %Y %I:%M%p\",\n                               locale = locale(tz = \"US/Eastern\")))\n\nactivity_times$doy<-factor(activity_times$doy,levels = day_order)\n\nLet’s ask a couple questions. What day of the week am I most active on Facebook?\n\n#make sure days of week are in sequential order. Monday first\n\nactivity_times %>% ggplot(aes(doy))+geom_bar()+\n  labs(title='Facebook Activity', x='Weekday',y='Posts')\n\n\n\n\nMonday stands out. I didn’t realize this. Perhaps I come to work Monday morning and catch up with the news which prompts me to post.\nAm I more cranky on different days?\n\n#cleaned_post_words$doy<-factor(cleaned_post_words$doy,levels = day_order)\n\nword_scores_by_weekday<-cleaned_post_words %>% \n  inner_join(get_sentiments('afinn'),by=\"word\") %>% \n  group_by(doy)\n\nword_scores_by_weekday %>%\n  summarise(mood=mean(value)) %>% \n  ggplot(aes(x=doy,y=mood))+geom_col()+labs(x=\"Weekday\",y=\"Mood Score\")\n\n\n\n\nThis is interesting! I am in a relatively good mood on Monday! It’s the middle of the week when I tend to use more negative words. Then I pick up going into the weekend.\nRemember though, these are numbers of small magnitude. Are the variations statistically significant? Let’s compare Tuesday to Sunday and (which have the most extreme differences). First visually then with a t-test to see if the differences are significant. For our hypothesis we assume the the true difference in the average mood on Monday is no different than the average mood on Sunday. Based on the differences we see, can we reject this hypothesis?\n\nsunday_moods<-word_scores_by_weekday %>% \n  filter(doy==\"Sunday\") %>% \n  group_by(doy,date) %>% \n  summarise(mood=mean(value)) %>% \n  select(doy,mood)\n\n`summarise()` has grouped output by 'doy'. You can override using the `.groups`\nargument.\n\ntuesday_moods<-word_scores_by_weekday %>% \n  filter(doy==\"Tuesday\") %>% \n  group_by(doy,date) %>% \n  summarise(mood=mean(value)) %>% \n  select(doy,mood)\n\n`summarise()` has grouped output by 'doy'. You can override using the `.groups`\nargument.\n\nbind_rows(tuesday_moods,sunday_moods) %>% ggplot(aes(mood,fill=doy))+geom_density(alpha=0.7)\n\n\n\n\n\nt.test(tuesday_moods$mood,sunday_moods$mood)\n\n\n    Welch Two Sample t-test\n\ndata:  tuesday_moods$mood and sunday_moods$mood\nt = -0.97824, df = 332.11, p-value = 0.3287\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.6292549  0.2112694\nsample estimates:\n mean of x  mean of y \n0.06088435 0.26987711 \n\n\nRats! It looks like our “interesting” observation is not interesting. The p-value of 0.32 is below 2, so we can’t reject our hypothesis. The difference in mean sentiment for Sunday and Tuesday would have to be beyond the confidence interval to give us acceptable certainty that I am most cranky on Tuesday.\nWe can’t get too excited by the density plot, either. My posts are bi-modally distributed but, given the relatively short length of my posts, chances are I use just one sentiment-loaded word and that skews the distribution. Again, small sample sizes are the problem. Pretty picture, though!\nWhat times am I most active?\n\nhours<-cleaned_post_words %>% mutate(hour=hour(date))\n\nhours %>% ggplot(aes(hour))+geom_bar()+\n  labs(title='Facebook Activity', x='Hour',y='Posts')\n\n\n\n\n#Trends over Time\nIs there any trend to my Facebook activity over time? Let’s bucket the posts by month and look for a pattern.\n\nactivity_times <- activity_times %>% \n  #filter dates before I joined as bad data\n  filter(date>as.Date(\"2008-01-01\")) %>% \n  mutate(month=as.yearmon(date))\n\nactivity_times %>% \n  ggplot(aes(as.Date(month))) + geom_bar() +labs(x=\"Month\",y=\"Posts\")\n\n\n\n\nWhat’s up with the beginning of 2013 and December in 2015? Looking at the raw activity log I see that I briefly let Spotify tell you what I was listening to via Facebook. That generated a lot of activity. I turned it off after a couple weeks. In late 2016 around the election we also see an uptick in activity. Otherwise there have been pretty mild ebbs and flows, averaging about 30 posts per month.\n\nactivity_times %>% \n  group_by(month) %>% \n  summarise(n=n()) %>% \n  summarise(avg_per_month=mean(n)) %>% \n  kable() %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    avg_per_month \n  \n \n\n  \n    29.42105 \n  \n\n\n\n\n\n\n\nDoes my mood change over time?\nWe can repeat the sentiment analysis from above but bucket it by month.\n\nword_scores_by_month<-cleaned_post_words %>% \n  inner_join(get_sentiments('afinn'),by=\"word\") %>% \n  select(date, word,value) %>% \n  mutate(yearmonth=as.yearmon(date)) %>% \n  group_by(yearmonth) %>% summarise(mood=sum(value))\n\nword_scores_by_month %>%  \n  ggplot(aes(x=as.Date(yearmonth),y=mood))+geom_col()+geom_smooth()+labs(x=\"Month\",y=\"Mood Score\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nA trend is not very evident. Month-to-month variation is very high. Is it that we don’t have a good sample size or do my moods swing wildly? The most extreme gyration is around the 2016 presidential election. Optimism followed by despair? Perhaps.\n\n\nPolitics Rears Its Ugly Head\nI try to avoid too much talk about politics on Facebook but, like most of us, it was tough in an election year. This gives us an opportunity to dig into a specific topic within the posting corpus.\nLet’s start by seeing how often I mentioned politicians names.\n\npoliticos=c(\"obama\",\"trump\",\"hillary\",\"clinton\",\"johnson\",\"kasich\",\"bush\",\"sanders\",\"romney\",\"mccain\",\"palin\")\n\ngg<-cleaned_post_words %>% \n  filter(word %in% politicos) %>%\n  mutate(word=str_to_title(word)) %>% \n  count(word, sort = TRUE) %>% \n  \n  ggplot(aes(x=reorder(word,n),y=n))+geom_col()+coord_flip()+labs(y=\"Mentions\",x=\"\")\n\ngg\n\n\n\n\n“Johnson”” is Gary Johnson, in case you forgot. Unlike the the “lamestream media,” I gave much more attention to the Libertarian candidate in my posts. Oddly, that didn’t help his success in the election.\n“Hillary”” is the only first name in the list. Using a woman’s first name when equally familiar men are referred to with their last name is often sexist. In my defense, during the election it was important to distinguish between her and husband Bill so that’s why I used “Hillary.” We are not on a first name basis. On the other hand, “Bill” is a common enough name (and noun) so it’s likely that many posts using it don’t refer to the politician. Just to get an idea let’s look at the words bracketing “bill”:\n\ncleaned_post_words %>% \n  mutate(phrase=paste(lag(word),word,lead(word))) %>% \n  filter(word=='bill') %>% \n  select(date,phrase) %>% \n  kable() %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    date \n    phrase \n  \n \n\n  \n    2017-09-20 08:53:00 \n    morning bill protection \n  \n  \n    2017-07-23 12:27:00 \n    water bill extra \n  \n  \n    2017-03-11 14:15:00 \n    stitch bill doctor \n  \n  \n    2017-03-11 13:51:00 \n    pass bill nancy \n  \n  \n    2017-03-05 10:45:00 \n    front bill maudlin \n  \n  \n    2016-10-18 13:13:00 \n    friends bill fast \n  \n  \n    2016-09-28 21:46:00 \n    johnson bill weld \n  \n  \n    2016-09-28 21:46:00 \n    idealist bill policy \n  \n  \n    2016-07-28 14:28:00 \n    delegates bill clinton \n  \n  \n    2012-08-02 12:59:00 \n    governor bill haslam \n  \n  \n    2012-05-03 18:43:00 \n    jobs bill money \n  \n  \n    2012-05-03 18:43:00 \n    talking bill drummond \n  \n  \n    2012-04-21 17:45:00 \n    xtc bill lot \n  \n  \n    2010-10-20 21:12:00 \n    betty bill bartlett \n  \n  \n    2009-07-02 16:57:00 \n    disco bill nelson \n  \n  \n    2009-03-09 08:36:00 \n    pay bill 20 \n  \n  \n    2009-02-27 18:17:00 \n    kill bill banks \n  \n\n\n\n\n\nIt looks like just one of the mentions of “Bill” is Bill Clinton, so we can ignore him (for this project, anyway).\nWas the uptick we saw above in posting activity around the election due to political activty? We asssume, but let’s look just the posts containing the names of the politcians I identified earlier. This does not include links I shared containing names but did not comment upon, as Facebook won’t tell me what the link was.\n\ncleaned_post_words %>% \n  mutate(month=as.yearmon(date)) %>% \n  filter(word %in% politicos) %>% \n  mutate(word=str_to_title(word)) %>% \n  group_by(month,word) %>% \n  summarize(n=n()) %>% \n  ggplot(aes(as.Date(x=month),y=n,fill=word)) + geom_col() +labs(x=\"Month\",y=\"Mentions\")\n\n`summarise()` has grouped output by 'month'. You can override using the\n`.groups` argument.\n\n\n\n\n\nYup, I guess so. Unlike some of my wonderful Facebook friends I have been able to let go of the emotions around the election, as my posts naming politicians have dropped back to their baseline levels. Naturally, the names have changed!\nCan you tell my political leanings from this data? Well, duh! You could just read the posts but where’s the fun in that? More practically, if we are analyzing big data sets it would be tough to glean the overall sentiment of many people from reading a feasible sample of posts. Let’s try running the sentiment analysis on all of the posts containing the name of a politician to see of anything emerges.\nNote, I often mention more than one name in a single post and this method won’t distinguish which words apply to which name.\n\npol_sentiment<- NULL\npol_sent<- NULL\nfor (pol in politicos) {\n  sent_words<-cleaned_post_words %>% \n    filter(word == pol) %>%\n    select(date) %>% \n    unique() %>% \n    inner_join(cleaned_post_words,by='date') %>%\n    select(word) %>% \n    inner_join(get_sentiments('afinn'),by=\"word\")\n  #did we get anything?\n  if(nrow(sent_words)>0){\n    avg_score<-summarise(sent_words,opinion=mean(value),post_count=n())\n    pol_sentiment<-bind_rows(bind_cols(Politician=str_to_title(pol),avg_score),pol_sentiment)\n    pol_sent<-bind_rows(tibble(Politician=str_to_title(pol),\n                                   opinion=avg_score$opinion,\n                                   post_count=avg_score$post_count,\n                                   sent_words=list(sent_words$value)\n                                   ),\n    pol_sent)\n    \n  }\n\n}\npol_sent[,1:3]\n\n# A tibble: 8 × 3\n  Politician opinion post_count\n  <chr>        <dbl>      <int>\n1 Romney     -1.75            4\n2 Sanders     0.0333         30\n3 Bush       -0.410          39\n4 Johnson     0.226          84\n5 Clinton     0.0345         29\n6 Hillary     0.0108         93\n7 Trump       0.171          82\n8 Obama      -0.112          98\n\n\nFirst off, Palin and McCain don’t appear in this list beacuse I apparently didn’t use any words from the “afinn” lexicon in my posts mentioning them. Second, Romney is only mentioned in four posts so I don’t think we have a valid sample size. Notice that we store the list of sentiment scores for each politician in sent_words. We’ll use that in a minute.\nTaking what’s left, let’s view a chart.\n\npol_sent %>% filter(Politician != \"Romney\") %>% \n  ggplot(aes(x=reorder(Politician,opinion),y=opinion))+geom_col()+\n  coord_flip()+labs(y=\"Sentiment\",x=\"\")\n\n\n\n\nI’m sad to say this is far from how I would rank order my feelings about each of these names. I was a Gary Johnson supporter but, beyond that, this list might as well be random. Sample size is an issue. I am not that prolific a poster and the words I have in common with the sentiment lexicon is fewer still. Also, remember the sentiment ranges run from -5 to +5. We are talking small magnitudes here. Here is the same chart with the maximum allowable range shown.\n\npol_sent %>% filter(Politician != \"Romney\") %>% \n  ggplot(aes(x=reorder(Politician,opinion),y=opinion))+geom_col()+\n  coord_flip()+labs(y=\"Sentiment\",x=\"\")+ylim(c(-5,5))\n\n\n\n\nLet’s subject this to a test of significance. The “null hypothesis” we want to test is that the mean sentiment score expressed for “Hillary” is not statistically different from the “Trump” score.\n\ntrump_opinion<-pol_sent %>% filter(Politician==\"Trump\") %>% pull(sent_words) %>% unlist()\nhillary_opinion<-pol_sent %>% filter(Politician==\"Hillary\") %>% pull(sent_words) %>% unlist()\nt.test(trump_opinion,hillary_opinion)\n\n\n    Welch Two Sample t-test\n\ndata:  trump_opinion and hillary_opinion\nt = 0.46581, df = 170.76, p-value = 0.6419\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5179631  0.8379211\nsample estimates:\n mean of x  mean of y \n0.17073171 0.01075269 \n\n\nLike before, we can’t reject our hypothesis, I liked Donald Trump as much as I liked Hillary Clinton or, rather, you can’t prove a thing!\nThis analysis shows I didn’t really express strong opinions. It was deliberate. During the election things got pretty heated, as you may recall. I have Facebook friends on all sides of the the political divides. Out of respect, I tried very hard not to trash anybody’s candidate and instead tried to accentuate the positive.\n#Bonus Friend Analysis!\nOne more thing. We’ve only looked at the ‘timeline.htm’ file but Facebook’s data dump includes a lot of other stuff including a list of your friends and the date they were added. We can look at a timeline and summary statistics for for this too.\n\nraw_friends<- read_html(paste0(path,\"friends.htm\"),encoding=\"UTC-8\")\n\nHere the <h2> tag separates the labels for the different friend interactions. What are the categories of interactions Facebook gives us?\n\nfriend_nodes <- raw_friends %>% \n  html_nodes(\"h2\") %>% html_text()\n\nfriend_nodes\n\n[1] \"Friends\"                  \"Sent Friend Requests\"    \n[3] \"Received Friend Requests\" \"Deleted Friend Requests\" \n[5] \"Removed Friends\"          \"Friend Peer Group\"       \n\n\nThe actual items are in lists separated by the <ul> tag. Let’s traverse the list, extracting the category, name and date for each. The first and the last lists do not contain relevant info so we’ll take just the middle five. We’ll also rename the “Friends” category to the more descriptive “Friends Added.”\n\nfriend_nodes[1]<-\"Friends Added\"\nlist_nodes <- raw_friends %>% \n  html_nodes(\"ul\") %>% \n  .[2:6]\n\nfriend_table<- NULL\nfor (node in 1:length(list_nodes)){\n  category<-friend_nodes[node]\n  items<- list_nodes[node] %>% \n    html_nodes(\"li\") %>% \n    html_text() %>% \n    tibble(item=.) %>% \n    separate(item,c(\"Name\",\"Date\"),sep=\"\\\\(\",remove=TRUE) %>% \n    mutate(Date=str_replace(Date,\"\\\\)\",\"\"))\n  \n    friend_table<-cbind(Category=category,items,stringsAsFactors=FALSE) %>% \n      bind_rows(friend_table) %>% \n      as_tibble()\n}\n\nHow many Facebook friends do I have? Not many, by Facebook standards. This is, of course, the question that measures our entire self-worth. I’m very discriminating about who I friend . Well, that plus I’m not a very likeable person. Only four people have become so annoying that I unfriended them (both political extremes). There are more that I unfollowed (too many cat videos) but Facebook doesn’t include them in the log for some reason. Facebook also won’t tell me who unfriended me. But I’m sure that no one would do THAT.\n\nfriend_table %>% group_by(Category) %>% summarize(Number=n())\n\n# A tibble: 5 × 2\n  Category                 Number\n  <chr>                     <int>\n1 Deleted Friend Requests      67\n2 Friends Added               167\n3 Received Friend Requests      7\n4 Removed Friends               4\n5 Sent Friend Requests          3\n\n\nOnce again we have a character string for the date which we need to turn into a proper date. The wrinkle here is dates in the current year don’t specify the year in the log. We have to manually add it. The data is at a daily resolution, which is too granular for a clear picture at my level of activity. Let’s make it quarterly.\n\nfriend_table2 <- friend_table %>% \n  mutate(Date=if_else(str_length(Date)<7,paste0(Date,\", \",year(Sys.Date())),Date)) %>%\n  mutate(Date=parse_date(Date,format=\"%b %d, %Y\")) %>% \n  select(Category,Date) %>%\n  mutate(yearquarter=as.yearqtr(Date)) %>%\n  group_by(yearquarter)\ngg<-friend_table2 %>% ggplot(aes(x=as.Date(yearquarter),fill=Category))+ geom_bar(width=70)\ngg<-gg +labs(title=\"Facebook Friending Activity\",y=\"Count\",x=\"Quarterly\")\ngg\n\n\n\n\nNot too surprising. There was a lot of friending happening when I first joined Facebook. Perhaps a bit more curious is the recent renewed uptick in friending. There has been an influx of renewed high school aquaintances.\nSometimes it is useful too look at the balance of opposites. For example, we can see the balance of the number of friendings vs. the number of delete friend requests by assigning a negative number to deletions. There is no simple way to do this with native dplyr functions, though there should be. Base R is actually better at transforming just certain elements in a column based on some condition. Fortunately, I found a super-useful bit of code on Stack Overflow, mutate_cond(), that does exactly what we need.\n\nmutate_cond <- function(.data, condition, ..., envir = parent.frame()) {\n  #change elements of a column based on a condition\n  #https://stackoverflow.com/questions/34096162/dplyr-mutate-replace-on-a-subset-of-rows/34096575#34096575\n  condition <- eval(substitute(condition), .data, envir)\n    condition[is.na(condition)] = FALSE\n    .data[condition, ] <- .data[condition, ] %>% mutate(...)\n    .data\n}\n\n# tabulate sums of categories by quarter\nfriend_table3 <- friend_table %>% \n  mutate(Date=if_else(str_length(Date)<7,paste0(Date,\", \",year(Sys.Date())),Date)) %>%\n  mutate(Date=parse_date(Date,format=\"%b %d, %Y\")) %>% \n  mutate(yearquarter=as.yearqtr(Date)) %>%\n  select(Category,yearquarter) %>%\n  group_by(Category,yearquarter) %>% \n  summarise(count=n())\n\n`summarise()` has grouped output by 'Category'. You can override using the\n`.groups` argument.\n\n#make deleted requests negative\ngg<-friend_table3 %>% \n  mutate_cond(Category==\"Deleted Friend Requests\",count=-count) %>% \n  filter(Category==\"Deleted Friend Requests\" | Category==\"Friends Added\") %>% \n  ggplot(aes(x=as.Date(yearquarter),y=count,fill=Category))+ geom_col() \n\ngg<-gg +labs(title=\"Facebook Friending Activity\",y=\"Count\",x=\"Quarterly\")\ngg\n\n\n\n\nIt seems that adding friends is associated with deleted requests. I’ll surmise that when I show up in a new friend’s network that will spark some friend requests from their network. Some, maybe most, will be from people I don’t actually know and I will reject. There are spikes in rejections because I let them stack up before I notice them.\n\n\nWrapping Up\nWell that was cool. We got to try a lot of things. HTML parsing, date functions, sentiment analysis, text mining and lots of dplyr manipulations. Like a lot of projects, once I got going I thought of many things to try beyond the initial scope. That’s where the fun is when you’re not on deadline and deliverables. Thanks for making it all the way through. Hopefully this gives you some ideas for your own explorations. Now you try!\n#Double Bonus! Making the cool speedometer at the top of this post\nBecause we love cool visualizations, let’s show my mood on a silly gauge. I won’t show the low-level code I used to generate it because it’s basically a copy of what you can find here: http://www.gastonsanchez.com/. Gaston is a visualization guru extrordinaire. This shows how far you can take base R graphics.\nWhat do you think your gauge would look like? If it’s in the red call the suicide prevention hotline.\nThe animation is created using the magick package. I am thrilled that this recent release brings the image magick program inboard to R so we no longer have to run an external program to render animated files like GIFs (which I insist on pronouncing with a hard ‘G’, BTW.)\n\n# create animated mood gif for top of notebook.\nlibrary(magick)\nmy_days_moods<-word_scores_by_weekday %>%\n  summarise(mood=mean(value))\n\n#interpolate to create more points for a smooth animation.\n# the trick is to create a series where the mood stays constant for a number of frames\n# then transitions smoothly to the next mood value. Examine interp_moods to see how.\ninterp_moods<-tibble(doy=unlist(lapply(levels(my_days_moods$doy),rep,10)),\n                         mood_label=round(unlist(lapply(my_days_moods$mood,rep,10)),2),\n                         mood=approx(x=1:14,unlist(lapply(my_days_moods$mood,rep,2)),n=70)$y)\n\n\ninterp_moods$mood_label<- paste(ifelse(interp_moods$mood_label>0,\"+\",\"\"),interp_moods$mood_label)\n\n# I'll spare you the details of the low-level code.\n# see it at http://www.gastonsanchez.com/.\nsource(\"g_speedometer.r\")\n\nimg <- image_graph(600, 400, res = 96)\nfor(n in 1:nrow(interp_moods)){\n  plot_speedometer(label=interp_moods$doy[n],\n                   value=round(interp_moods$mood[n],2),\n                   bottom_label=interp_moods$mood_label[n],\n                   min=-0.5,\n                   max=0.5)\n  text(-0.1,1.0,\"Faceboook Mood-o-Meter\",cex=1.3)\n}\ndev.off()\nimg <- image_background(image_trim(img), 'white')\nanimation <- image_animate(img, fps = 10)\nimage_write(animation, path = \"moods.gif\", format = \"gif\")"
  },
  {
    "objectID": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html",
    "href": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html",
    "title": "Live Fast, Die Young, Stay Pretty?",
    "section": "",
    "text": "Live fast, die young, stay pretty? That’s the stereotype for rockers, or it was. We only need to look at Keith Richards, over 70 and going strong, to find a major counterexample. Do rockers die young? What do they die of? How does that compare to the broader population (in the U.S., anyway). It turns out there are some suprising answers to those questions.\n\n\nAlong the way we’ll learn something about web scraping, html parsing and some ggplot2 tricks. We use the tidyverse dialect throughout, just so you know."
  },
  {
    "objectID": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html#a-note-on-pointers.",
    "href": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html#a-note-on-pointers.",
    "title": "Live Fast, Die Young, Stay Pretty?",
    "section": "\nA note on “pointers.”\n",
    "text": "A note on “pointers.”\n\n\nR and the rvest package have some great functions for converting html <table>s into data frames. rvest is a very powerful package but one thing I learned is that it works with pointers to the data rather than the actual data. C programmers and old geezers like me will be familiar with this. I remember pop and push and stacks and all that stuff from the old days. R generally doesn’t pass values “by reference.” It passes “by value.” That’s why when you modify data in the scope of a function it doesn’t affect the value of the data outside unless you assign it with return <data>. Using pointers in rvest functions means modifications to html data happen without an explicit assigment.\n\n\nConsider a trival example:\n\n#usual R behavior\nmy_function<- function(x){return (x+1)}\ndata=3\n`#this doesn't change data but it would if data was passed by reference\nmy_function(data)\n`# [1] 4\ndata\n`# [1] 3\n`#this does change data in the usual R way\ndata<-my_function(data)\ndata\n`# [1] 4\n\nIf we were passing values “by reference” my_function(data) would change data without the need to assign it back to data. That’s how rvest works.\n\n\nWe use this behavior to combine the tables in the two Wikipedia articles into one html page by extracting the tables in the second wiki article and making them xml siblings of the tables in the first.\n\n\nAlternatively, we could load the two pages, extract the tables separately and combine them later but this is trickier!\n\n#join pre-2010 to post 2010\ndeath_page<-death_page1\ndeath_page_child<-death_page1 %>% xml_children() %>% .[2]\ndeath_page2_child<-death_page2 %>% xml_children() %>% .[2]\n#create one big web page by adding all the first level children from the second\n#page to the first.\n#This modifies death_page by changing the list of pointers associated with it.\nxml_add_sibling(death_page_child,death_page2_child)\nwrite_html(death_page,file=\"death_page.html\")"
  },
  {
    "objectID": "posts/2018-02-26-new-winter-sports-for-new-countries/2018-02-26-new-winter-sports-for-new-countries.html",
    "href": "posts/2018-02-26-new-winter-sports-for-new-countries/2018-02-26-new-winter-sports-for-new-countries.html",
    "title": "New Winter Sports for New Countries",
    "section": "",
    "text": "Norway is a tiny country that punches way above its weight in the Winter Olympic medal count. We are not surprised as those folks are practically born on skis. At the same time, toussle-haired surfer dudes and dudettes from the US seem to be all over the hill when snowboards are involved. Notably, the sports where the US is most visible are sports which arose fairly recently. Is there a pattern here? Let’s do a quick hit to see if we can visualize the dominance of countries, not by event, but by vintage of a sport’s introduction to the games.\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(knitr)"
  },
  {
    "objectID": "posts/2018-02-26-new-winter-sports-for-new-countries/2018-02-26-new-winter-sports-for-new-countries.html#a-digression",
    "href": "posts/2018-02-26-new-winter-sports-for-new-countries/2018-02-26-new-winter-sports-for-new-countries.html#a-digression",
    "title": "New Winter Sports for New Countries",
    "section": "\nA Digression\n",
    "text": "A Digression\n\n\nA best practice with tidy data is to have every observation and every variable in a single data table. Where we want to use the data in a related table we use _join to add the data to the main table. This runs contrary to best practice in the early days of PC databases where “relational” was a big part of data manipulation. The data tables were kept separate and linked by keys. Keys are still how _join works, of course, but we just make one humongous table rather than look up the related fields on the fly. This is faster but uses more memory and/or storage. Back in the day when a couple megabytes of RAM was a lot, we cared about those things, even for small data projects. Now, we use local million-row tables with nary a blink of the eye. You kids don’t know how tough it was!"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "",
    "text": "A hallmark of mayoral administration of NYC Mayor Bill DeBlasio has been free pre-K for all New York families. When the program was initially rolled out there were complaints in some quarters that upper-income neighborhoods were getting more slots.\n\n\nThis is an exploration comparing income to pre-K seats by neighborhoods. It was done mainly to help me practice with the whole workflow of data gathering, document parsing, and data tidying - plus making cool bi-variate choropleth maps! I had to invent a novel method in R to get a good looking bivariate legend onto the chart.\n\n\nThanks to Joshua Stevens for the inspiration and color theory of bi-variate maps (http://www.joshuastevens.net/cartography/make-a-bivariate-choropleth-map/). Thanks to Ari Lamstein for the awesome suite of choropleth packages (http://www.arilamstein.com/).\n\n\nIn my original version I use an outside program, PDFTOTEXT.EXE, to get parseable text out of the PDF documents at the NYC.gov web site. I share the commented out code for this here but skip the step in the notebook to save run time. Instead, I load the raw converted text files to illustrate the parsing.\n\n\nA further complication is to directly grab the income and population data from the census bureau requires an API key. You’ll have to get your own here: . I comment out the relevant lines but instead provide the raw downloaded data sets to illustrate how they get manipulated.\n\n\nNOTE: This analysis was originally done back in 201. The data is from that time. The URLs for city’s directories have changed and so too have the formats. The web scraping routines need to be modified accordingly.\n\n\nYou can find the raw data in CSV format at https://github.com/apsteinmetz/PreK."
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#use-the-acs-package-to-construct-the-queries-for-census-api",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#use-the-acs-package-to-construct-the-queries-for-census-api",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nUse the acs package to construct the queries for census api\n",
    "text": "Use the acs package to construct the queries for census api\n\n# -----------------------------------------------------------------------\n# get census data on children and income\n#census api key\n#see acs package documentation\n#api.key.install('your key here')\n\n# NYC county codes\nnyc_fips = c(36085,36005, 36047, 36061, 36081)\n#get the zips for all nyc counties\ndata(\"zip.regions\")\nnyc_zips<-data.frame(county.fips.numeric=nyc_fips)%>%inner_join(zip.regions)%>%select(region)%>%t\n# make an ACS geo set\nnycgeo<- acs::geo.make(zip.code = nyc_zips)\n\n##Connect to census.gov Requires an API key. You can uncomment the lines below if you have a key. Otherwise skip to the next section to load the raw csv files which were prepared for this notebook.\n\n\n# Household Household income is table 190013, per capita income is 19301\n#income<-acs::acs.fetch(endyear=2011,geography=nycgeo,table.number=\"B19013\")\n# #get relevant data into a data frame format\n#inc<-cbind(acs::geography(income),acs::estimate(income))\n# kidsUnder3<-acs::acs.fetch(endyear=2011,geography=nycgeo,table.number=\"B09001\",keyword = \"Under 3\")\n# kids<-cbind(acs::geography(kidsUnder3),acs::estimate(kidsUnder3))\n# totalPop<-acs.fetch(endyear=2011,geography=nycgeo,table.number=\"B01003\")\n# pop<-cbind(geography(totalPop),estimate(totalPop))\n\n##Alternatively, load from csv files …the data we would have otherwise gotten from census.gov. Comment this chunk out if you fetch the census data directly.\n\n#if we can't connect to census.gov\ninc<-read_csv('data/NYCincome.csv',col_types = \"ccd\")\nkids<-read_csv('data/NYCkids.csv',col_types = \"ccd\")\npop<-read_csv('data/NYCpopulation.csv',col_types = \"ccd\")\n\n##Massage the census data\n\nnames(inc)<-c(\"NAME\",\"zip\",\"HouseholdIncome\")\n#needs some cleanup of dupes. I don't know why\ninc<-distinct(select(inc,zip,HouseholdIncome))\n\n#kids under 3 in 2011 should approximate Pre-K kids in 2015\nnames(kids)<-c(\"NAME\",\"zip\",\"kidsUnder3\")\nkids<-distinct(select(kids,zip,kidsUnder3))\nkids<-kids %>% select(zip,kidsUnder3) %>% distinct() %>% filter(kidsUnder3!=0 | !is.na(kidsUnder3))\n\nnames(pop)<-c(\"NAME\",\"zip\",\"totPop\")\npop<-pop%>%select(zip,totPop)%>%distinct()%>%filter(totPop!=0)\n\ncensus<-pop%>%inner_join(kids)%>%inner_join(inc)%>%mutate(zip=as.character(zip))"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#look-at-some-preliminary-pictures-from-the-census",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#look-at-some-preliminary-pictures-from-the-census",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nLook at some preliminary pictures from the census\n",
    "text": "Look at some preliminary pictures from the census\n\n\nSo now we have some census data. We can use the ‘chorplethr’ package to easily create some meaningful maps. Let’s look at where the kids are and what incomes are in NYC Zip codes. Note that the ‘choroplethr’ package requires the inputs to be in a data frame where the geographic identifier is labeled “region” and the data to be displayed is labeled “value.”\n\n#where are zips with the most rugrats?\nkidsChor <- census %>% \n  transmute(region = zip, value = kidsUnder3 / totPop * 100)\nzip_choropleth(kidsChor, \n               zip_zoom = nyc_zips, \n               title = \"Percentage of Kids Under 3 in 2011\")\n\n\n\nincomeChor <- census %>% \n  transmute(region = zip, \n            value = HouseholdIncome)\nzip_choropleth(incomeChor, \n               zip_zoom = nyc_zips, \n               title = \"Household Income 2011\")\n## Warning in self$bind(): The following regions were missing and are being\n## set to NA: 10174, 10119, 11371, 10110, 10271, 10171, 10177, 10152, 10279,\n## 10115, 11430, 10111, 10112, 10167, 11351, 11359, 11424, 11425, 11451,\n## 10169, 10103, 10311, 10153, 10154, 10199, 10165, 10168, 10278, 10020,\n## 10173, 10170, 10172"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#download-pdfs-from-nyc.gov",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#download-pdfs-from-nyc.gov",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nDownload PDFs from NYC.gov\n",
    "text": "Download PDFs from NYC.gov\n\n\nDownload the PDFs then convert to text using an outside program, PDFTOTEXT.EXE (http://www.foolabs.com/xpdf/home.html).\n\n\n# # -----------------------------------------------------------------------\n# # get NYC data on pre-K programs\n# # scan seat directory pdfs and put into a data frame by zip code\n# #DOE pre-k directories\n# urls<- c(\"http://schools.nyc.gov/NR/rdonlyres/1F829192-ABE8-4BE6-93B5-1A33A6CCC32E/0/2015PreKDirectoryManhattan.pdf\",\n#          \"http://schools.nyc.gov/NR/rdonlyres/5337838E-EBE8-479A-8AB5-616C135A4B3C/0/2015PreKDirectoryBronx.pdf\",\n#          \"http://schools.nyc.gov/NR/rdonlyres/F2D95BF9-553A-4B92-BEAA-785A2D6C0798/0/2015PreKDirectoryBrooklyn.pdf\",\n#          \"http://schools.nyc.gov/NR/rdonlyres/B9B2080A-0121-4C73-AF4A-45CBC3E28CA3/0/2015PreKDirectoryQueens.pdf\",\n#          \"http://schools.nyc.gov/NR/rdonlyres/4DE31FBF-DA0D-4628-B709-F9A7421F7152/0/2015PreKDirectoryStatenIsland.pdf\")\n# \n# #assumes pdftotext.exe is in the current directory.  Edit as necessary\n# exe <- \"pdftotext.exe\"\n# \n# #regex to parse address line\n# pkseattokens <-\"(Address: )([.[:alnum:]- ()]+),+ ([0-9]{5})([a-zA-Z .()-:]+) ([0-9]{1,4}) (FD|HD|AM|PM|5H)\"\n# \n# # each of the PDF directories have 27 pages of intro material. Skip it. This might change for different years. Check PDFs\n# firstPage = 28\n# \n# dests <- tempfile(str_match(urls,\"Directory(\\\\w.+).pdf\")[,2],fileext = \".pdf\")\n# txt<- NULL\n# for (i in 1:length(urls)) {\n#   download.file(urls[i],destfile = dests[i],mode = \"wb\")\n#   # pdftotxt.exe is in current directory and convert pdf to text using \"table\" style at firstpage\n#   result<-system(paste(exe, \"-table -f\", firstPage, dests[i], sep = \" \"), intern=T)\n#   # get txt-file name and open it  \n#   filetxt <- sub(\".pdf\", \".txt\", dests[i])\n#   txt <- append(txt,readLines(filetxt,warn=FALSE))\n# }"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#alternatively-import-and-combine-the-already-converted-text-files.",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#alternatively-import-and-combine-the-already-converted-text-files.",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nAlternatively, import and combine the already converted text files.\n",
    "text": "Alternatively, import and combine the already converted text files.\n\nboroughList <- c('Manhattan','Bronx','Brooklyn','Queens','Staten')\ntxt<-NULL\nfor (borough in  boroughList){\n  # get txt-file name and open it  \n  filetxt <- paste(\"data/\",borough, \".txt\", sep='')\n  txt <- append(txt,readLines(filetxt,warn = FALSE))\n}"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#extract-relevant-info-from-text-files",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#extract-relevant-info-from-text-files",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nExtract relevant info from text files\n",
    "text": "Extract relevant info from text files\n\n\nPull out the Zip, seat count and day length of each school. Note the pretty heroic (for me, anyway) regular expression, “pkseattokens.”\"\n\n# find address line which contains zip and seat count\ntxt2<-txt[grep(\"Address:\",txt)]\n# strip chars that will mess up regex\npkseattokens <-\"(Address: )([.[:alnum:]- ()]+),+ ([0-9]{5})([a-zA-Z .()-:]+) ([0-9]{1,4}) (FD|HD|AM|PM|5H)\"\ntxt2<-sub(\"'\",\"\",txt2)\nschools<-as_data_frame(str_match(txt2,pkseattokens))[,c(4,6,7)]\nnames(schools)<-c(\"zip\",\"seats\",\"dayLength\")\n#have to convert from factor to character THEN to integer.  Don't know why\nschools$seats<-as.integer(as.character(schools$seats))\n\n# aggregate seat count by zip code\nsumSeats <- schools %>% \n  group_by(zip) %>% \n  summarise(count = n(), \n            numSeats = sum(seats, na.rm = TRUE))\n  names(sumSeats)<-c(\"zip\",\"schools\",\"numSeats\")\n\nSo we go from this: \n\n\nthen to this:\n\ntxt[1:3]\n## [1] \"    District 1: Full-Day Pre-K Programs                                                                                      You may apply to these programs online, over the phone, or at a Family Welcome Center.\"\n## [2] \"\"                                                                                                                                                                                                                   \n## [3] \"    Bank Street Head Start (01MATK)                                                                                                            Other School Features         2015   2014 Lowest\"\n\nand then to this:\n\ntxt2[1:3]\n## [1] \"    Address: 535 East 5th Street, 10009 (East Village)                               Phone:    212-353-2532                                    Breakfast/Lunch/Snack(s)      40 FD  N/A\"\n## [2] \"    Address: 280 Rivington Street, 10002 (Lower East Side)                           Phone:    212-254-3070                                    Breakfast/Lunch/Snack(s)      40 FD  N/A\"\n## [3] \"    Address: 180 Suffolk Street, 10002 (Chinatown)                                   Phone:    212-982-6650                                    Breakfast/Lunch/Snack(s)      29 FD  N/A\"\n\n…and finally to this:\n\nschools[1:3,]\n## # A tibble: 3 x 3\n##   zip   seats dayLength\n##   <chr> <int> <chr>    \n## 1 10009    40 FD       \n## 2 10002    40 FD       \n## 3 10002    29 FD\n\nMan, I love when the regex works! Magic!"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#look-at-some-preliminary-pictures-from-the-pre-k-data",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#look-at-some-preliminary-pictures-from-the-pre-k-data",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nLook at some preliminary pictures from the pre-K data\n",
    "text": "Look at some preliminary pictures from the pre-K data\n\n\nNot all the programs are full day. Are there a lot of schools offering shorter programs? We won’t use this data further in our analysis, but lets look at how many seats are full day vs. something else. Full day is the overwhelming majority.\n\n#how do the programs break out in terms of day length?\nsumDayLength<-schools%>%group_by(dayLength)%>%summarise(NumSchools=n(),NumSeats=sum(seats,na.rm=TRUE))\nggplot(sumDayLength,aes(x=dayLength,y=NumSeats)) + geom_col() +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\nWhere are the most schools? Where are the most seats? We might assume this pictures look the same, and they do.\n\n# some preliminary pictures\nsumSeats %>% transmute(region = zip, value = schools) %>%\n  zip_choropleth(zip_zoom = nyc_zips, \n                 title = \"Number of Schools\")\n\n\n\nsumSeats %>% transmute(region=zip,value=numSeats) %>% \n  zip_choropleth(zip_zoom = nyc_zips,\n                 title = \"Number of Pre-K Seats\")\n## Warning in super$initialize(zip.map, user.df): Your data.frame contains the\n## following regions which are not mappable: 11249, 11376, NA\n## Warning in self$bind(): The following regions were missing and are being\n## set to NA: 10464, 11040, 10280, 10174, 10017, 10119, 11371, 10110, 10271,\n## 11003, 11370, 10171, 10069, 10162, 10177, 10152, 10279, 10115, 10005,\n## 10111, 10112, 10167, 11351, 11359, 11424, 11425, 11451, 10006, 10169,\n## 10103, 10311, 10153, 10154, 10199, 10165, 10168, 10278, 10020, 10173,\n## 10170, 10172, 11005"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#create-the-custom-legend.",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#create-the-custom-legend.",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nCreate the custom legend.\n",
    "text": "Create the custom legend.\n\n\nTo create the legend we ‘simply’ create a heat map of the 3x3 bins in the map and label the axes appropriately. Then, using ‘cowplot’, shove it into a corner of the map. There are other ways we could use, but they don’t look nearly as nice.\n\n#first create a legend plot\nlegendGoal = melt(matrix(1:9, nrow = 3))\nlg <- ggplot(legendGoal, aes(Var2, Var1, fill = as.factor(value))) + geom_tile()\nlg <- lg + scale_fill_manual(name = \"\", values = bvColors)\nlg <- lg + theme(legend.position = \"none\")\nlg <- lg + theme(axis.title.x = element_text(size = rel(1), color = bvColors[3])) + \n  xlab(\" More Income -->\")\nlg <- lg + theme(axis.title.y = element_text(size = rel(1), color = bvColors[3])) + \n  ylab(\"   More Seats -->\")\nlg <- lg + theme(axis.text = element_blank())\nlg <- lg + theme(line = element_blank())\nlg\n\n\n\n\nAbove we see the legend as a custom rolled heat map. There is no data in it, just a matrix corresponding to the bin indices in the zip code map. We assign colors to match."
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#put-both-plots-on-a-grid",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#put-both-plots-on-a-grid",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nPut both plots on a grid\n",
    "text": "Put both plots on a grid\n\n\nNow we have the map in the ‘gg’ variable and the legend in the ‘lg’ variable. ‘ggdraw()’ and ‘draw_plot()’ are the ‘cowplot’ functions that let us create the canvas. We tweak the location and size parameters for rendering the legend element until it looks nice inset with the map.\n\n# put the legend together with the map\n# further annotate plot in the ggplot2 environment\n#strip out the ugly legend\ngg<-bvc$render()  + theme(legend.position=\"none\")\nggdraw() + draw_plot(lg,0.2,0.5,width=0.2,height=0.35) + \n  draw_plot(gg)\n\n\n\n\nThis map shows clearly where the low income, well served areas of the city are and that the swanky manhattan zip codes have the fewest free pre-K seats per child."
  },
  {
    "objectID": "posts/2019-02-04-rick-and-morty-palettes/2019-02-04-rick-and-morty-palettes.html",
    "href": "posts/2019-02-04-rick-and-morty-palettes/2019-02-04-rick-and-morty-palettes.html",
    "title": "Rick and Morty Palettes",
    "section": "",
    "text": "This was just a fun morning exercise. Let’s mix multiple images to make a palette of their principal colors using k-means. We’ll also use the totally awesome list-columns concept to put each image’s jpeg data into a data frame of lists that we can map to a function that turns the jpeg data into a list of palette colors in a new data frame.\n\n\nThis more-or-less copies http://www.milanor.net/blog/build-color-palette-from-image-with-paletter/ with the added twist of using multiple images before creating the palette. We’ll also get into the weeds a bit more with dissecting the images. I wanted to see if some cartoon show palettes using this method matched those in the ggsci package. Did the authors use the algorithmic approach I will use here? Will my approach look any better? Don’t know. I decided to use “Rick and Morty” because my kids like it. I would certainly never watch such drivel. I’m a scientist.\n\n\nFor the record, the one pop culture derived palette I really like is the Wes Anderson palette and on CRAN. These are presumably lovingly curated and created, not like the ones created by the stupid robot I use here.\n\n\nThe drawback to using K-means to create palettes from images is that it’s likely that none of the colors created are actually in the image. They just represent the mathematical centers of the clusters of colors.\n\n\nLoad libraries.\n\nlibrary(tidyverse)\nlibrary(jpeg) #import images\nlibrary(scales) #just for for the show_col() function\nlibrary(ggsci) #to compare my palettes to its palettes\nlibrary(ggfortify) #to support kmeans plots\nlibrary(gridExtra) #multiple plots on a page\n\nLoad mulitple images. They are all Google image search thumbnails so the size is the same. This matters since we are combining images. A larger image would have a disproportional weight in our analysis.\n\n\nI first thought that, since I am combining multiple images to get one palette, I needed to tile the images then process. No. We just care about the pixel color values so it really doesn’t matter what position they are in. The most efficient approach is to just chain all the RGB values together. Duh. Still we want to do some work with the individual images so let’s label them.\n\nrm_list<-list()\nfor (n in 1:6){\n  img<-jpeg::readJPEG(paste0(\"img/rm\",n,\".jpg\"))\n  R<-as.vector(img[,,1])\n  G<-as.vector(img[,,2])\n  B<-as.vector(img[,,3])\n  rm_list<-bind_rows(data_frame(img=n,R,G,B),rm_list) %>% \n    arrange(img)\n}\n\nrm_list <- left_join(rm_list,\n                     data_frame(\n                     img = c(1, 2, 3, 4, 5, 6),\n                     name = c(\"Schwifty\",\"Portal\",\"Cable\",\n                     \"Family\", \"Outdoor\", \"Wedding\")\n                     ))\n\n\nShow Me What You Got\n\n\nI chose the images from Google image search to be representative of varying but typical scenes.\n\n\n Cable\n\n\n Family\n\n\n Wedding\n\n\n Outdoor\n\n\n Portal\n\n\n Schwifty\n\n\nFor fun let’s do some density plots of the color values.\n\n#make data tidy first\nrm_tidy <- rm_list %>% gather(\"color\",\"level\",-img,-name)\nggplot(rm_tidy,aes(x=level,fill=color))+\n  geom_density(alpha=0.7) + \n  scale_fill_manual(values=c(\"blue\",\"green\",\"red\")) + \n  theme_void()\n\n\n\n\nWe can see some evidence of bimodality, a preference for very bright and very dark hues. Red is more often cranked to the max, while blue is much more evenly distributed. Perhaps that is typical of the limited palette of cartoons or just a function of the small number of frames I chose.\n\nggplot(rm_tidy,aes(x=level,fill=color))+\n  geom_density(alpha=0.7) + \n  scale_fill_manual(values=c(\"blue\",\"green\",\"red\")) + \n  facet_wrap(~name)+\n  theme_void()\n\n\n\n\nIt’s interesting to compare “Cable” with “Family.” Both images share the same backdrop but “Family” is much darker.\n\n\n\n\nMake the Palettes\n\n\nWhen I was a kid with watercolors I wanted to come up with a name for the filthy color that resulted when I mixed all the colors together. I called it (trigger warning) “Hitler” (but, really, brown). What is the color that results when we average all the RGB values? What named R colors resemble it? It looks to me like it’s between “cornsilk4”\" and “darkkhaki.”\"\n\nblend_color<-rm_list %>% \n  summarise(R=mean(R),G=mean(G),B=mean(B)) %>% \n  rgb()\n\nshow_col(c(\"cornsilk4\",blend_color,\"darkkhaki\"))\n\n\n\n\nLet’s call it “desertkhaki” which, hopefully, is not a trigger word.\n\n\nNow, for the fun part. In the Wes Anderson palette set, each movie get’s a different palette. Let’s make palettes for each of the images, which I chose for their distinctiveness.\n\n\nFor me, the good thing about open source is that I can stand on the shoulders of giants in the community. R also makes very muscular analysis trivally simple. On the other hand, it makes “script kiddies” like me potentially dangerous. I can only describe k-means in the most general terms but can run it in a snap.\n\nnum_colors = 16\npal_schwifty <- rm_list %>% \n  filter(name==\"Schwifty\") %>% \n  select(R,G,B) %>% \n  kmeans(centers = num_colors, iter.max = 30) %>% \n  .$centers %>% \n  rgb()\n\nshow_col(pal_schwifty)\n\n\n\n\nFor data plotting the separation between some of these colors is too small. I think 9 colors will suffice.\n\nnum_colors = 9\npal_schwifty <- rm_list %>% \n  filter(name==\"Schwifty\") %>% \n  select(R,G,B) %>% \n  kmeans(centers = num_colors, iter.max = 30) %>% \n  .$centers %>% \n  as.tibble() %>% \n  {.}\n## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics).\n## This warning is displayed once per session.\nshow_col(rgb(pal_schwifty))\n\n\n\n\nFor plotting purposes I would like use these colors in order of intensity. Sorting colors is a topic in itself but here we’ll do it quick and simple.\n\npal_schwifty %>% \n  mutate(saturation=rowSums(.[1:3])) %>% \n  arrange(saturation) %>% \n  rgb() %>% \n  show_col()\n\n\n\n\nThat’s about right. Let’s put it all together. Go through all the images to create a series of palettes.\n\n\n#function to turn a table of RGB values to an ordered list of colors\ngen_pal <- function(rgb_table) {\n  num_colors = 9\n  pal <- rgb_table %>%\n  select(R, G, B) %>%\n  kmeans(centers = num_colors, iter.max = 30) %>%\n  .$centers %>%\n  as.tibble() %>%\n  mutate(saturation = rowSums(.[1:3])) %>%\n  arrange(saturation) %>%\n  rgb()\n  return(pal)\n}\n#now make list columns, which are totally awesome, for each palette\npalette_rick<-rm_list %>% \n  group_by(name) %>% \n  select(-img) %>% \n  nest(.key=\"rgb\") %>% \n  transmute(name=name,pal= map(rgb,gen_pal))\npalette_rick\n## # A tibble: 6 x 2\n##   name     pal      \n##   <chr>    <list>   \n## 1 Schwifty <chr [9]>\n## 2 Portal   <chr [9]>\n## 3 Cable    <chr [9]>\n## 4 Family   <chr [9]>\n## 5 Outdoor  <chr [9]>\n## 6 Wedding  <chr [9]>\n#a function to extract the individual palettes, given a name.\n\nextract_pal<-function(palette_list,pal_name){\n  pal<-palette_list %>% filter(name==pal_name) %>% \n    select(pal) %>% \n    unlist() %>% \n    as.vector()\n  return(pal)\n}\nplot_one<-function(pal_name){\n  tmp <- palette_rick %>% unnest() %>% filter(name==pal_name)\n  g<- ggplot(tmp,aes(pal,fill=pal)) + geom_bar() + \n  scale_fill_manual(values=tmp$pal,guide=F) +\n  theme_void()+ggtitle(pal_name)\n  return (g)\n  \n}\n\nlapply(palette_rick$name,plot_one) %>% \n  grid.arrange(grobs=.)\n\n\n\n\nFinally, let’s do what we said we’d do at the beginning, put all these images together and add it to our list column of palettes.\n\nmulti_img_pal <- gen_pal(rm_list)\npalette_rick<-data_frame(name=\"all\",pal=list(multi_img_pal)) %>% bind_rows(palette_rick)\nshow_col(multi_img_pal)\n\n\n\n\nNot too bad. I’m glad something resembling Rick’s hair makes it into the list. Compare it to the ggsci package Rick and Morty palette. Here we see the weaknesses of an algorithmic approach. ggsci is more interesting since it has more color diversity and vividness. I assume they were hand selected. You can see Rick’s hair and Morty’s shirt color.\n\nshow_col(ggsci::pal_rickandmorty()(9))\n\n\n\n\nSince the (rather flimsy) point of this excercise is to make palettes for data graphics, let’s make some plots.\n\n#use the example in help for dplyr::gather\nstocks <- data.frame(\n  time = as.Date('2009-01-01') + 0:9,\n  W = rnorm(10, 0, 1),\n  X = rnorm(10, 0, 1),\n  Y = rnorm(10, 0, 2),\n  Z = rnorm(10, 0, 4)\n)\nstocksm <- stocks %>% gather(stock, price, -time)\n\nggplot(stocksm,aes(time,price,color=stock))+geom_line(size=2)+\n  scale_color_manual(values = multi_img_pal) + theme_minimal()\n\n\n\nggplot(stocksm,aes(time,price,color=stock))+geom_line(size=2) +\n  theme_minimal() +\n  scale_color_manual(values = extract_pal(palette_rick,\"Wedding\"))\n\n Arguably, the perceptual differnces among the colors are less than ideal, even if the colors are pleasing. We might take the additional step of hand-selecting colors from a larger generated palette that are more suitable for plots.\n\n\n\n\nOne more thing…\n\n\nBack to the k-means analysis. When we created these palettes we were really assigning colors to the centers of the clusters of near neigbors in the a 2D space. This is a form of principal components analysis (PCA). Let’s visualize those clusters. The ggplot::autoplot() function makes this trivally easy. While we are at it, let’s crank up the number of colors to 20.\n\nnum_colors = 20\n#assign each pixel to a cluster\nkm <-  rm_list[c(\"R\",\"G\",\"B\")] %>% kmeans(centers = num_colors, iter.max = 30)\nrm_PCA<-prcomp(rm_list[c(\"R\",\"G\",\"B\")])\n\nrm_list <- rm_list %>% mutate(cluster=as.factor(km$cluster))\nautoplot(rm_PCA, x=1,y=2,data = rm_list, colour = \"cluster\",\n         loadings = TRUE, loadings.colour = 'blue',\n         loadings.label = TRUE, loadings.label.size = 10) +\n  scale_color_manual(values=rgb(km$centers),guide=FALSE)+\n  theme_classic()\n\n This is every pixel colored by it’s cluster assignment and plotted. It’s clear that the x-dimension, which happens to explain 74% of the color variance, is luminosity, with darker shades on the right. The other dimension seems to be related to hue.\n\n\nWe can make it clear by plotting the second and third principal component.\n\nrm_list <- rm_list %>% mutate(cluster=as.factor(km$cluster))\nautoplot(rm_PCA, x=2,y=3,data = rm_list, colour = \"cluster\",\n         loadings = TRUE, loadings.colour = 'blue',\n         loadings.label = TRUE, loadings.label.size = 10) +\n  scale_color_manual(values=rgb(km$centers),guide=F)+\n  theme_classic()\n\n\n\n\nNow it’s quite clear that the second and third principal components map to the color space even though this explains only about 25% of the variation in the data.\n\n\nFeel free to get schwifty with these palettes!"
  },
  {
    "objectID": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#choose-the-questions-to-use",
    "href": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#choose-the-questions-to-use",
    "title": "Where Are The Libertarians?",
    "section": "\nChoose the Questions to Use\n",
    "text": "Choose the Questions to Use\n\n\nWhile Mr. Drutman’s analysis of the survey did not show many libertarian-leaning voters, I hoped that selecting my own set of questions narrowly focused on the relevant issues might provide more support for my point of view. So, to be honest, I went into this with a preconceived notion of the answer I wanted to get. Beware.\n\n\nOut of the dozens of questions the survey asked, I pulled out those which seemed to go to the separate dimensions of the conservative/liberal spectrum. The questions involved:\n\n\nFiscal Issues\n\n\n\nTrust of the government in Washington\n\n\nAmount of regulation of business by the government\n\n\nImportance of reducing the federal deficit\n\n\nRole of government in economy\n\n\nDesired third party position on economic issues\n\n\n\nSocial Issues\n\n\n\nDifficulty of foreigners to immigrate to US\n\n\nGender Roles “Women belong in the kitchen!”\n\n\nViews about the holy scriptures of own religion, literal truth?\n\n\nOpinion on gay marriage\n\n\nPublic restroom usage of transgender people\n\n\nView on abortion\n\n\nDesired third party position on social and cultural issues"
  },
  {
    "objectID": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#pull-out-demographic-features",
    "href": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#pull-out-demographic-features",
    "title": "Where Are The Libertarians?",
    "section": "\nPull Out Demographic Features\n",
    "text": "Pull Out Demographic Features\n\n\nNow we massage the raw data a few ways. First we gather() the data to group the interesting demographic features as separate variables and tidy up all the remaining questions and answers into two variables.\n\nvoter_18<- gather(voter_18_raw,\"question\",\"answer\",\n                  -caseid,\n                  -pid3_2018,\n                  -race_2018,\n                  -gender_2018,\n                  -faminc_new_2018,\n                  -inputstate_2018\n                  ) %>% \n  as_tibble() %>%\n  filter(!is.na(caseid)) %>% \n  filter(!is.na(answer)) %>% \n  distinct()\n\n# labels of the questions we want to keep, with a (f)iscal or (s)ocial tag\nquestions_to_keep <-  read_csv(\n  \"axis_flag,question\\n\n  f,trustgovt_2018\\n\n  s,immi_makedifficult_2018\\n\n  f,tax_goal_federal_2018\\n\n  f,govt_reg_2016\\n\n  s,sexism1_2018\\n\n  s,holy_2018\\n\n  s,gaymar_2016\\n\n  s,abortview3_2016\\n\n  s,third_soc_2018\\n\n  f,third_econ_2018\\n\n  f,gvmt_involment_2016\\n\",trim_ws=T)\n\nvoter_18 <- voter_18 %>% filter(question %in% questions_to_keep$question)\n\nvoter_18 <- voter_18 %>% mutate(answer=as.numeric(answer))\n# make demographic variables factors\nvoter_18 <- voter_18 %>%\n  mutate(caseid =as.character(caseid)) %>% \n  mutate(gender_2018=as.factor(gender_2018)) %>% \n  mutate(race_2018=as.factor(race_2018)) %>% \n  mutate(faminc_new_2018=as.factor(faminc_new_2018)) %>% \n  mutate(pid3_2018=as.factor(pid3_2018)) %>% \n  rename(party_2018=pid3_2018) %>% \n  rename(state_2018=inputstate_2018) %>% \n  rename(income_2018=faminc_new_2018)\n  \n#map state numbers to state abbreviations\nstate_plus <- c(state.abb[1:8],\"DC\",state.abb[9:50])\nvoter_18$state_2018 <- factor(voter_18$state_2018)\nlevels(voter_18$state_2018) <- state_plus\n\n\nlevels(voter_18$gender_2018) <- c(\"Male\",\"Female\")\nlevels(voter_18$race_2018) <- c(\"White\",\"Black\",\"Hispanic\",\n                                \"Asian\",\"Native Amerian\",\"Mixed\",\n                                \"Other\",\"Middle Eastern\")\n\nlevels(voter_18$party_2018) <- c(\"Democrat\",\"Republican\",\"Independent\",\n                                \"Other\",\"Not Sure\")\n#Make human-readable income column\nincome_key<-read_csv(\n  \"Response,Label\\n\n  1, Less than $10\\n\n  2, $10 - $19\\n\n  3,  $20 - $29\\n\n  4,  $30 - $39\\n\n  5,  $40 - $49\\n\n  6,  $50 - $59\\n\n  7,  $60 - $69\\n\n  8,  $70 - $79\\n\n  9,  $80 - $99\\n\n  10,   $100 - $119\\n\n  11,   $120 - $149\\n\n  12,   $150 - $199\\n\n  13,   $200 - $249\\n\n  14,   $250 - $349\\n\n  15,   $350 - $499\\n\n  16,   $500 or more\\n\n  97,   Prefer not to say\\n\"\n  ,col_types = \"ff\",trim_ws = TRUE)\n\nvoter_18 <- voter_18 %>% mutate(income_2018_000=income_2018)\nlevels(voter_18$income_2018_000)<-levels(income_key$Label)\n\n# now make income_2018 continuous again, keeping income_2018_000 as a factor\n# for labeling\n# \"Prefer not to say\" (coded as 97) is set to NA.   \nvoter_18 <- voter_18 %>% mutate(income_2018=ifelse(income_2018==97,NA,income_2018)) %>%\n                    mutate(income_2018=as.numeric(income_2018))\nvoter_18[1:10,]\n## # A tibble: 10 x 9\n##    caseid gender_2018 race_2018 income_2018 party_2018 state_2018 question\n##    <chr>  <fct>       <fct>           <dbl> <fct>      <fct>      <chr>   \n##  1 38248~ Female      Hispanic            7 Democrat   CA         trustgo~\n##  2 38216~ Female      White               8 Republican AZ         trustgo~\n##  3 38216~ Male        White               6 Independe~ WI         trustgo~\n##  4 38233~ Male        White               7 Republican TX         trustgo~\n##  5 38248~ Female      White               5 Democrat   CA         trustgo~\n##  6 38329~ Male        White              NA Republican WI         trustgo~\n##  7 38222~ Female      White               3 Democrat   VT         trustgo~\n##  8 38233~ Female      White              12 Independe~ FL         trustgo~\n##  9 38226~ Female      White               9 Democrat   AZ         trustgo~\n## 10 38216~ Female      White               6 Independe~ NE         trustgo~\n## # ... with 2 more variables: answer <dbl>, income_2018_000 <fct>\n# We did a lot of work.  Save it.\nsave(voter_18,file=\"data/voter_18.rdata\")\n# free up 30mb of memory\nrm(voter_18_raw)\n\nLook at some of the demographics.\n\ndemographics <- voter_18 %>% \n  distinct(caseid,.keep_all = TRUE) %>% \n  select(-question,-answer)\n\ndemographics %>% group_by(gender_2018) %>%\n  summarise(count=n()) %>% kable()\n\n\n\n\n\ngender_2018\n\n\ncount\n\n\n\n\n\n\nMale\n\n\n2762\n\n\n\n\nFemale\n\n\n3239\n\n\n\n\n\ndemographics %>%\n  ggplot(aes(race_2018))+geom_bar()+coord_flip() + \n  labs(caption = \"Source: VoterStudyGroup.org\")"
  },
  {
    "objectID": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#rescale-answers-for-consistency",
    "href": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#rescale-answers-for-consistency",
    "title": "Where Are The Libertarians?",
    "section": "\nRescale Answers for Consistency\n",
    "text": "Rescale Answers for Consistency\n\n\nThe final step in massaging the data is to rescale all the question answers to between one and minus one, interpreted as liberal to conservative, respectively, in two dimensions. “Don’t know” (usually coded as 8) is treated as neutral (zero). If the question is “fiscal”, set “social” to NA and vice versa.\n\n#add two new columns to hold scaled answers.\nvoter_18_scaled<-voter_18 %>% mutate(fiscal=NA,social=NA)\n# -1 is fiscal liberal\nvoter_18_scaled <- voter_18_scaled %>% \n  mutate(fiscal=ifelse(question==\"trustgovt_2018\",-(answer-2),fiscal))\n\n# -1 is social liberal\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(social=ifelse(question==\"immi_makedifficult_2018\",(answer-3)*0.5,social))\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(social=ifelse(question==\"immi_makedifficult_2018\",\n                       ifelse(answer==8,0,social),social))\n\n# -1 is fiscal liberal\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(fiscal=ifelse(question==\"tax_goal_federal_2018\",(answer-2.5)*-(2/3),fiscal))\n\n# -1 is fiscal liberal\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(fiscal=ifelse(question==\"govt_reg_2016\",-(answer-2),fiscal))\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(fiscal=ifelse(question==\"govt_reg_2016\",\n                       ifelse(answer==8,0,fiscal),fiscal))\n\n# -1 is social liberal\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(social=ifelse(question==\"sexism1_2018\",(answer-2.5)*-(2/3),social))\n\n# -1 is social liberal\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(social=ifelse(question==\"holy_2018\",-(answer-2),social))\n\n# -1 is social liberal\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(social=ifelse(question==\"gaymar_2016\",(answer-1.5)*2,social))\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(social=ifelse(question==\"gaymar_2016\",\n                       ifelse(answer==8,0,social),social))\n\n# -1 is social liberal\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(social=ifelse(question==\"view_transgender_2016\",(answer-1.5)*2,social))\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(social=ifelse(question==\"view_transgender_2016\",\n                       ifelse(answer==8,0,social),social))\n\n# -1 is social liberal\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(social=ifelse(question==\"abortview3_2016\",(answer-2),social))\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(social=ifelse(question==\"abortview3_2016\",\n                       ifelse(answer==8,0,social),social))\n\n# -1 is social liberal\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(social=ifelse(question==\"third_soc_2018\",(answer-3)*0.5,social))\n\n# -1 is fiscal liberal\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(fiscal=ifelse(question==\"third_econ_2018\",(answer-3)*0.5,fiscal))\n\n# -1 is fiscal liberal\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(fiscal=ifelse(question==\"gvmt_involment_2016\",(answer-1),fiscal))\nvoter_18_scaled <- voter_18_scaled %>%\n  mutate(fiscal=ifelse(question==\"gvmt_involment_2016\",\n                       ifelse(answer==8,0,fiscal),fiscal))\n\n# We did a lot of work.  Save it.\nsave(voter_18_scaled,file=\"data/voter_18_scaled.rdata\")\n\nNow we have values that we can aggregate for each question. They are all normalized and given equal weight. Should each question be given equal weight? I don’t know, but now we can compute average scores for each caseid (each one is one voter) . We also add the demographic features to each observation. So now every caseid in the survey is assigned a separate fiscal and social temperament score.\n\nscores <- voter_18_scaled %>% \n  group_by(caseid) %>% \n  summarise(social=mean(social,na.rm = T),fiscal=mean(fiscal,na.rm = T)) %>% \n  left_join(demographics)   #Add demographics to scores\n\nLet’s start off at the highest level. What are the mean values for each dimension?\n\nmean_social <- mean(scores$social,na.rm = T)\nmean_fiscal <- mean(scores$fiscal,na.rm = T)\nprint(paste(\"Mean Fiscal=\",round(mean_fiscal,2)))\n## [1] \"Mean Fiscal= 0.06\"\nprint(paste(\"Mean Social=\",round(mean_social,2)))\n## [1] \"Mean Social= -0.16\"\n\nWell that is an encouraging start. The signs are in the libertarian quadrant, anyway, but are they statistically significant? Specifically, can we reject the hypothesis that the true mean is greater than zero for social, and less than zero for fiscal?\n\nt_s <-t.test(scores$social,mu=0,conf.level = 0.95,alternative=\"greater\") %>% broom::tidy()\nt_f <-t.test(scores$fiscal,mu=0,conf.level = 0.95,alternative=\"less\") %>% broom::tidy()\nt_both<-bind_cols(Dimension=c(\"Social\",\"Fiscal\"),bind_rows(t_s,t_f)) %>% \n  select(Dimension,estimate,statistic,conf99.low=conf.low,conf99.high=conf.high)\nt_both\n## # A tibble: 2 x 5\n##   Dimension estimate statistic conf99.low conf99.high\n##   <chr>        <dbl>     <dbl>      <dbl>       <dbl>\n## 1 Social     -0.157      -24.6     -0.167    Inf     \n## 2 Fiscal      0.0585      11.4   -Inf          0.0669\n\nWith such a large sample size we can be pretty confident that the true mean is close to the sample mean and therefore leans libertarian. Alas, that is not enough to form an opinion. The magnitudes are still very small and a slight relative shift in the aggregate may not support my hypothesis that most people have a libertarian bias when you break down the issues. Further, we haven’t even touched on the survey methodology. It is an online survey and therefore means the respondents have computers and are facile with internet access. That population is closer and closer to “everyone” with each passing day but is still not universal.\n\n\nWith our data all cleaned up, let’s look at some pictures!\n\ngg <- ggplot(scores,aes(fiscal,social)) + geom_point()\ngg <- gg +  geom_jitter(width=0.05,height=0.05)\ngg <- gg + geom_hline(yintercept = 0,color=\"red\")\ngg <- gg + geom_vline(xintercept = 0,color=\"red\")\ngg <- gg + annotate(\"text\",label=c(\"Libertarian\"),x=0.9,y=-0.9,color=\"red\")\ngg <- gg + labs(title=\"Separation of Social and Fiscal Values\",\n                y = \"Social Score (Lower=More Liberal)\",\n                x = \"Fiscal Score (Lower=More Liberal)\",\n                caption = \"Source: VoterStudyGroup.org\")\ngg <- gg + annotate(\"text\",x=-0.7,y=1.0,color=\"red\",\n                    label=paste(\"Mean Fiscal=\",round(mean_fiscal,2),\n                                \"Mean Social=\",round(mean_social,2)))\ngg <- gg + geom_smooth(method=\"lm\")\ngg\n\n\n\n\nThe first thing to note is the values are all over the chart. We’ve added some random “jitter” noise to the position of each point with geom_jitter(). Otherwise, many of the points would overlap and obscure the density of the points. Even so, careful scrutiny of of the standard error range around the regression line shows that a huge number of points lie very close to the line.\n\n\nSadly, for a libertarian, the scores tend to line up close to the 45 degree axis, which means people who are more socially conservative are more likely to be fiscally conservative as well. The libertarian quadrant is the lower right, which is more sparsely populated.\n\nlm(scores$social~scores$fiscal) %>% broom::tidy()\n## # A tibble: 2 x 5\n##   term          estimate std.error statistic   p.value\n##   <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)     -0.200   0.00518     -38.5 5.39e-290\n## 2 scores$fiscal    0.732   0.0129       56.7 0.\n\nLet’s count voter incidence in each quadrant.\n\n#call zero scores \"Neutral\"\nscores <- scores %>% \n  mutate(fiscal_label=cut(scores$fiscal,c(-1,-0.0001,0.0001,1),\n                      labels=c(\"Liberal\",\"Neutral\",\"Conservative\"))) %>% \n  mutate(social_label=cut(scores$social,c(-1,-0.01,0.01,1),\n                      labels=c(\"Liberal\",\"Neutral\",\"Conservative\")))\n\nxtabs(~fiscal_label+social_label,scores) %>% \n  as_tibble() %>% \n  arrange(desc(n)) %>% \n  filter(fiscal_label != \"Neutral\",social_label != \"Neutral\")\n## # A tibble: 4 x 3\n##   fiscal_label social_label     n\n##   <chr>        <chr>        <int>\n## 1 Liberal      Liberal       1903\n## 2 Conservative Conservative  1745\n## 3 Conservative Liberal       1046\n## 4 Liberal      Conservative   387\n\nThe largest quadrant is Liberal/Liberal followed by Conservative/Conservative. Leaving out the neutral axes, the libertarian quadrant (liberal social, conservative fiscal) is third with a respectable number of respondents. This is about 18% of the sample, far more than the 4% Mr. Drutman found. The liberal fiscal, conservative social quadrant, which is populist I suppose, includes the fewest voters.\n\n\nThis is suggestive of traditional party platforms so how does this look broken out by party?\n\ngg <-ggplot(scores,aes(fiscal,social,color=party_2018))+geom_point()\ngg <- gg +  geom_jitter(width=0.05,height=0.05)\ngg <- gg + geom_hline(yintercept = 0)\ngg <- gg + geom_vline(xintercept = 0)\ngg <- gg + annotate(\"text\",label=c(\"Libertarian\"),x=0.9,y=-0.9)\ngg <- gg + scale_color_manual(values=c(Republican='#e41a1c',\n                                       Democrat='#377eb8',\n                                       Independent='#4daf4a',\n                                       Other='#984ea3',\n                                       `Not Sure`='#ff7f00'))\ngg <- gg + labs(title=\"Party Lines Align With Temperament\",\n                y = \"Social Score (Lower=More Liberal)\",\n                x = \"Fiscal Score (Lower=More Liberal)\",\n                caption = \"Source: VoterStudyGroup.org\")\ngg\n\n\n\n\nThere is a clear bifurcation around party, which is exactly what we’d expect.\n\n\nThe survey respondents are overwhelmingly white. What does the plot look like if we remove them from data set?\n\ngg <- scores %>% filter(race_2018 != \"White\") %>% \n  ggplot(aes(fiscal,social,color=race_2018))+geom_point()\ngg <- gg +  geom_jitter(width=0.05,height=0.05)\ngg <- gg + geom_hline(yintercept = 0)\ngg <- gg + geom_vline(xintercept = 0)\ngg <- gg + annotate(\"text\",label=c(\"Libertarian\"),x=0.9,y=-0.9)\ngg <- gg + labs(title=\"Minorities Are Not Too Different from Whites\",\n                y = \"Social Score (Lower=More Liberal)\",\n                x = \"Fiscal Score (Lower=More Liberal)\",\n                caption = \"Source: VoterStudyGroup.org\")\ngg\n\n\n\n\nThe sub sample above looks very similar to the whole data set. Black voters do skew more to the Liberal/Liberal side but Hispanic voters do not.\n\n\nLet’s meet some individuals. Who are the folks who show strong libertarian sentiments (greater than 0.5 social, less than -0.5 fiscal), all nineteen of them?\n\nscores %>% filter(fiscal < (0.5),social > (-0.5)) %>% select(gender_2018,race_2018,party_2018,income_2018_000,state_2018)\n## # A tibble: 2,984 x 5\n##    gender_2018 race_2018 party_2018  income_2018_000 state_2018\n##    <fct>       <fct>     <fct>       <fct>           <fct>     \n##  1 Male        White     Independent $150 - $199     MI        \n##  2 Male        White     Independent $60 - $69       SD        \n##  3 Male        White     Republican  $100 - $119     OK        \n##  4 Female      White     Republican  $10 - $19       WI        \n##  5 Male        White     Republican  $30 - $39       CA        \n##  6 Female      White     Republican  $40 - $49       WA        \n##  7 Female      White     Republican  $10 - $19       IN        \n##  8 Female      Hispanic  Democrat    $50 - $59       NY        \n##  9 Female      White     Independent $30 - $39       MD        \n## 10 Female      White     Independent $120 - $149     MA        \n## # ... with 2,974 more rows\n\nThese folks are almost all white, but our set is a tiny sub sample so I doubt any generalizations are significant. There is only one Democrat in the bunch. They are not rich and they’re spread around the country. They are men and women.\n\n\nWe have a number of additional demographic variables but let’s just look at one more of them. How do scores look conditioned on income?\n\ngg <- scores %>% filter(!is.na(income_2018)) %>%\n  ggplot(aes(income_2018_000,fiscal,group=income_2018)) + geom_boxplot()\ngg <- gg + coord_flip() + theme(axis.text.x = element_text(angle=-90))\ngg <- gg + geom_hline(yintercept = 0,color=\"red\")\ngg <- gg + labs(title=\"Higher Income Does  Not Make a Fiscal Conservative\",\n                x = \"Annual Income ($000)\",\n                y = \"Fiscal Score (Lower=More Liberal)\",\n                caption = \"Source: VoterStudyGroup.org\")\ngg\n\n\n\n\nSurprisingly, to me, there is no trend to prefer less government as income rises. The desire for government involvement in the economy is close to neutral across all income cohorts. Note, I did not include any tax questions for this measure. People are happy to favor higher taxes on anybody who makes more money than they do.\n\ngg <- scores %>% filter(!is.na(income_2018)) %>%\n  ggplot(aes(income_2018_000,social,group=income_2018))+ geom_boxplot()\n\ngg <- gg + coord_flip() + theme(axis.text.x = element_text(angle=-90))\ngg <- gg + geom_hline(yintercept = 0,color=\"red\")\ngg <- gg + labs(title=\"Higher Income Does Make One More Socially Liberal\",\n                x = \"Annual Income ($000)\",\n                y = \"Social Score (Lower=More Liberal)\",\n                caption = \"Source: VoterStudyGroup.org\")\ngg\n\n\n\n\nThere is some association with more socially liberal views as income rises. The richer you are, the more tolerant you are of other’s lifestyles, I guess. During the 2016 election there was some questioning around why poor people (mainly rural whites) voted “against their economic interest.” This suggests that voting WITH their conservative social interests was more important (I am not saying that our current president embodies conservative social values). Most pundits put a racial angle on this. In all income cohorts the median voter is at least a shade liberal on social issues."
  },
  {
    "objectID": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#how-much-does-location-matter",
    "href": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#how-much-does-location-matter",
    "title": "Where Are The Libertarians?",
    "section": "\nHow Much Does Location Matter?\n",
    "text": "How Much Does Location Matter?\n\n\nLet’s look at average scores by state. To remind us of the influence that larger states have on the overall numbers we’ll grab population data from the Census Bureau. There are a number of R packages to access the census API but those are more than we need and require an API key. Here, we’ll just grab a summary CSV file from the web site.\n\n# download population summary from census bureau\nstate_pop_raw<-read_csv(\"https://www2.census.gov/programs-surveys/popest/datasets/2010-2018/national/totals/nst-est2018-alldata.csv\")\n## Parsed with column specification:\n## cols(\n##   .default = col_double(),\n##   SUMLEV = col_character(),\n##   REGION = col_character(),\n##   DIVISION = col_character(),\n##   STATE = col_character(),\n##   NAME = col_character()\n## )\n## See spec(...) for full column specifications.\nsave(state_pop_raw,file=\"data/state_pop_raw.rdata\")\nload(\"data/state_pop_raw.rdata\")\n# filter to keep only state level data and add abbreviations\n# manually insert District of Columbia as a state\nstate_pop <- state_pop_raw %>% \n  transmute(state=NAME,population_2018=POPESTIMATE2018) %>%\n  filter(state %in% c(state.name,\"District of Columbia\")) %>%\n  bind_cols(state_2018=as_factor(state_plus))\n\ngg <- scores %>% group_by(state_2018) %>% \n  summarize(fiscal=mean(fiscal,na.rm = T),social=mean(social,na.rm = T)) %>%\n  left_join(state_pop, by = \"state_2018\") %>% \n  ggplot(aes(fiscal,social)) + geom_point(aes(color=population_2018,\n                                              size=population_2018))\ngg <- gg + ggrepel::geom_text_repel(aes(label=state_2018))\ngg <- gg + scale_size(trans=\"log10\",\n                      labels=c(\"0\",\"1 mm\",\"3 mm\",\"10 mm\",\"30 mm\",\"More\"))\ngg <- gg + scale_color_gradient(trans=\"log10\",\n                                labels=c(\"0\",\"1 mm\",\"3 mm\",\"10 mm\",\"30 mm\",\"More\"))\ngg <- gg + geom_hline(yintercept = 0)\ngg <- gg + geom_vline(xintercept = 0)\ngg <- gg + annotate(\"text\",label=c(\"Libertarian\"),x=0.15,y=-0.4)\ngg <- gg + labs(title=\"Separation of Social and Fiscal Values\",\n                y = \"Social Score (Lower=More Liberal)\",\n                x = \"Fiscal Score (Lower=More Liberal)\",\n                caption = \"Source: VoterStudyGroup.org\")\n\ngg\n\n\n\n\nIf I had created this chart first I might have been excited. It shows that the average voter in most states is in the libertarian quadrant. That is NOT the same thing as saying most voters in the “libertarian” states are libertarian. We already showed that the vast majority of voters fall outside the libertarian quadrant. Still, there are some interesting things to note. The fiscal sentiments of New Hampshire voters are far different than their Vermont neighbors. I don’t see Bernie Sanders sporting this license plate:\n\n\n\n\nLive Free or Die\n\n\n\nBy the way, I wish I knew how to get color and size combined into one legend."
  },
  {
    "objectID": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#my-last-attempt-at-validation",
    "href": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#my-last-attempt-at-validation",
    "title": "Where Are The Libertarians?",
    "section": "\nMy Last Attempt at Validation\n",
    "text": "My Last Attempt at Validation\n\n\nI went through the YouGov.com survey and picked out the questions I feel are relevant, a highly subjective exercise. Even so,the results do not support my belief that maybe a plurality of people have libertarian sensibilities. But there were hints that gave me some hope.\n\n\nFirst, there is a clear yearning for a choice beyond the existing parties as this question shows:\n\n\nIn your view, do the Republican and Democratic parties do an adequate job of representing the American people, or do they do such a poor job that a third major party is needed?\n\n\n\n\n\n\nCount\n\n\nAnswer\n\n\n\n\n\n\n1,851\n\n\nDo adequate job\n\n\n\n\n4,036\n\n\nThird party is needed\n\n\n\n\n\n\nThe fact that most people want another choice tells us nothing about what that choice is. Another question does seem to suggest libertarian economic sentiment in excess of what the number of Republicans might indicate:\n\n\nIn general, do you think there is too much or too little regulation of business by the government?\n\n\n\n\n\n\nCount\n\n\nAnswer\n\n\n\n\n\n\n3,473\n\n\nToo much\n\n\n\n\n1,628\n\n\nAbout the right amount\n\n\n\n\n1,999\n\n\nToo little\n\n\n\n\n871\n\n\nDon’t know\n\n\n\n\n\n\nFinally, there are two questions in the survey that go explicitly to the separation of social and fiscal values.\n\n\n1. If you were to vote for a new third party, where would you like it to stand on social and cultural issues—like abortion and same-sex marriage?\n\n\n2. If you were to vote for a new third party, where would you like it to stand on economic issues—like how much the government spends and how many services it provides?\n\n\nThe range of answers for both is:\n\n\n\n\n\n\nScore\n\n\nAnswer\n\n\n\n\n\n\n1.0\n\n\nMore liberal than the Democratic Party\n\n\n\n\n0.5\n\n\nAbout where the Democratic Party is now\n\n\n\n\n0.0\n\n\nIn between the Democratic Party and the Republican Party\n\n\n\n\n-0.5\n\n\nAbout where the Republican Party is now\n\n\n\n\n-1.0\n\n\nMore conservative than the Republican Party\n\n\n\n\n\n\nLet’s re-do the scatter based on the answers to just those two questions. Since we are using only two questions with possible values of only 1,0 and minus 1, there are many more respondents than possible values. Again we add some random jitter to make the density clear. Every dot within each square is actually the same value. The result is a visual cross tab. I quite like the effect.\n\nscores_narrow <- voter_18_scaled %>% \n  filter(str_detect(question,\"third_\")) %>%  \n  group_by(caseid) %>% \n  summarise(social=mean(social,na.rm = T),fiscal=mean(fiscal,na.rm = T)) %>% \n  left_join(demographics)\n\ngg <- ggplot(scores_narrow,aes(fiscal,social,color=party_2018))+geom_point() + geom_jitter()\ngg <- gg + geom_hline(yintercept = 0)\ngg <- gg + geom_vline(xintercept = 0)\ngg <- gg + labs(title=\"What Kind of Third Party Would Voters Prefer?\",\n                y = \"Social Score (Lower=More Liberal)\",\n                x = \"Fiscal Score (Lower=More Liberal)\",\n                caption = \"Source: VoterStudyGroup.org\")\ngg <- gg + annotate(\"text\",label=c(\"Libertarian\"),x=0.9,y=-0.9,fontface=\"bold\")\ngg <- gg + annotate(\"text\",label=c(\"Populist?\"),x=-0.9,y=0.9,fontface=\"bold\")\ngg <- gg + annotate(\"text\",label=c(\"Left of Democrats\"),x=-0.9,y=-0.9,fontface=\"bold\")\ngg <- gg + annotate(\"text\",\n                    label=c(\"Right of Republicans\"),\n                    x=0.9,y=0.9,\n                    fontface=\"bold\")\ngg <- gg + scale_color_manual(values=c(Republican='#e41a1c',\n                                       Democrat='#377eb8',\n                                       Independent='#4daf4a',\n                                       Other='#984ea3',\n                                       `Not Sure`='#ff7f00'))\n\ngg\n\n\n\n\nWhat I don’t like is the result. Contrary to my pre-conceived notion, it’s clear the American electorate is not crypto-libertarian. Rather, voters want a third party that is highly centrist or highly polarized along traditional liberal/conservative lines. This makes it unlikely that any single third party could be successful at the ballot box. Rather, both an extreme left-wing and an extreme right wing party could take votes away from the traditional parties. The Republican party is more hollowed out in its relative middle than the Democrats.\n\n\nCould Howard Schulz be something of a spoiler from the center? Possibly. There are a large number of voters who would like an alternative that is less intrusive than the Democrats on economic issues and less intrusive than the Republicans on moral issues. I disagree with the Times’ assessment that, since there are so few absolute libertarians, Schulz will not find a base. As we see, there are many people who lean toward the center and away from the extremes within their parties, even if they are not libertarian, per se. But, far too many people are happy with the status quo or would like their party more on the left or right to make this likely as we see below.\n\ntmp <-scores_narrow %>% \n  mutate(social_direction=cut(abs(social),breaks=c(-0.1,0.25,1.1),\n                    labels=c(\"To the Center\",\n                             \"Status Quo or More Extreme\"))) %>% \n  mutate(fiscal_direction=cut(abs(fiscal),breaks=c(-0.1,0.25,1.1),\n                    labels=c(\"To the Center\",\n                             \"Status Quo or More Extreme\")))\n\n\n\nxtabs(~social_direction+fiscal_direction,tmp) %>% as_tibble() %>% kable()\n\n\n\n\n\nsocial_direction\n\n\nfiscal_direction\n\n\nn\n\n\n\n\n\n\nTo the Center\n\n\nTo the Center\n\n\n1211\n\n\n\n\nStatus Quo or More Extreme\n\n\nTo the Center\n\n\n1116\n\n\n\n\nTo the Center\n\n\nStatus Quo or More Extreme\n\n\n496\n\n\n\n\nStatus Quo or More Extreme\n\n\nStatus Quo or More Extreme\n\n\n3037"
  },
  {
    "objectID": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#conclusion",
    "href": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#conclusion",
    "title": "Where Are The Libertarians?",
    "section": "\nConclusion\n",
    "text": "Conclusion\n\n\nI started this exercise hoping to find some support for my personal views among the broader electorate. Sadly, I didn’t find much. The strongest statement I can make is there is a slight bias among both Republicans and Democrats for more centrist policies. But the fun of data science is finding things you didn’t expect and in validating or refuting hunches and feelings with good science. I know something today I didn’t know yesterday so I’ll call it a win!\n\n\nUPDATE 2/8/2019: Based on feedback, I changed party colors and left/right positions to those most Americans are accomstomed to.\n\nsessionInfo()\n\n## R version 3.5.1 (2018-07-02)\n## Platform: x86_64-w64-mingw32/x64 (64-bit)\n## Running under: Windows 10 x64 (build 17134)\n## \n## Matrix products: default\n## \n## locale:\n## [1] LC_COLLATE=English_United States.1252 \n## [2] LC_CTYPE=English_United States.1252   \n## [3] LC_MONETARY=English_United States.1252\n## [4] LC_NUMERIC=C                          \n## [5] LC_TIME=English_United States.1252    \n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] bindrcpp_0.2.2  knitr_1.21      forcats_0.3.0   stringr_1.3.1  \n##  [5] dplyr_0.7.8     purrr_0.3.0     readr_1.3.1     tidyr_0.8.2    \n##  [9] tibble_2.0.1    ggplot2_3.1.0   tidyverse_1.2.1\n## \n## loaded via a namespace (and not attached):\n##  [1] tidyselect_0.2.5 xfun_0.4         haven_2.0.0      lattice_0.20-35 \n##  [5] colorspace_1.4-0 generics_0.0.2   htmltools_0.3.6  yaml_2.2.0      \n##  [9] utf8_1.1.4       rlang_0.3.1      pillar_1.3.1     glue_1.3.0      \n## [13] withr_2.1.2      modelr_0.1.2     readxl_1.2.0     bindr_0.1.1     \n## [17] plyr_1.8.4       munsell_0.5.0    blogdown_0.10    gtable_0.2.0    \n## [21] cellranger_1.1.0 rvest_0.3.2      evaluate_0.12    labeling_0.3    \n## [25] curl_3.3         fansi_0.4.0      highr_0.7        broom_0.5.1     \n## [29] Rcpp_1.0.0       scales_1.0.0     backports_1.1.3  jsonlite_1.6    \n## [33] hms_0.4.2        digest_0.6.18    stringi_1.2.4    ggrepel_0.8.0   \n## [37] bookdown_0.9     grid_3.5.1       cli_1.0.1        tools_3.5.1     \n## [41] magrittr_1.5     lazyeval_0.2.1   crayon_1.3.4     pkgconfig_2.0.2 \n## [45] xml2_1.2.0       lubridate_1.7.4  assertthat_0.2.0 rmarkdown_1.11  \n## [49] httr_1.4.0       rstudioapi_0.9.0 R6_2.3.0         nlme_3.1-137    \n## [53] compiler_3.5.1"
  },
  {
    "objectID": "posts/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times.html",
    "href": "posts/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times.html",
    "title": "Solving the Letterboxed Puzzle in the New York Times",
    "section": "",
    "text": "What is the difference between “computer programming” and “data science?” To someone not invovled in either they look much the same. Most data scientists are also coders, though they don’t need to be. Data scientists (especially amateurs like me) don’t need to be concerned with pointers, stacks, heaps, recursion, etc., but this is not a data science post.\n\n\nFor this post, I go back to my roots in the 1980s as an amateur computer scientist to solve a new New York Times puzzle called “Letterboxed.” In particular I employ recursion to build a tree of possible solutions. This exercise reminded me how languages like R allow such easy plug-ins to high-powered algorithms written by “real” computer scientists in “real” languages like C++. Data scientists stand on the shoulders of giants who wrote the low-level code.\n\n\nI’ll confess, I don’t like playing games and doing puzzles much. I also take the fun out of it for other people. When someone gave my kids “Cards Against Humanity” as a gift, I went through the deck and removed all the really filthy cards (the kids were relieved to see I left in plenty of poop references). When I see a puzzle I immediately think about an algorithm to play or solve it.\n\n\n\n\nSample board from The Times\n\n\n\nIn “Letterboxed” the object is to string words together that use all the letters in the square using as few words as possible by tracing the spelling of each word in the square. You must start each new word with the last letter of the previous word and you may not use any consecutive letters that lie on the same side of the square. In the example above, “NIL” and “LAP” would not be permitted. “TORN” followed by “NEAR” would be fine.\n\n\nToday we will forsake the usual data science workflow of: ask a question, source data, clean data, explore data and, finally, analyze data. Those proceed in a linear (in practice, circular) fashion but here we’ll go over the functions that do specific subroutines to generate and solve these puzzles.\n\n\nThe steps we’ll take are\n\n\n\nGenerate the puzzle with random letters.\n\n\nDraw the board.\n\n\nSolve the puzzle.\n\n\nPrint the answer that solves the puzzle in the fewest words.\n\n\n\nGenerating the puzzle is the easy part.\n\n\nThe first task is to generate the puzzle with random letters. It would be cruel to place no requirement to use vowels so we also specify a minimum number of vowels. We sample the required number of consonants and vowels and assign them to each segment of the polygon. The default is four sides with two consonants and one vowel per side.\n\n\nJust to be cute, let’s write the function so we can optionally expand the geometry of the puzzle to an arbitrary number of sides and number of letters per side, not just a square as we see in The Times.\n\n\nIf you are playing along at home, delete the set.seed() line in the code below after you have established you get the same results I do or you will get the same puzzle every time you call generate_puzzle.\n\n# letterboxed game\nlibrary(tidyverse)\nlibrary(wfindr)\nlibrary(gtools)\n\nsides <- 4\nletters_per_side <- 3\nvowels <- c(\"a\",\"e\",\"i\",\"o\",\"u\")\nconsonants <- letters[!(letters %in% vowels)]\n\n# scrabble dictionary from wfinder. You can subsitute any list\n# you desire, in any language.\nword_list <- words.eng\n\n# ------------------------------------------------------------\ngenerate_puzzle <- function(sides=4,letters_per_side=3,\n                            vowel_count=4,replacement = FALSE){\n  set.seed(1234567) # DELETE THIS LINE OR YOU WILL GET THE SAME PUZZLE EVERY TIME\n  if(sides < 4){\n    print(\"Minimum Side is 4, changing to 4\")\n    sides = 4\n  }\n  if (vowel_count < sides) replacement=TRUE\n  if (vowel_count > length(vowels)) replacement=TRUE\n  use_vowels <- sample(vowels,vowel_count,replace = replacement)\n  use_consonants <- sample(consonants,letters_per_side*sides-vowel_count,replace = replacement)\n  # deal out the letters\n  letter = NULL\n  vowels_used = 1\n  consonants_used = 1\n  spot = 1\n  for (i in 1:letters_per_side){\n    for(j in 1:sides){\n      # don't put vowel at edge of side but it's just cosmetic\n      if (i == 2 & vowels_used <= vowel_count){\n        letter[spot] <- use_vowels[vowels_used]\n        vowels_used <- vowels_used + 1\n        spot <- spot + 1\n      } else{\n        letter[spot] <- use_consonants[consonants_used]\n        consonants_used <- consonants_used + 1      \n        spot <- spot + 1\n        \n      }\n    }\n  }\n  puzzle <- tibble(side=rep(1:sides,letters_per_side),\n                   spot=unlist(map(1:letters_per_side,rep,sides)), \n                   letter=letter) %>% arrange(side,spot)\n  return(puzzle)\n}\n\n# let's see what this does\ngenerate_puzzle()\n## # A tibble: 12 x 3\n##     side  spot letter\n##    <int> <int> <chr> \n##  1     1     1 v     \n##  2     1     2 i     \n##  3     1     3 l     \n##  4     2     1 m     \n##  5     2     2 u     \n##  6     2     3 r     \n##  7     3     1 c     \n##  8     3     2 o     \n##  9     3     3 y     \n## 10     4     1 t     \n## 11     4     2 a     \n## 12     4     3 f\n\nNow we have a data frame with twelve random letters, including four vowels, assigned to one of three spots on four sides.\n\n\nIt’s not necessary to solve the puzzle, but it would be nice to draw the puzzle in the style that appears in The Times. If all we needed to do was make a square the task of drawing it would be trivial but, as noted above, I can’t leave well enough alone. If we want to make polygons of arbitrary sizes we need to do a bit of trigonometry. First we generate the vertices of our polygon, then the points on each segment where the letters will go (as an aside, I say “vertices,” the proper Latin plural. The “newspaper of record” abandoned Latin plurals a decade ago. It grinds my gears to see the Times printing “vertexes”).\n\n# -------------------------------------------------------------\nget_polygon <- function(sides=4){\n  x_center <- 0\n  y_center <- 0\n  radius <- 5\n  y <- NULL\n  x <- NULL\n  angle = 3.925\n  angle_increment <-  2 * pi / sides\n  for (i in 1:sides){\n    x[i] = x_center + radius * cos(angle)\n    y[i] = y_center + radius * sin(angle)\n    angle = angle + angle_increment\n  }\n  #close figure\n  x[i+1] <- x[1]\n  y[i+1] <- y[1]\n  return(data.frame(x=x,y=y))\n}\n# -------------------------------------------------------------\nget_points_on_segment <- function(end_points,num_points){\n  # poin tdistance is fraction of segment length\n  a <- as.numeric(end_points[1,])\n  b <- as.numeric(end_points[2,])\n  # Use atan2!\n  th = atan2( b[2]-a[2] , b[1]-a[1] )\n  # length of segment AB\n  AB = sqrt( (b[2]-a[2])^2 + (b[1]-a[1])^2 )\n  AB_fraction <- AB / (num_points +1 )\n  # points equidistant on the line\n  AP = sapply(1:(num_points),function(x) x * AB_fraction)\n  # The points of interest\n  c = sapply(AP,function(d) c(x = a[1] + d*cos( th ),\n                              y = a[2] + d*sin( th ))) %>% \n    t() %>%\n    as.data.frame()\n  return(c)\n}\n# -----------------------------------------------------\nget_letter_coords <- function(puzzle,sides=4,letters_per_side=3){\n  \n  puzzle_shape <- get_polygon(sides)\n  puzzle<-lapply(1:(nrow(puzzle_shape)-1),\n                     function(p) get_points_on_segment(puzzle_shape[p:(p+1),],\n                                                       letters_per_side)) %>% \n    bind_rows() %>% \n    bind_cols(puzzle)\n  return(puzzle)\n}\n# -------------------------------------------------------------\ndraw_puzzle <-function(puzzle,sides=4,letters_per_side=3){\n  puzzle_shape <- get_polygon(sides)\n  gg <- puzzle_shape %>% ggplot(aes(x,y)) + geom_path() + coord_fixed() +\n    geom_point(data = puzzle,aes(x,y),size=20,color=\"white\") + \n    geom_text(data = puzzle,aes(x,y,label = letter),size=10) + \n    theme_void() + \n    theme(panel.background = element_rect(fill=\"pink\")) + \n    NULL \nreturn(gg)\n}\n\n# Draw puzzle sample\ngenerate_puzzle() %>%\n  get_letter_coords(sides=sides,letters_per_side = letters_per_side) %>% \n  draw_puzzle()\n\n\n\n\nRemember we designed the generator to work with arbitrary dimensions. Let’s try five sides with two letters per side.\n\ngenerate_puzzle(5,2) %>%\n  get_letter_coords(5,2) %>% \n  draw_puzzle(5,2)\n\n\n\n\nFun!\n\n\n\nSolve the Puzzle\n\n\nMuch of the grunt work is done by the wfinder package, which generates a word list from an aribtrary set of letters, as in Scrabble. Unlike Scrabble, we CAN reuse the same letter more than once. This package also contains a list of English words we use. You can substitute any word list you like, in any language. My Mom, whose native language was German, was the champion in our family. I always struggled even though I liked to brag about my high SAT verbal score. I am grateful to Mom for knocking me down a peg. Anyhoo, I am really in awe of the power of the grep function. Regexes are a dark art to me. The idea that a short line could find every possible word in an instant boggles (don’t like that game either) the mind. Suppose you pull the Scrabble tiles “ABAHRTY”.\n\ngrep(\"^[abahrty]*$\",word_list,value = T)\n##   [1] \"aa\"       \"aah\"      \"ab\"       \"aba\"      \"abaya\"    \"abb\"     \n##   [7] \"abba\"     \"abray\"    \"aby\"      \"ah\"       \"aha\"      \"ahh\"     \n##  [13] \"ar\"       \"araba\"    \"arar\"     \"arb\"      \"arba\"     \"arhat\"   \n##  [19] \"arrah\"    \"array\"    \"art\"      \"arty\"     \"ary\"      \"at\"      \n##  [25] \"atar\"     \"att\"      \"attar\"    \"ay\"       \"ayah\"     \"ba\"      \n##  [31] \"baa\"      \"baba\"     \"baby\"     \"bah\"      \"baht\"     \"bar\"     \n##  [37] \"barb\"     \"barra\"    \"barrat\"   \"barratry\" \"baryta\"   \"bat\"     \n##  [43] \"batata\"   \"bath\"     \"batt\"     \"batta\"    \"batty\"    \"bay\"     \n##  [49] \"bayt\"     \"bra\"      \"brat\"     \"bratty\"   \"bray\"     \"brr\"     \n##  [55] \"brrr\"     \"by\"       \"ha\"       \"haar\"     \"hah\"      \"haha\"    \n##  [61] \"harry\"    \"hart\"     \"hat\"      \"hath\"     \"hay\"      \"rabat\"   \n##  [67] \"rah\"      \"rat\"      \"rata\"     \"ratatat\"  \"rath\"     \"ratty\"   \n##  [73] \"ray\"      \"raya\"     \"rayah\"    \"rhy\"      \"rhyta\"    \"rya\"     \n##  [79] \"rybat\"    \"ta\"       \"tab\"      \"tabby\"    \"taha\"     \"tahr\"    \n##  [85] \"tar\"      \"tara\"     \"tarry\"    \"tart\"     \"tartar\"   \"tarty\"   \n##  [91] \"tat\"      \"tatar\"    \"tath\"     \"tatt\"     \"tatty\"    \"tay\"     \n##  [97] \"tayra\"    \"thar\"     \"that\"     \"thy\"      \"trat\"     \"tratt\"   \n## [103] \"tray\"     \"try\"      \"ya\"       \"yabby\"    \"yah\"      \"yar\"     \n## [109] \"yarr\"     \"yarta\"    \"yatra\"    \"yay\"\n\n112 words out of a corpus of over 260 thousand. Instantly. That’s all the code it takes? That’s nuts! That’s efficient low-level coding. wfindr wraps that bit of magic with some bells and whistles to aid with word puzzles. In particular it crafts regexes that conform to the rules of scrabble. The example above creates a word list that might use more of a letter than we have in our tiles. To fix that, the simple regex I show above gets converted to a much fancier one.\n\nmodel_to_regex(allow=\"abahrty\",type=\"scrabble\")\n## [1] \"(?=^((([^a]*a[^a]*){1,2})|([^a]*))$)(?=^((([^b]*b[^b]*){1,1})|([^b]*))$)(?=^((([^h]*h[^h]*){1,1})|([^h]*))$)(?=^((([^r]*r[^r]*){1,1})|([^r]*))$)(?=^((([^t]*t[^t]*){1,1})|([^t]*))$)(?=^((([^y]*y[^y]*){1,1})|([^y]*))$)^[abhrty]*$\"\n\nWhoa! Like I said. It’s regex is a dark art.\n\n\nNow we have all the possible words to use in the puzzle. Just throwing random words around from the solution set would eventually find some answers but we can do much better than that. To find the “best” next word, we can pick the word that has the most yet-unused letters. By default, the function below returns one word but it could return more. In practice, I found iterating through more words was rarely necessary to get a solution but drastically increased computation time and memory usage of the recursive function that calls it.\n\nfind_next_best_words <- function(w,needed_letters,max_return=1){\n  # the higher max_return is the more words will be traversed.  Careful,\n  # computation times will geometrically increase.\n  # puzzle_words is global\n  # find words that start with last letter of w\n  next_words<-puzzle_words[str_starts(puzzle_words,str_sub(w,-1))]\n  # prioritize words by greatest overlap with unused letters\n  next_word_chars <-  map(next_words,strsplit,split=\"\") %>% unlist(recursive = F)\n  temp <- map(next_word_chars,function(x) length(setdiff(needed_letters,x))) %>% unlist()\n  if (is.vector(temp)){\n    next_words <- next_words[order(temp)]\n    max_return <- min(length(next_words),max_return)\n    return(next_words[1:max_return])  \n  } else{\n    return()\n  }\n}\n# -----------------------------------------------------\n# check if we have used all the letters yet\ntest_needed_letters <- function(word_chain){\n  word_chain_chars <-  paste0(word_chain,collapse = \"\") %>% \n    strsplit(split=\"\") %>%\n    unlist() %>% \n    unique()\n  return(setdiff(all_puzzle_letters,\n                     word_chain_chars))\n}\n\nNow we come to the workhorse recursive function. “Recursive” just means it calls itself. I’ve learned the trick to recursive functions is getting out of them, otherwise you get deeper and deeper into the “Beyond” section of “Bed, Bath and Beyond” and run out of memory pretty quickly. At least nowadays you kids don’t have to worry about the whole machine crashing. You can just nuke the process that’s stuck.\n\n\nWe start by preparing to iterate make_chain over the full list of valid words. Naturally we expect to find a solution before traversing much of the list. We build the solution chain by choosing a word that ends with a letter that has not been an ending letter yet. Otherwise we might chase our tail forever if a solution doesn’t lie on that path. Then we pick the best next word as described above. Then we call make_chain again and again and again.\n\n\nHere we limit the solution chain to a maximum of five words. Each time make_chain is called we run some tests and climb back out of the recursive stack if one of these conditions has been met:\n\n\n\nThe chain is more than five words with no solution.\n\n\nA solution is found.\n\n\nWe run out of last letter/first letter possibilities\n\n\nThe are no next words found.\n\n\nmake_chain <- function(word_chain,used_last_letters){\n  needed_letters <- test_needed_letters(word_chain)\n  if (length(word_chain)>6){\n    # Come on, if you can't solve in 5 words, you suck!\n    return()\n  }\n  if (length(needed_letters)==0) {\n    # Yay! We have a solution.\n    return(list(word_chain))\n  }\n  else {\n    last_word <- tail(word_chain,1)\n    last_letter <-str_sub(last_word,-1L)\n    if (str_detect(used_last_letters,last_letter,negate=T)){\n      used_last_letters <- paste0(last_letter,used_last_letters,collapse = \"\")\n      next_word<-find_next_best_words(last_word,needed_letters,max_return=1)\n       if (length(next_word)>0){\n        word_chain <- make_chain(c(word_chain,next_word),used_last_letters)\n        } else {\n          # no next word found\n          return()\n        }\n    } else{\n      # start of next word would be a letter that has already been used\n      return()\n    }\n  }\n} \n\nThe function solve_puzzle is a wrapper around make_chain that first gets all the possible words that our letters allow, removing words that violate the rule of no consecutive letters from the same line. Note the use of the <<– assignment operator that accesses global variables from within functions. This practice is frowned upon in some circles but, since we are using nested recursion, I didn’t want to make new copies of every variable each time make_chain is called.\n\n# dplyr chain-friendly permuatations\nd_permute <- function(v, n, r,  set, repeats.allowed){\n  return(permutations(n, r, v, set, repeats.allowed))\n}\n\nget_line_combos <- function(a_side,puzzle){\n  combos <- puzzle %>% filter(side==a_side) %>% \n    pull(letter) %>% \n    as.character() %>% \n    d_permute(n=3,r=2,set=F,repeats.allowed = T) %>% \n    apply(1,paste0,collapse=\"\")\n  return(combos)\n}\n\n\nsolve_puzzle <- function (puzzle) {\n  # get all letter combos that are invalid because they lie on the same line segment\n  bans <- map(1:sides,get_line_combos,puzzle=puzzle) %>% unlist()\n  #get all possible words\n  puzzle_words <<- scrabble(paste0(puzzle$letter,collapse = \"\"),words=word_list)\n  length(puzzle_words)\n  #winnow out illegal ones\n  banned_words <- map(bans,function(x) puzzle_words[str_which(puzzle_words,x)]) %>% \n    unlist()\n  puzzle_words <<- puzzle_words[!(puzzle_words %in% banned_words)]\n  length(puzzle_words)\n  puzzle_words <<-puzzle_words[order(nchar(puzzle_words),decreasing = TRUE, puzzle_words)]\n  \n  \n  all_puzzle_letters <<- puzzle$letter %>% as.vector()\n  \n  solutions <- map(puzzle_words,make_chain,\"\") %>% unlist(recursive = F)\n  return(solutions)\n}\n\nWhew! Now let’s actually solve a puzzle. The solve_puzzle function returns a list of lists with all the found solutions.\n\nvowel_count <- sides\n# global variables\nall_puzzle_letters <- NULL\npuzzle_words <- NULL\npuzzle <- generate_puzzle(sides=sides,\n                          letters_per_side = letters_per_side,\n                          vowel_count = vowel_count)\n# add letter coordinates for plot\npuzzle <- get_letter_coords(puzzle,\n                            sides=sides,\n                            letters_per_side = letters_per_side)\n#draw_puzzle(puzzle)\nsolutions <- solve_puzzle(puzzle)\n\nsolutions %>% head()\n## [[1]]\n## [1] \"vortical\" \"loamy\"    \"yuca\"     \"aimful\"  \n## \n## [[2]]\n## [1] \"voracity\" \"ymolt\"    \"trayful\" \n## \n## [[3]]\n## [1] \"foulmart\" \"trifoly\"  \"yuca\"     \"avoutry\" \n## \n## [[4]]\n## [1] \"vacuity\" \"ymolt\"   \"trifoly\"\n## \n## [[5]]\n## [1] \"trayful\" \"lorica\"  \"avoutry\" \"ymolt\"  \n## \n## [[6]]\n## [1] \"flavory\" \"ymolt\"   \"toluic\"\n\nWe may have hundreds of solutions or none. You can look at the solutions variable to see all we found. The goal of The Times puzzle is to solve in the minimum number of words so we’ll take the solution with the least number of words (there may be many) and print that on the puzzle.\n\n# ---------------------------------------------------------\ndraw_solution <- function(puzzle, solutions){\n  if (is.null(solutions)) {\n    solution <- \"No Solution\"\n  } else {\n    ideal <- map(solutions,length) %>% unlist() %>% which.min()\n    solution <- c(solutions[[ideal]],paste(length(solutions)-1,\"other solutions\")) \n  }\n  gg <- draw_puzzle(puzzle)\n  gg <- gg + annotate(\"text\",x=0,y=0.9,label=paste(solution, collapse = \"\\n\"), size = 6)\n  print (gg)\n}\n\ndraw_solution(puzzle, solutions)\n\n\n\n\nLet’s go back to the image at the top of this post which is from The Times. We’ll use those letters to solve an actual puzzle. Do the puzzle authors generate the puzzles randomly or do they work backword from a selected word list? I have no idea.\n\nsample_letters <- \"taperdnilyco\"\npuzzle <- generate_puzzle() %>% get_letter_coords()\n#replace random letters with the one in the known puzzle\npuzzle$letter <- strsplit(sample_letters,split = NULL) %>% unlist()\nsolutions <- solve_puzzle(puzzle)\nsolutions %>% head()\n## [[1]]\n## [1] \"lectionary\" \"yealdon\"    \"noplace\"   \n## \n## [[2]]\n## [1] \"centroidal\" \"lectionary\" \"yipe\"      \n## \n## [[3]]\n## [1] \"rantipole\" \"etypical\"  \"leporid\"  \n## \n## [[4]]\n## [1] \"planetoid\" \"dielytra\"  \"article\"  \n## \n## [[5]]\n## [1] \"placitory\" \"yealdon\"  \n## \n## [[6]]\n## [1] \"clarionet\" \"torpidly\"\n\nWe found 851 solutions to this particular puzzle, quite a few. Furthermore, If you are really good, you could solve this puzzle with two words!\n\ndraw_solution(puzzle, solutions)\n\n\n\n\nThere you have it. You might grumble that too many of the words in the scrabble dictionary are not in your vocabulary. They certainly aren’t in mine. Feel free to use a shorter word list with more common words. Here are a bunch. That will increase the liklihood that no solution is found, though.\n\n\nFurther work that might be done would be to filter for completely unique solutions, with no overlapping words. Also we might create a Shiny application that does pretty animation drawing lines across the puzzle of the solution.\n\n\nNaturally, you should only use this code to check your answer. No cheating!"
  },
  {
    "objectID": "posts/2019-06-12-why-i-migrated-from-excel-to-r/2019-06-12-why-i-migrated-from-excel-to-r.html",
    "href": "posts/2019-06-12-why-i-migrated-from-excel-to-r/2019-06-12-why-i-migrated-from-excel-to-r.html",
    "title": "Why I migrated from Excel to R",
    "section": "",
    "text": "Old Spreadsheets\n\n\nI’ve been a spreadsheet power user from the days of Visicalc for the Apple ][. I migrated to Lotus 1-2-3, to Borland Quattro and finally to Excel. With Excel, I’ve bludgeoned Visual Basic to create some pretty complicated dashboards and analytics. When I started using R I used tools like RExcel that plug R into Excel as an analytic server, or I would use Excel to download data from investment databases and export it for use in R. But now I find I open up Excel only rarely and do all my quantitative investigations entirely within R. Why?\n\nSimplicity. R uses is a very different paradigm than a spreadsheet so it takes some getting used to. On the surface, R is a programming language, like C or Java. Spreadsheets were invented to free humans to get real analytic work done without becoming coders. Yet R comes at the coding angle from a very different direction than application languages. It is a data science tool first and a programming language second. Like a spreadsheet, the central unit of analytic work is data in a row and column format. Once you master the vernacular of manipulating these “data frames” the rest is easy. Like a spreadsheet, R is interactive. You try single operations and can paste the successful ones into your “program” in a sequential fashion, building up your analysis step by step. In doing so you are creating a log of your work that lets you pick up later where you left off without missing a beat or reuse bits in other projects. Most people use the R Studio development environment as their workbench. It’s simple, powerful and free!\nAuditability. Big Excel spreadsheets are a labyrinth of linked formulas. Tracing errors is extremely difficult. Noticing errors at all is often tough. They often go unnoticed for the life of the spreadsheet. When I come back to a spreadsheet I’ve created months ago, I often can’t remember how I did something. If something breaks, tracking down the broken piece takes a long time. So while spreadsheets are easy to master, mastery comes at the expense of maintainability. With R, each step of the analysis proceeds in a roughly linear fashion. Each piece building on the previous one. It is easy to see where the problems are and to insert the fixes without blowing up something else you didn’t realize was connected.\nReproducibility. How often do you share a big spreadsheet with someone else? Can they use it? Have you been bequeathed a spreadsheet that is part of the team workflow that you have no idea how to maintain? With R the logic of the analysis flows in discrete steps so every step is immediately visible. The code is its own log of all the work performed. That’s not to say you are off the hook for documentation. Well commented code is a sign of disciplined thinking and a courtesy to both others and your future self. Inserting a comment line in R code comes naturally, I find, while it requires conscious effort in Excel.\nShareability. “Notebooks” are the new thing and I love them. These are HTML documents, like this blog, that include descriptive text, R code (or whatever language you use) and the output of the code. They make sharing and showing off your work visually attractive and simple to follow. You can attach the HTML document to an email, render it as a PDF or publish it to a web site. Embedding markup language is a little extra work but the R Studio IDE creates the templates automatically, and the result is sharp.\nVisualization. It is easy to create charts in Excel. It is easy to create charts in R, though it is done in code, not interactively. What R can do that Excel can’t, is to go further to make great looking charts because of the customization that is possible. I won’t claim it is easy, though. The learning curve is steep, but rewarding.\n\n SOURCE\n\nScalability. As your data sets become larger, R scales with them. Excel becomes unwieldy.\nPlug-in packages. This is the single biggest reason to drop spreadsheets. There are hundreds of plug-in packages for R that extend its analytic power. They can all be installed with a couple clicks. Any new task I want to perform starts with asking if there is a package that will do it for me. The answer is almost always yes. Further, my own education in data science has advanced by leaps and bounds as I’ve learned to use these powerful new analytic tools. I would go so far as to say this is a big career hack opportunity! If you are producing highly sophisticated analyses you are going to get noticed compared to the person that is confined to the primitive capabilities of Excel.\n\nAt the end of the day the tool that gets the job done is the right tool. For me, R is a big step forward in efficiency, power and fun over Excel.\nUPDATE June 2019: I originally wrote this note for an internal corporate blog in April 2017. Then, I recommended to NOT to use R for real-time live dashboards. I would amend that statement to say “it depends.” The Shiny interactive web framework from RStudio makes interactive dashboards look very good indeed. Whether or not you can query live data APIs at the requisite frequency depends on the availability of a data feed, a package to grab it or your ability to write your own."
  },
  {
    "objectID": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html",
    "href": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html",
    "title": "Gender Diversity in R and Python Package Contributors",
    "section": "",
    "text": "Over the last few years I have really enjoyed becoming part of the R community. One of the best things about the community is the welcoming, inclusive and supportive nature of it. I can’t speak for other communities in the computer or data science worlds but I am well aware of the “brogrammer” culture in some circles that can be off-putting at times. The rise of codes of conduct across the open source world is changing things for the better, I think.\n\n\nA couple months ago the creator of Python was interviewed saying he thinks open source programming languages have a gender diversity problem. This got me to thinking about whether the inclusive environment I observe in the R community is reflected in female contributions to popular packages and how it compares to the Python world. Most of these packages are maintained on Github which includes all the contributors who use the Github environment to contribute. Let’s take a stab at identifying the gender of these contributors by name.\n\n\nWe will take a multi-stage approach to getting an answer to this question.\n\n\n\nGet the names of the top packages in R and Python.\n\n\nIdentify which those packages which are maintained on Github.\n\n\nGet the contributors to those packages (not as easy as it sounds).\n\n\nGet baby names by gender from the U.S. Social Security database.\n\n\nDecide whether a name is likely to be female or male.\n\n\nMap all package conrtributors to gender, where possible.\n\n\n\nAs usual I follow a couple conventions. The Tidyverse dialect is used throughout. All functions to fetch data from the Web are wrapped in a test to see if the data was already retrieved. This ensures that this notebook won’t break if things in the wild change. In that event, you must get the data files from this Github repo for this to work.\n\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(jsonlite)\nlibrary(rvest)\nlibrary(data.table) #for downloading CRAN/RStudio logs\nlibrary(httr)\nlibrary(gh)\nlibrary(formattable) #percent"
  },
  {
    "objectID": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#identify-the-top-packages-in-r-and-python.",
    "href": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#identify-the-top-packages-in-r-and-python.",
    "title": "Gender Diversity in R and Python Package Contributors",
    "section": "\nIdentify the top packages in R and Python.\n",
    "text": "Identify the top packages in R and Python.\n\n\nUse the cranlogs api from RStudio to get top package downloads from their CRAN mirror. This is potentially a slow function but the top package downloads are pretty stable so we choose five randomly selected dates.\n\n# ----------------------------------------------------------------\n#select 5 random days from the last six months\n# Read data from RStudio site\n# custom version of a function from the installr package. See my Github repo.\nsource(file=\"data/download_RStudio_CRAN_data.R\") \n\nif (!file.exists(\"data/r_pkg_list.rdata\")) {\n RStudio_CRAN_dir <- download_RStudio_CRAN_data(START = Sys.Date()-180,END = Sys.Date(),sample=5)\n # read .gz compressed files form local directory\n RStudio_CRAN_data <- read_RStudio_CRAN_data(RStudio_CRAN_dir)\n \n dim(RStudio_CRAN_data)\n \n # Find the most downloaded packages\n r_pkg_list <- most_downloaded_packages(RStudio_CRAN_data,n=100) %>% \n  as_tibble(.name_repair = make.names,c(\"downloads\")) %>% \n  rename(package=X)\n \n save(r_pkg_list,file=\"data/r_pkg_list.rdata\")\n} else load(\"data/r_pkg_list.rdata\")\n\nWith Python the work as already been done for us here: https://hugovk.github.io/top-pypi-packages/. How helpful!\n\nif (!file.exists(\"data/python_pkg_list.rdata\")){\n \n py_pkgs_raw<-read_json(\"https://hugovk.github.io/top-pypi-packages/top-pypi-packages-365-days.json\",\n             simplifyVector = TRUE)\n python_pkg_list <- py_pkgs_raw$rows[1:100,] %>% \n  as_tibble() %>% \n  rename(package=project,downloads=download_count)\n save(python_pkg_list,file=\"data/python_pkg_list.rdata\")\n} else load(\"data/python_pkg_list.rdata\")"
  },
  {
    "objectID": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#get-the-contributor-names-for-each-package-repo.",
    "href": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#get-the-contributor-names-for-each-package-repo.",
    "title": "Gender Diversity in R and Python Package Contributors",
    "section": "\nGet the contributor names for each package repo.\n",
    "text": "Get the contributor names for each package repo.\n\n\nThis is the messy stuff. We build functions to get contributors to packages and then the real names of those contributors.\n\n\nWe start with a search for the relevant repo with just repo name and optionally the language. Suppose we want to know the names of the R dplyr contributors. The workflow looks like this:\n\n\nCall the API:\n\n\nhttps://api.github.com/search/repositories?q=dplyr+language:r\n\n\nGithub returns a list of the most relevant results based on their point system. In practice this means the package we care about will be the first item in the list. In this case:\n\n\n“full_name”: “tidyverse/dplyr”\n\n\nOne problem I encountered is that not all R packages are tagged as being in the R language. In particular, Rcpp and data.table are considered C language repos by Github. This is one reason why not all the top packages appear to have Github repos. I manually grab the contributors for the two packages mentioned above but, out of laziness, I didn’t go looking for any other missing packages. As we will see, most of the top 100 packages for both languages are found so we have a fairly representative sample…I assume.\n\n\nOnce we have the full package name we can create URLs to get the usernames of all the contributors.\n\n\nContributor url: https://api.github.com/repos/tidyverse/dplyr/contributors\n\n\nThis JSON object will not contain the “real” names but the links to user profiles. We have to make yet another call to the API to extract the real names. Note some people use pseudonyms so the real name won’t be available.\n\n\nCalling the endpoint for the username “https://api.github.com/users/romainfrancois”,\n\n\nwill return, among other things:\n\n\n“name”: “Romain François”\n\n\nFinally, we get what we are after!\n\n\nNOTE: You will need a Github API key for this work. Please refer to the documentation for the gh package.\n\n\nThe utility functions are below:\n\nmy_gh <- function(end_point) {\n  return(jsonlite::fromJSON(jsonlite::toJSON(gh::gh(end_point)),simplifyVector = T))\n}\n\njson_to_df <- function(json){\n  return(jsonlite::fromJSON(jsonlite::toJSON(json),simplifyVector = T))\n}\n\n# --------------------------------------------------------------------\nget_contributor_ids <- function(target_repo){\n# loop through all pages of contributors \n search_url <- paste0(\"/repos/\",\n            target_repo,\n            \"/contributors\")\n contributors_json <- gh(search_url)\n \n # return null in case of no contributors\n if (nchar(contributors_json[1])==0) return(NULL)\n \n contrib_node <- contributors_json\n repeat {\n  contrib_node <- try(gh_next(contrib_node),silent=TRUE)\n  if (is(contrib_node) == \"try-error\") break\n  contributors_json <- c(contributors_json,contrib_node) \n }\n\n contributor_ids <- json_to_df(contributors_json) %>%\n  bind_rows() %>%  \n  select(login,url,avatar_url)\n return(contributor_ids)\n}\n\n# ---------------------------------------------------------------------------\n get_name <- function(contrib_url){\n  user_data <- my_gh(contrib_url)\n  # just return login name if real name is missing\n  if (is_empty(user_data$name)) return(user_data$login) else return(user_data$name)\n }\n\n# --------------------------------------------------------------------\nget_contrib_info <- function(repo_name=\"dplyr\",language=NULL){\n  print(repo_name)\n  # we don't know the Github username associated with the package\n  #so construct a search to get the most likely candidate\n  search_url <- paste0(\"/search/repositories?q=\",\n                       repo_name)\n  if (!is.null(language)){\n    search_url <- paste0(search_url,\"+language:\", language)\n  }\n  # first api call.\n  repos <- my_gh(search_url) %>% .$items\n  # return NULL if no repos in Github are found\n  if (length(repos) == 0) return(NULL)\n  \n  # get full path for exact match on repo name\n  # there might be more than one user with repo of the same name\n  # Since they will be in order of Github \"score\", take just the first one\n  target_repo <- repos %>% \n    select(name,full_name) %>% \n    filter(name == repo_name) %>%\n    pull(full_name) %>% \n    .[1] %>% \n    unlist()\n  # return NULL if no repos in Github are found\n  if (is.null(target_repo)) return(NULL)\n  \n  #second api call\n  # get user urls for all contributors\n  contributor_ids <- get_contributor_ids(target_repo)\n  \n  # return null in case of no contributors\n  if (is.null(contributor_ids)) return(NULL)\n  if (is.null(language)) language <- \"none\"\n  contrib_names<-map(contributor_ids$url,get_name) %>% unlist()\n  print(paste(length(contrib_names),\" contributors\"))\n  contrib_info <- tibble(language=language,\n                         package=repo_name,\n                         path=target_repo,\n                         contributor=contrib_names) %>% \n    bind_cols(contributor_ids) %>% \n    select(-url) %>% unnest()\n  return(contrib_info)\n}\n\nNow let’s do the work of iterating through the package lists. As mentioned above, I get two packages manually before looping through the remaining packages. I chose to use a for loop, as opposed to map or apply so we can save the intermediate results. It is a fairly slow process and you may reach your API data limit before finishing. You don’t want to start from scratch halfway through! If you have to do this in multiple sessions, manually edit the package lists to include just what is left to retrieve.\n\n\nload(\"data/r_pkg_list.rdata\")\nif (!file.exists(\"data/r_pkg_contributors.rdata\")){\n  r_pkg_contributors <- NULL\n  # Rcpp package is categorized as C++, not R, langauge so get it manually.\n  contrib_info_rcpp <- get_contrib_info(\"Rcpp\")\n  contrib_info_rcpp <- contrib_info_rcpp %>% mutate(language = \"r\")\n  r_pkg_contributors <- bind_rows(r_pkg_contributors,contrib_info_rcpp)\n  r_pkg_list <- r_pkg_list %>% filter(package != \"Rcpp\")\n  \n  # data.table package is categorized as C++, not R, langauge so get it manually.\n  contrib_info_dt <- get_contrib_info(\"data.table\")\n  contrib_info_dt <- contrib_info_dt %>% mutate(language = \"r\")\n  r_pkg_contributors <- bind_rows(r_pkg_contributors,contrib_info_dt)\n  r_pkg_list <- r_pkg_list %>% filter(package != \"dt\")\n  \n  # use for loop instead of map or apply so we can save intermediate steps\n  for(pkg in r_pkg_list$package) {\n    r_pkg_contributors <- r_pkg_contributors %>% \n      bind_rows(get_contrib_info(pkg,language=\"r\"))\n    save(r_pkg_contributors,file=\"data/r_pkg_contributors.rdata\")\n  }\n} else load(\"data/r_pkg_contributors.rdata\")\n\nload(\"data/python_pkg_list.rdata\")\nif (!file.exists(\"data/python_pkg_contributors.rdata\")){\n python_pkg_contributors <- NULL\n for(pkg in python_pkg_list$package) {\n  python_pkg_contributors <- python_pkg_contributors %>% \n   bind_rows(get_contrib_info(pkg,language=\"python\"))\n  save(python_pkg_contributors,file=\"data/python_pkg_contributors.rdata\")\n } \n} else load(\"data/python_pkg_contributors.rdata\")\n\n#Let's merge the two datasets to simplify handling.\npkg_contributors <- bind_rows(r_pkg_contributors,python_pkg_contributors)"
  },
  {
    "objectID": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#who-are-the-most-prolific-r-package-contributors",
    "href": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#who-are-the-most-prolific-r-package-contributors",
    "title": "Gender Diversity in R and Python Package Contributors",
    "section": "\nWho are the most prolific R package contributors?\n",
    "text": "Who are the most prolific R package contributors?\n\npkg_contributors %>% \n filter(language==\"r\") %>% \n group_by(contributor) %>% \n summarise(packages=n()) %>% \n arrange(desc(packages))\n## # A tibble: 1,299 x 2\n##    contributor            packages\n##    <chr>                     <int>\n##  1 Hadley Wickham               45\n##  2 Jim Hester                   36\n##  3 Kirill Müller                31\n##  4 Mara Averick                 26\n##  5 Jennifer (Jenny) Bryan       24\n##  6 Gábor Csárdi                 19\n##  7 Hiroaki Yutani               18\n##  8 Lionel Henry                 16\n##  9 Yihui Xie                    16\n## 10 Christophe Dervieux          15\n## # ... with 1,289 more rows"
  },
  {
    "objectID": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#who-are-the-most-prolific-python-package-contributors",
    "href": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#who-are-the-most-prolific-python-package-contributors",
    "title": "Gender Diversity in R and Python Package Contributors",
    "section": "\nWho are the most prolific Python package contributors?\n",
    "text": "Who are the most prolific Python package contributors?\n\npkg_contributors %>% \n filter(language==\"python\") %>% \n group_by(contributor) %>% \n summarise(packages=n()) %>% \n arrange(desc(packages))\n## # A tibble: 6,194 x 2\n##    contributor     packages\n##    <chr>              <int>\n##  1 Jon Dufresne          30\n##  2 Hugo                  27\n##  3 Marc Abramowitz       24\n##  4 Jason R. Coombs       18\n##  5 Jakub Wilk            17\n##  6 Alex Gaynor           16\n##  7 Anthony Sottile       15\n##  8 Felix Yan             15\n##  9 Ville Skyttä          15\n## 10 Donald Stufft         14\n## # ... with 6,184 more rows"
  },
  {
    "objectID": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#people-who-swing-both-ways.",
    "href": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#people-who-swing-both-ways.",
    "title": "Gender Diversity in R and Python Package Contributors",
    "section": "\nPeople who swing both ways.\n",
    "text": "People who swing both ways.\n\n\nWho are the awesome humans who have contributed to both top R and Python packages? Grouping by login name ensures that we don’t get two different people with the same name but we drop it for display. There are 44 people who have contributed to some of both the top Python and R packages.\n\ntwo_lang_contrib <- pkg_contributors %>% \n group_by(login,contributor,language) %>%\n summarise(packages=n()) %>% \n spread(language,packages) %>% \n ungroup() %>% \n select(-login)\n\ntwo_lang_contrib <- two_lang_contrib[complete.cases(two_lang_contrib),] %>% \n arrange(desc(r))\n\ntwo_lang_contrib \n## # A tibble: 46 x 3\n##    contributor              python     r\n##    <chr>                     <int> <int>\n##  1 Craig Citro                   3     7\n##  2 Elliott Sales de Andrade      2     5\n##  3 Philipp A.                    4     4\n##  4 Aaron Schumacher              2     3\n##  5 Ayappan                       1     2\n##  6 Chapman Siu                   1     2\n##  7 Ethan White                   1     2\n##  8 Katrin Leinweber              2     2\n##  9 Mark Sandan                   1     2\n## 10 Tim D. Smith                  3     2\n## # ... with 36 more rows"
  },
  {
    "objectID": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#try-to-determine-gender-of-contributors.",
    "href": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#try-to-determine-gender-of-contributors.",
    "title": "Gender Diversity in R and Python Package Contributors",
    "section": "\nTry to determine gender of contributors.\n",
    "text": "Try to determine gender of contributors.\n\n\nI hope you found the digressions above interesting. Now let’s do what we came to do.\n\n\nTo flag names by gender we use the Social Security baby names database for 1990. It is important to be aware of the limitations of this.\n\n\n\n\nI used 1990 because I guess that is close to the average birth year of most package contributors. Is it? My birth year is (cough) 1958. I am an outlier.\n\n\n\n\nThe dataset contains registered births for only the United States. Many contributors were born, or live today, outside the U.S. The U.S, while more of a melting pot than many countries, will have a subset of global names.\n\n\n\n\nTransliteration of names from languages that don’t use Western characters don’t follow hard and fast rules. The same name might be transliterated multiple ways. “Sergey” or “Sergei?”\n\n\n\n\nOrdering of surname and given name. Chinese names typically are reported surname first. Many Chinese people follow western conventions in global settings but maybe not. I may be tagging the surname as the given name in some (many?) cases.\n\n\n\n\nMany names are used for “both” (yes, I know) genders. I choose an aribitrary ratio of gender predominance of 75% to pronounce certainty. Noteworthy: “Hadley” is in our “Uncertain” bucket.\n\n\n\n\nGender identity becomes a choice at some age. People may choose (or not choose) a gender inconsistant with the identification in this dataset.\n\n\n\n\nSome people use pseudonyms that are not common names.\n\n\n\n\nKnowing all that, let’s plunge on.\n\n\nYou can find the link to the baby names data set here. There are CSV files for each birth year in a zip file. Download, extract and import the file called “yob1990.txt”\n\nnames_90 <- read_csv(\"data/yob1990.txt\",\n           col_names=c(\"first\",\"gender\",\"count\"),\n           col_types = list(col_character(),col_character(),col_number()))\n\nnames_90 <- names_90 %>% \n mutate(first = tolower(first)) %>% \n select(first,gender,count) %>% \n spread(gender,count) %>% \n mutate_if(is.numeric, ~replace(., is.na(.), 0)) %>% \n mutate(prob_female=F/(F+M))\n\ncutoff = 0.75 # threshhold probability for calling gender\nnames_90 <- names_90 %>% mutate(gender=\"Uncertain\")\nnames_90 <- names_90 %>% mutate(gender=if_else(prob_female>cutoff,\"Female\",gender))\nnames_90 <- names_90 %>% mutate(gender=if_else(prob_female<(1-cutoff),\"Male\",gender))\nnames_90_subset <- names_90 %>% select(first,gender)\n\nNow let’s join the baby names to our contributors.\n\npkg_contributors <-pkg_contributors %>%\n separate(\"contributor\",into=c(\"first\"),remove=FALSE,extra=\"drop\")\ngenders <- pkg_contributors %>% \n select(-path,-avatar_url,-login) %>% \n mutate(first = tolower(first)) %>% \n left_join(names_90_subset,by=\"first\") %>% \n mutate_all(~replace(., is.na(.),\"Uncertain\")) \n\nOur answer now looms into view. Base R has a nice tile plot that illustrates the proportions and sizes of the cells in a crosstab so we’ll use that.\n\n\nagg_gender <- genders %>%  \n select(language,gender) %>% \n table() \nagg_gender %>% plot(main=\"Gender Representation in Package Contributions\")\n\n\n\n\nRight away we note the large fraction of “Uncertain” genders, about a third. As we noted above, there are many more contributors to Python packages, as reflected in the width of the tiles. We also can see that the fraction of women contributing to R packages looks greater.\n\n\nFor our ultimate conclusion, let’s assume that the “Uncertain” gender breaks into male and female in the same proportions that already exist.\n\n\nagg_gender <- genders %>% \n filter(gender != \"Uncertain\") %>% \n select(language,gender) %>% \n table() %>% prop.table(margin=1) \n\npercent(agg_gender,digits = 0)\n##         gender\n## language Female Male\n##   python  4%    96% \n##   r       8%    92%\n\nThere it is. This was certainly a lot of work to get to a four cell crosstab but we have our answer. Women contribute to the top R packages at twice the rate of top Python packages. Can we speculate as to a reason? R is almost exclusively a data science language and most of the top packages reflect that. Python is more of a general purpose language that is also quite popular for data science, but as we look down the list of most popular Python packages we see more utility packages. Perhaps women are less represented in general computer science than they are in data science. With both languages, more than 90% of the contributors are men. Clearly, we have a way to go with gender diversity in both communities. Narrowing down the package list to focus on just data science packages is an avenue for further exploration.\n\n\nAs a bonus, what are the most “feminine” packages?\n\ngenders %>% group_by(language,package,gender) %>% \n  filter(gender != \"Uncertain\") %>% \n  count() %>% \n  spread(gender,n) %>% \n  mutate(frac_female = Female/(Female+Male)) %>% \n  arrange(desc(frac_female))\n## # A tibble: 147 x 5\n## # Groups:   language, package [147]\n##    language package    Female  Male frac_female\n##    <chr>    <chr>       <int> <int>       <dbl>\n##  1 r        cellranger      1     1       0.5  \n##  2 r        hms             1     2       0.333\n##  3 r        rprojroot       1     2       0.333\n##  4 r        tidyselect      3     6       0.333\n##  5 r        forcats         7    18       0.28 \n##  6 r        ellipsis        1     3       0.25 \n##  7 python   decorator       2     7       0.222\n##  8 r        scales          5    18       0.217\n##  9 r        tidyr          11    41       0.212\n## 10 r        pillar          1     4       0.2  \n## # ... with 137 more rows\n\nThat’s interesting. There are 28 popular packages across both languages where more than 10% of the contributors are female. Of those 25 are R packages and only 3 are Python packages.\n\n\nThere are other dimensions of diversity we might look at that are beyond the ability to infer from names. It would be nice if we could see actual images of all contributors so we might make some observations about racial diversity or remove some of the ambiguities around gender identification. This approach would come with its own set of challenges and risks, however.\n\n\nAs mentioned at the start of this ariticle, there are many reasons to take our conclusions with a grain of salt. I certainly do not claim this analysis is definitive. A better approach might be to simply survey the contributors. Still, the results conform with what intuition might provide.\n\n\nI welcome critiques of my methods or conclusions. I have a sneaky suspicion I got the Github contributor names the hard way. Thanks for reading!"
  },
  {
    "objectID": "posts/2019-12-08-state-taxes-it-s-not-just-about-income/2019-12-08-state-taxes-it-s-not-just-about-income.html",
    "href": "posts/2019-12-08-state-taxes-it-s-not-just-about-income/2019-12-08-state-taxes-it-s-not-just-about-income.html",
    "title": "State Taxes: It’s not just about Income",
    "section": "",
    "text": "Much of the discussion around tax burdens focuses on income taxes but, at the state level, that leaves out two other big sources of tax liability, sales and property taxes. Here we’ll quickly look at the interplay of all three taxes in a graphical way. This can inform our thinking about how attractive it is to live in each state and on public policy questions involving tax fairness. The plotly package lets us easily create an interactive 3D scatter plot that is uniquely useful to visualize this.\nSales taxes vary greatly by state but, for lower income people, might be the biggest tax burden. Indeed, since low-income families spend a larger fraction of their income, these taxes are “regressive” since the relative burden grows as income falls. Income taxes are typically “progressive” since, in most states, the rate grows with income levels. Property taxes aren’t directly levied on renters but the landlords pass the tax through via higher rents, so everyone pays. Let’s take a quick look at how tax rates vary by state and category.\nThe tax data was found in three different places:\n\nIncome tax rates from https://taxfoundation.org/state-individual-income-tax-rates-brackets-2019/\nProperty tax Rates from https://wallethub.com/edu/states-with-the-highest-and-lowest-property-taxes/11585/\nSales Tax Rates https://www.salestaxinstitute.com/resources/rates\n\nI make some choices in how to present the data. First of all, I use the top marginal rates, so this represents the “worst-case” tax burden. It should be representative of the overall tax structure and useful to compare across states. Next, I add average municipal income taxes computed by the Tax Foundation for each state to the state income tax rate. If you live in New York City, this will substantially understate your tax burden and overstate it elsewhere. Some municipalities levy sales taxes as well but I do NOT include these because they vary so widely and we don’t have all day. Also, municipalities love to tax people who can’t vote, like out of towners, with hotel and rental car taxes. These would not affect your view of where to live. How about excise taxes on gasoline, cigarettes, etc? Not included.\nI already combined the data from each source with the adjustments mentioned above into a single CSV file. Load it with the required libraries.\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(plotly))\n\nstate_rates <- read_csv(\"data/state_rate.csv\",col_types = \"fnnn\") \n\nLet’s take a quick look at the summary statistics.\n\nsummary(state_rates[,2:4])\n\n   income_tax       sales_tax      property_tax  \n Min.   : 0.000   Min.   :0.000   Min.   :0.270  \n 1st Qu.: 4.925   1st Qu.:4.375   1st Qu.:0.730  \n Median : 5.950   Median :6.000   Median :0.980  \n Mean   : 5.835   Mean   :5.062   Mean   :1.119  \n 3rd Qu.: 7.190   3rd Qu.:6.250   3rd Qu.:1.550  \n Max.   :13.300   Max.   :7.250   Max.   :2.440  \n\n\nSome states have no personal income tax at all but have to raise revenue somehow. Most commonly, sales tax forms a big part of the budget. Is there a pattern where lower income tax rates correlate with higher sales or property taxes? A correlation matrix provides a quick check.\n\nknitr::kable(cor(state_rates[2:4]))\n\n\n\n\n\nincome_tax\nsales_tax\nproperty_tax\n\n\n\n\nincome_tax\n1.0000000\n0.0292638\n0.1074844\n\n\nsales_tax\n0.0292638\n1.0000000\n0.1115520\n\n\nproperty_tax\n0.1074844\n0.1115520\n1.0000000\n\n\n\n\n\nIt doesn’t look like there is any relationship.\nTax rates are not the same thing as cash out of pocket. As mentioned above, several issues affect the translation of rates to dollars. Ideally, we would like to know which states are the most expensive to live in, tax-wise. We don’t care which pocket it comes out of but we have to make assumptions.\nLet’s add adjustment factors for the impact of sales and property taxes relative to income taxes. This will let us add all three together to come up with a “tax pain” index. In theory, property taxes are levied according to a percentage of the value of the home. But there are complex formulas that go beyond just the published “rate.” In New York, it turns out that the median property tax bill is roughly equal to the median income tax liability, so I chose an adjustment factor of 1.0. How much of your taxable income is spent on consumption of things that sales tax is levied on? As mentioned above, low earners typically live hand-to-mouth. Affluent people can save more for deferred consumption, philanthropy or passing to heirs. I chose to assume 30% of household income is spent where sales taxes apply. Also note that sales tax rates are flat. Not only do poor people consume a higher fraction of their income, sales taxes aren’t scaled by income. You can play around with both of these adjustment factors based on what you want to see. There is no “correct” number. Low income families might pay no income tax and property taxes only indirectly, so sales tax is really the only tax that matters for them.\nThe tax pain index can be crudely interpreted as the fraction of a high earner’s income that will be paid in just state taxes. I call it an “index” because it can also be interpreted as a comparison of the relative tax burden across states for all wage earners.\n\n# judge how to weight realized cost of sales and property relative to income tax.\nsales_adj    = 0.3 # assume we spend 30% of our taxable income on items subject to sales tax.\nproperty_adj = 1.0 # assume median income tax liability is about equal to the property tax on the median home. \n\n# use these adjustments to create ranking that we will use to color the markers in the plot.\n# the sum of the adjusted values is a *rough* guide to the total tax burden.\n\nstate_rates_adj <- state_rates %>% \n   mutate(tax_pain = income_tax + (sales_tax * sales_adj) + (property_tax * property_adj)) %>% \n   arrange(desc(tax_pain))\n\nknitr::kable(state_rates_adj[1:10,])\n\n\n\n\nstate\nincome_tax\nsales_tax\nproperty_tax\ntax_pain\n\n\n\n\nCalifornia\n13.30\n7.25\n0.77\n16.245\n\n\nNew Jersey\n11.25\n6.63\n2.44\n15.679\n\n\nNew York\n10.69\n4.00\n1.68\n13.570\n\n\nMinnesota\n9.85\n6.88\n1.15\n13.064\n\n\nHawaii\n11.00\n4.00\n0.27\n12.470\n\n\nVermont\n8.75\n6.00\n1.83\n12.380\n\n\nIowa\n8.75\n6.00\n1.53\n12.080\n\n\nMaryland\n8.60\n6.00\n1.10\n11.500\n\n\nOregon\n10.28\n0.00\n1.04\n11.320\n\n\nDistrict of Columbia\n8.95\n6.00\n0.55\n11.300\n\n\n\n\n\n\nstate_rates_adj %>% \n   # reorder the state factor levels so they display in order of tax pain, not alphabetically\n   mutate(state = fct_reorder(state,tax_pain)) %>% \n   ggplot(aes(state,tax_pain)) + geom_col() + \n   labs(title = \"Cumulative Impact of State Taxes\",\n        subtitle = \"Income, Sales and Property\",\n        x = \"State\",\n        y = '\"Tax Pain\" Index') + \n   theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\n\n\n\nNo big surprises here. Florida, good. California, bad. Seeing Vermont at the high tax end while New Hampshire is at the low end is interesting. The two states are about the same size and have the same climate. The low tax state has over twice the population and a 33% higher median income. Just sayin’….\nWe would like to visualize the interplay of the three tax vectors and a 3D scatterplot is ideal for this. Further, the plotly package lets us interactively rotate the plot, which is critical for perceiving the 3D volume on a 2D surface. There are a lot of gratuitous uses of 3D visualization out there. This is one instance where 3D really adds to our understanding.\n\n# Create 3d animated plot of 3 state tax rate dimensions,\n# income, property and sales\nplot_ly(state_rates_adj,x = ~income_tax,\n        y= ~sales_tax,\n        z= ~property_tax,\n        type=\"scatter3d\", \n        mode=\"markers\",\n        color = ~tax_pain,\n        hoverinfo = \"text\",\n        text= ~state) %>% \n   layout(title = \"Major Tax Rates by State\",\n          scene = list(xaxis = list(title = 'Income Tax'),\n                       yaxis = list(title = 'Sales Tax'),\n                       zaxis = list(title = 'Property Tax')))\n\n\n\n\n\nPlay around with dragging the image and you start to appreciate the volume. Each piece of the tax picture gets an axis. The tax pain index is represented by color of the markers. You can quickly see that income tax is still the big driver of tax pain across the nation. New Jersey applies high taxes in all dimensions. California is heavily skewed to income tax but is comparatively low in the property tax dimension.\nNevada is a great state to live in if you have income and property. The state gets about a third of its revenue from out-of-state tourists who are spending liberally. Gambling is big, obviously, but a high sales tax is a way to get revenue from visitors while making the tax burden lighter on residents. As we know, sales taxes are regressive so, at first glance, the poor residents of Nevada might be the unintended losers from this scheme. Fortunately, Nevada lightens the relative burden on the poor by exempting drugs and groceries from sales tax.\nAnother great place to live if you hate taxes is in Washington State, on the Oregon border. Washington levies no income tax and Oregon levies no sales tax. I was surprised to see, in a quick Google maps search, no evidence that big box retailers shun the Washington side of the border. In theory, if an Oregon business knows you live in Washington they are supposed to charge taxes (Ha!). Across the border, Oregon residents could avoid paying sales tax in Washington by flashing an Oregon ID but that ended in the summer of 2019.\nFinally, Alaska is the most tax-friendly state overall with low taxes in all dimensions. The state goes even further, though. Oil revenues go into a fund which pays a cash dividend to every resident, every year. Most recently it was $1,600 so some residents, in effect, receive taxes from the state. So, move there."
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html",
    "title": "What Do The Ramones Want?",
    "section": "",
    "text": "Recently I saw a tweet that shared this hilarious poster of Ramones “wants”.  Dan Gneiding (aka Grayhood) is the graphic designer who created this. You can buy it here\nVery cool, but how accurate is it? I asked Dan and says he took some artistic license, as he should! You may accuse me of being that pedantic “Comic Book Guy” from “The Simpsons” but, when I saw it, I immediately wondered how I could tally these Ramones lyrics myself or, rather, get R to do it for me. The tidytext mining package makes short work of the project, as we’ll see."
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#why-the-ramones",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#why-the-ramones",
    "title": "What Do The Ramones Want?",
    "section": "\nWhy the RAMONES?\n",
    "text": "Why the RAMONES?\n\n\nThe Ramones hold a special place in my heart. As a college student in central Ohio, in the late 70s, my frat brothers and I were huge fans. We were completely ridiculous of course. Preppy nerds bobbing to “Beat on the Brat” The Sigma Chis thought we were idiots (Lynrd Skynrd? Come on! History has judged). I never saw the Ramones at CBGBs but when we heard they were coming to a cowboy bar on notorious High St. across from Ohio State, we were thrilled. I blew off studying for a Poly-Sci mid-term the next day. I got my worst college grade ever but it was totally worth it. I said my future self would thank me and I was right!\n\n\nWithout any further adieu, hey, ho, let’s go!"
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#load-packages",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#load-packages",
    "title": "What Do The Ramones Want?",
    "section": "\nLoad Packages\n",
    "text": "Load Packages\n\n\nFirst, load packages.\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(rvest)\nlibrary(reshape2)\nlibrary(wordcloud)\nlibrary(scales)\nlibrary(genius)\nlibrary(ggthemr)\nlibrary(ggrepel)\n\nggthemr(\"earth\",type=\"outer\")"
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#get-lyrics-from-genius-api",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#get-lyrics-from-genius-api",
    "title": "What Do The Ramones Want?",
    "section": "\nGet Lyrics from Genius API\n",
    "text": "Get Lyrics from Genius API\n\n\nHave you ever spent an enormous amount of time on something, only to discover there was a much simpler way? Yes, yes you have. For this project we need a source of Ramones lyrics. Originally, I built a very finicky web scraping routine to get lyrics from a site I commonly use in my browser. I coaxed it to get all of the lyrics but I didn’t want to share it in this post because you would likely not be able to get it to work smoothly. Months passed then it occurred to me to Google “lyrics api” and, “viola!”, I found http://genius.com and the genius R package by Josiah Parry, available on CRAN. Access to the lyric API does require a free application access token. You can generate one here. I will leave installing the token called GENIUS_API_TOKEN into your R environment as an exercise for the reader. There are numerous tutorials on this subject around.\n\n\nAs always, we will be working in the tidyverse veracular. First we build a data frame of album names and the year of release. This gets fed into a single function genius::add_genius which returns all the lyrics. I’m embarressed to think about the tangled mess of web scraping code I was previously using.\n\n\nAs usual, we check to see if the file with all the downloaded data is already available so, as we iterate versions of our project, we don’t hit the API over and over.\n\n#make sure you have a Genius API token\n# my token is in the .Reviron file\n\n# All the studio albums\nramones_albums <- tribble(\n  ~album, ~year,\n  \"Ramones\", 1976,\n  \"Leave Home\", 1977,\n  \"Rocket To Russia\", 1977,\n  \"Road To Ruin\", 1978,\n  \"End Of The Century\", 1980,\n  \"Pleasant Dreams\", 1981,\n  \"Subterranean Jungle\", 1983,\n  \"Too Tough To Die\", 1984,\n  \"Animal Boy\", 1986,\n  \"Halfway To Sanity\",1987,\n  \"Brain Drain\",1989,\n  \"Mondo Bizarro\",1992,\n  \"Acid Eaters\",1993,\n  \"¡Adios Amigos!\",1995\n)\nartist_albums <- ramones_albums %>% \n  mutate(artist=\"Ramones\") %>% \n  select(artist,album) %>%\n  {.}\n\nif (file.exists(\"data/ramones_lyrics_genius.rdata\")){\n  load(\"data/ramones_lyrics_genius.rdata\")\n} else {\n  ramones_lyrics_genius <- genius::add_genius(artist_albums,artist,album)\n  save(ramones_lyrics_genius,file=\"data/ramones_lyrics_genius.rdata\")\n}"
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#put-lyics-in-tidytext-form",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#put-lyics-in-tidytext-form",
    "title": "What Do The Ramones Want?",
    "section": "\nPut Lyics in Tidytext Form\n",
    "text": "Put Lyics in Tidytext Form\n\n\nMost projects require a huge amount of data wrangling before we can get any real analysis done. This project is pretty clean. We are already nearly good to go. Further, tidytext makes the remaining manipulation of the data soooo easy! To wit, let’s tokenize the data into individual words.\n\nramones_lyrics <- ramones_lyrics_genius\n#make factor to keep albums in order of issue date\nramones_lyrics$album <- as_factor(ramones_lyrics$album)\nramones_albums$album <- as_factor(ramones_albums$album)\nramones_lyrics <- right_join(ramones_lyrics,ramones_albums,by=\"album\")\nlyric_words <- ramones_lyrics  %>% \n  unnest_tokens(word,lyric) %>%\n  rename(song_name=track_title)\n\nSee, I said it was easy."
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#how-needy-are-the-ramones",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#how-needy-are-the-ramones",
    "title": "What Do The Ramones Want?",
    "section": "\nHow Needy Are The Ramones?\n",
    "text": "How Needy Are The Ramones?\n\n\nOut of 193 songs on all their studio albums, 16 mention wanting or not wanting in the title. “I Wanna” songs are a thing with the Ramones.\n\nwant_phrases <- \"Wanna|Want\"\nramones_lyrics %>% \n  select(album, track_title) %>% \n  distinct() %>%\n  filter(str_detect(track_title,want_phrases)) %>% \n  {.}\n## # A tibble: 16 x 2\n##    album             track_title                                    \n##    <fct>             <chr>                                          \n##  1 Ramones           I Wanna Be Your Boyfriend                      \n##  2 Ramones           Now I Wanna Sniff Some Glue                    \n##  3 Ramones           I Don't Wanna Go Down to the Basement          \n##  4 Ramones           I Don't Wanna Walk Around with You             \n##  5 Leave Home        Now I Wanna Be a Good Boy                      \n##  6 Rocket To Russia  Do You Wanna Dance?                            \n##  7 Rocket To Russia  I Wanna Be Well                                \n##  8 Road To Ruin      I Just Want To Have Something To Do            \n##  9 Road To Ruin      I Wanted Everything                            \n## 10 Road To Ruin      I Don't Want You                               \n## 11 Road To Ruin      I Wanna Be Sedated                             \n## 12 Road To Ruin      I Want You Around (Ed Stasium Version)         \n## 13 Pleasant Dreams   We Want the Airwaves                           \n## 14 Halfway To Sanity I Wanna Live                                   \n## 15 Brain Drain       Merry Christmas (I Don't Want To Fight Tonight)\n## 16 ¡Adios Amigos!    I Don't Want to Grow Up"
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#do-some-sentiment-analysis",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#do-some-sentiment-analysis",
    "title": "What Do The Ramones Want?",
    "section": "\nDo Some Sentiment Analysis\n",
    "text": "Do Some Sentiment Analysis\n\n\nBefore we look at the what the Ramones want we might as well run the, now routine, sentiment analysis you may have learned about from Julia Silge and David Robinson here. The Ramones are no Jane Austen but, hey, they have feelings, ya know? SAD NOTE: They “had” feelings. All the original four are dead.\n\n\nTo start our sentiment analysis let’s pull out stop words that don’t provide much context and label all the words in the “bing” sentiment database as either positive or negative.\n\nlyric_words_cleaned <- lyric_words %>% anti_join(get_stopwords(),by=\"word\")\n\n#quick sentiment analysis\npositive <- get_sentiments(\"bing\") %>%\n  filter(sentiment == \"positive\")\n\nnegative <- get_sentiments(\"bing\") %>%\n  filter(sentiment == \"negative\")\n\nlyric_words_cleaned %>%\n  semi_join(positive,by=\"word\") %>%\n  group_by(song_name) %>% \n  count(word) %>% \n  group_by(song_name) %>% \n  tally(sort = TRUE,name=\"Happy Words\")\n## # A tibble: 162 x 2\n##    song_name                   `Happy Words`\n##    <chr>                               <int>\n##  1 The Crusher                            10\n##  2 It's Gonna Be Alright                   9\n##  3 Palisades Park                          9\n##  4 Too Tough to Die                        9\n##  5 Censorshit                              8\n##  6 I Don't Want to Grow Up                 8\n##  7 In the Park                             8\n##  8 My Back Pages                           8\n##  9 Gimme Gimme Shock Treatment             7\n## 10 Glad to See You Go                      7\n## # ... with 152 more rows\nlyric_words_cleaned %>%\n  semi_join(negative,by=\"word\") %>%\n  group_by(song_name) %>% \n  count(word) %>% \n  group_by(song_name) %>% \n  tally(sort = TRUE,name=\"Sad Words\")\n## # A tibble: 156 x 2\n##    song_name                       `Sad Words`\n##    <chr>                                 <int>\n##  1 I'm Not Afraid of Life                   21\n##  2 Endless Vacation                         17\n##  3 Don't Bust My Chops                      16\n##  4 Love Kills                               15\n##  5 Wart Hog                                 13\n##  6 My Back Pages                            12\n##  7 Cretin Family                            10\n##  8 Something to Believe In                  10\n##  9 Anxiety                                   9\n## 10 Howling at the Moon (Sha-La-La)           9\n## # ... with 146 more rows\n\nNow we change the sign of the count of negative words so we can get the net balance of happy vs. sad words.\n\nlyric_words_cleaned %>%\n  inner_join(get_sentiments(\"bing\"),by=\"word\") %>%\n  group_by(song_name) %>% \n  count(sentiment,sort=TRUE) %>% \n  mutate(n = ifelse(sentiment == \"negative\", -n, n)) %>%\n  group_by(song_name) %>% \n  summarise(net_sentiment=sum(n)) %>% \n  filter(abs(net_sentiment) > 10) %>%\n  mutate(song_name = reorder(song_name, net_sentiment)) %>%\n  mutate(sentiment=ifelse(net_sentiment<0,\"Negative\",\"Positive\")) %>% \n  ggplot(aes(song_name, net_sentiment, fill = sentiment)) +\n  geom_col() +\n  coord_flip() +\n  labs(title=\"How Happy are RAMONES Songs?\",\n       y = \"Very Sad <---   ---> Very Happy\",\n       x= \"\") +\n  scale_fill_manual(values = c(\"red\",\"darkgrey\"))+\n  theme(axis.text.y =  element_text(size=7,hjust=1))"
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#sentiment-over-time",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#sentiment-over-time",
    "title": "What Do The Ramones Want?",
    "section": "\nSentiment Over Time\n",
    "text": "Sentiment Over Time\n\n\nThe average sentiment over the whole lyric corpus is about evenly split between positive and negative words but if we look at sentiment by album we see a gyrating trend with an intersting dip in their middle years.\n\nlyric_words_cleaned %>%\n  inner_join(get_sentiments(\"bing\"),by=\"word\") %>%\n  group_by(album, year) %>% \n  count(sentiment,sort=TRUE) %>% \n  arrange(album) %>% \n  pivot_wider(values_from = n,names_from = sentiment) %>% \n  mutate(fraction_happy = positive/(negative+positive)) %>%\n  ggplot(aes(year,fraction_happy)) + geom_line(color=\"red\") + geom_point(color=\"red\") +\n  labs(title = \"RAMONES Mood Over Time\",\n       y= \"Fraction of Happy Words\",\n       x= \"Album Release Year\") + \n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    geom_text_repel(aes(label=album),\n                    color=\"white\",\n                    segment.color = \"white\")\n\n\n\n\nWe can generate word clouds for any album. Their “happiest” album is “Road to Ruin.”\n\n{par(bg=\"black\")\n  lyric_words_cleaned %>%\n    filter(album == \"Road To Ruin\") %>% \n    inner_join(get_sentiments(\"bing\"),by=\"word\") %>%\n    count(word, sentiment, sort = TRUE) %>%\n    acast(word ~ sentiment, value.var = \"n\", fill = 0) %>%\n    #  comparison.cloud(colors = c(\"#F8766D\", \"#00BFC4\"),\n    #                   max.words = 100)\n    comparison.cloud(colors = c(\"red\", \"grey60\"),\n                     max.words = 100,\n                     title.bg.colors=\"grey60\")\n    text(x=1.1,y=0.5,\"RAMONES\",col=\"red\",cex=4,srt=270)\n    text(x=-0.1,y=0.5,\"Road To Ruin\",col=\"grey60\",cex=4,srt=90)\n}\n\n\n\n\n… and their angriest, “Animal Boy.” You start to think there is something to this sentiment analysis stuff when you read the opening of this album’s review at http://allmusic.com:\n\n\n\nAnimal Boy wasn’t a very happy record for the Ramones. Since the release of Too Tough to Die (a slight return to form) nearly two years earlier, the band’s fortunes had gone from bad to worse; interest in the band kept dwindling with every release and the “bruthas” were constantly at each other’s throat.\n\n\n{par(bg=\"black\")\n  lyric_words_cleaned %>%\n    filter(album == \"Animal Boy\") %>% \n    inner_join(get_sentiments(\"bing\"),by=\"word\") %>%\n    count(word, sentiment, sort = TRUE) %>%\n    acast(word ~ sentiment, value.var = \"n\", fill = 0) %>%\n    #  comparison.cloud(colors = c(\"#F8766D\", \"#00BFC4\"),\n    #                   max.words = 100)\n    comparison.cloud(colors = c(\"red\", \"grey60\"),\n                     max.words = 100,\n                     title.bg.colors=\"grey60\")\n    text(x=1.1,y=0.5,\"RAMONES\",col=\"red\",cex=4,srt=270)\n    text(x=-0.1,y=0.5,\"Animal Boy\",col=\"grey60\",cex=4,srt=90)\n}"
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#what-do-the-ramones-want-and-not-want",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#what-do-the-ramones-want-and-not-want",
    "title": "What Do The Ramones Want?",
    "section": "\nWhat do the RAMONES want… and not want?\n",
    "text": "What do the RAMONES want… and not want?\n\n\nNow lets find what the Ramones Want. An n-gram is simply a cluster of words of length n. Let’s look at the most common n-grams, which would include the phrases like “I want” and “I wanna.”\n\n\nStart with shortest n-gram that is a complete thought and work up to longer phrases. We take the the shortest phrase that makes sense unless appending more words doesn’t change the frequency. Then we take the longer phrase. For instance if “I wanna steal some money” and “I wanna steal from the rich” both exist we take “I wanna steal” since it would have a higher frequency than either longer phrase. In this case, the only phrase starting with “I wanna steal” is “I wanna steal from the rich” so we use that.\n\nwant_phrases <- \"^(i wanna |i want |we want |we wanna |i wanted |i just want |i just wanna )\"\n\nget_ngrams <- function(lyrics,n,prefixes=\"\"){\n  min_instance = 0\n  lyric_ngram <- lyrics %>% \n    unnest_tokens(ngram,lyric,token = \"ngrams\",n=n) %>% \n    group_by(ngram) %>% \n    filter(str_detect(ngram,prefixes)) %>% \n    count() %>% \n    arrange(desc(n)) %>% \n    filter(n>min_instance) %>% \n    mutate(want=str_remove(ngram,prefixes)) %>% \n   rowid_to_column()\n  return(lyric_ngram)\n  \n}\n\nwant <- ramones_lyrics %>% get_ngrams(5,want_phrases)\nwant\n## # A tibble: 43 x 4\n## # Groups:   ngram [43]\n##    rowid ngram                         n want             \n##    <int> <chr>                     <int> <chr>            \n##  1     1 i want i want i              14 i want i         \n##  2     2 i want to be your            13 to be your       \n##  3     3 i just want to walk           7 to walk          \n##  4     4 i just want to have           6 to have          \n##  5     5 i just want to be             4 to be            \n##  6     6 i wanna be your boyfriend     4 be your boyfriend\n##  7     7 i want to live my             4 to live my       \n##  8     8 i want to run away            4 to run away      \n##  9     9 i want to be a                3 to be a          \n## 10    10 i want you by my              3 you by my        \n## # ... with 33 more rows\n\nWhat a human needs to do is decide which phrases are complete thoughts. We manually select the row numbers to build our ultimate table.\n\n\nRemember what I said before about data wrangling? Well, sure, getting the words was easy. Determining meaningful phrases not (for a computer). If this was Spotify, our AI could figure these out, but this is not Spotify. This is an iterative process of manually inspecting tables of ever-longer n-grams and noting which rows have complete thoughts until we don’t see any sensible new phrases. We run through twice, first for “want” then “don’t want.” We flip the sign on the count of “don’t wants” to negative. I won’t bore you with every iteration so let’s skip ahead. Think of this as the cinematic training montage.\n\n# WANT\n# make \"wanna\" in to \"want to\" which also frequently appears so we get a good count.\nramones_lyrics <- ramones_lyrics %>% mutate(lyric=str_replace_all(lyric,\"wanna\",\"want to\"))\ndo_want <- tibble()\nall_wants <- tibble() # for debugging\n# why make the code below a function, if we only call it once?\n# Since we cumulatively modify all_wants each step is dependent on the prior one executing first\n# this organizes the code into a block that tells future self to execute as a block\nbuild_wants <- function(all_wants) {\n  want_phrases <- \"^(i wanna |i want |we want |we wanna |i wanted |i just want |i just wanna )\"\n  #select the 3-gram phrases that are complete thoughts using manual inspection\n  want <- ramones_lyrics %>% get_ngrams(3,want_phrases)\n  # visually inspect the want variable and select which lines to add to all_wants\n  # pause after each instance of get_ngrams to do this.\n  all_wants <- bind_rows(all_wants,want[c(2,8,11,13),])\n  # move to the 4-gram phrases, etc\n  want <- ramones_lyrics %>% get_ngrams(4,want_phrases)\n  all_wants <- bind_rows(all_wants,want[c(5,6,9,13,14,17,24,28,30,31,37),])\n  want <- ramones_lyrics %>% get_ngrams(5,want_phrases)\n  all_wants <- bind_rows(all_wants,want[c(3,4,6,9,21,22),])\n  want <- ramones_lyrics %>% get_ngrams(6,want_phrases)\n  all_wants <- bind_rows(all_wants,want[c(1,11,12,22,25,28),])\n  want <- ramones_lyrics %>% get_ngrams(7,want_phrases)\n  all_wants <- bind_rows(all_wants,want[c(5,6,7,9,10,12,21),])\n  want <- ramones_lyrics %>% get_ngrams(8,want_phrases)\n  all_wants <- bind_rows(all_wants,want[c(7,3),])\n  return (all_wants)\n}\n\ndo_want <- build_wants(do_want)\ndo_want <- do_want %>% \n  mutate(want=str_to_title(want)) %>% \n  group_by(want) %>% \n  summarise(n=sum(n)) %>% \n  arrange(desc(n))\n\n# DONT'T WANT\ndont_want <- tibble()\nall_wants <- tibble() # for debugging only\nramones_lyrics <- ramones_lyrics %>% mutate(lyric=str_replace_all(lyric,\"wanna\",\"want to\"))\nwant_phrases <- \"^(i don't want |we don't want |i didn't want )\"\nbuild_dont_wants <- function(all_wants) {\n  want <- ramones_lyrics %>% get_ngrams(4,want_phrases)\n  all_wants <- bind_rows(all_wants,want[c(2),])\n  want <- ramones_lyrics %>% get_ngrams(5,want_phrases)\n  all_wants <- bind_rows(all_wants,want[c(3,5,6,7,9,11,15),])\n  want <- ramones_lyrics %>% get_ngrams(6,want_phrases)\n  all_wants <- bind_rows(all_wants,want[c(1,7),])\n  want <- ramones_lyrics %>% get_ngrams(7,want_phrases)\n  all_wants <- bind_rows(all_wants,want[c(2,17),])\n  want <- ramones_lyrics %>% get_ngrams(8,want_phrases)\n  all_wants <- bind_rows(all_wants,want[c(7,8,9,16),])\n  want <- ramones_lyrics %>% get_ngrams(9,want_phrases)\n  all_wants <- bind_rows(all_wants,want[c(3,10,12),])\n  want <- ramones_lyrics %>% get_ngrams(10,want_phrases)\n  #there it is - Pet Sematary!\n  all_wants <- bind_rows(all_wants,want[c(1),])\n}\ndont_want <- build_dont_wants(dont_want)\ndont_want <- dont_want %>%\n  mutate(n = -n) %>% \n  mutate(want=str_to_title(want)) %>% \n  group_by(want) %>%\n  summarise(n=sum(n)) %>% \n  arrange(n)\n\nFinally we put it all together to get what we’re after.\n\nultimate_want <- bind_rows(do_want,dont_want) %>% \n  group_by(want) %>%\n  summarise(n=sum(n)) %>%   \n  mutate(Sentiment = ifelse(n > 0,\"Want\",\"Don't Want\")) %>% \n  arrange(n) %>% \n  {.}\n\np <- ultimate_want %>% mutate(want=reorder(want,n)) %>% \n  filter(abs(n) > 1) %>% \n  ggplot(aes(want,n,fill=Sentiment)) + geom_col()+coord_flip()+\n  labs(title=\"What Do The RAMONES Want?\",\n       y=\"How Much Do The RAMONES Want It?\",\n       x=\"\")\np + \n  scale_fill_manual(values = c(\"red\",\"darkgrey\"))+\n  theme(axis.text.y =  element_text(size=7,hjust=1))"
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#bringing-it-full-circle",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#bringing-it-full-circle",
    "title": "What Do The Ramones Want?",
    "section": "\nBringing It Full Circle\n",
    "text": "Bringing It Full Circle\n\n\nSometimes, late at night, after everyone else is asleep, I hide under the covers, open my laptop and look at… pie charts. Ed Tufte says I will go blind if I keep doing it. Still, for the sake of bringing this full circle (ahem) back to the chart that inspired it, let’s make a version of Grayhood’s poster with our data. So it’s not a complete mess, we lump any phrases that occur less than 4 times in “Other.” That takes some of the fun out of things since we lose memorable phrases like “I wanna sniff some glue” which the poster above includes. This is data science, not art. It’s not supposed to be fun! While I use ggplot2 pretty much exclusively, the base R pie plot produces pretty clean results that approximate the style of the poster with no embellishment.\n\ncollapsed_want <- ultimate_want %>%\n  filter(Sentiment==\"Want\") %>%\n  mutate(want = ifelse(n<4,\"Other\",want)) %>%\n  group_by(want) %>% \n  summarise(n=sum(n)) %>% \n  arrange(desc(n)) %>% \n  {.}\n\n with(collapsed_want,\n      pie(n, \n          labels=paste0(as.character(want), \" \", n, \"%\"),\n          col=c(\"brown\",\"red\",\"black\",\"darkblue\",\"pink\",\"purple\"),\n          radius=1,\n          density=30,\n          bg=\"sienna\",\n          main=\"The Ramones Want...\"))\n\n\n\ncollapsed_want <- ultimate_want %>%\n  filter(Sentiment==\"Don't Want\") %>%\n  mutate(n = -n) %>% \n  mutate(want = ifelse(n<2,\"Other\",want)) %>%\n  group_by(want) %>% \n  summarise(n=sum(n)) %>% \n  arrange(desc(n)) %>% \n  {.}\n\n with(collapsed_want,\n      pie(n, \n          labels=paste0(as.character(want), \" \", n, \"%\"),\n          col=c(\"brown\",\"red\",\"black\",\"darkblue\",\"pink\",\"purple\"),\n          radius=1,\n          density=30,\n          bg=\"sienna\",\n          main=\"The RAMONES Don't Want...\"))\n\n\n\n\nIt must be comforting to know the Ramones want you more than anything but they aren’t going down to the basement with you. Okay, so maybe this was a little fun. Thanks for reading!"
  },
  {
    "objectID": "posts/2020-12-06-covid-cases-vs-deaths/2020-12-06-covid-cases-vs-deaths.html",
    "href": "posts/2020-12-06-covid-cases-vs-deaths/2020-12-06-covid-cases-vs-deaths.html",
    "title": "Covid Cases vs. Deaths",
    "section": "",
    "text": "Introduction\n\n\nI have a macabre fascination with tracking the course of the COVID-19 pandemic. I suspect there are two reasons for this. One, by delving into the numbers I imagine I have some control over this thing. Second, it feels like lighting a candle to show that science can reveal truth at a time when the darkness of anti-science is creeping across the land.\n\n\nThe purpose of this project is, as usual, twofold. First, to explore an interesting data science question and, second, to explore some techniques and packages in the R universe. We will be looking at the relationship of COVID-19 cases to mortality. What is the lag between a positive case and a death? How does that vary among states? How has it varied as the pandemic has progressed? This is an interesting project because is combines elements of time series forecasting and dependent variable prediction.\n\n\nI have been thinking about how to measure mortality lags for a while now. What prompted to do a write-up was discovering a new function in Matt Dancho’s timetk package, tk_augment_lags, which makes short work of building multiple lags. Not too long ago, managing models for multiple lags and multiple states would have been a bit messy. The emerging “tidy models” framework from RStudio using “list columns” is immensely powerful for this sort of thing. It’s great to reduce so much analysis into so few lines of code.\n\n\nThis was an exciting project because I got some validation of my approach. I am NOT an epidemiologist or a professional data scientist. None of the results I show here should be considered authoritative. Still, while I was working on this project I saw this article in the “Wall Street Journal” which referenced the work by Dr. Trevor Bedford, an epidemiologist at the University of Washington. He took the same approach I did and got about the same result.\n\n\n\n\nAquire and Clean Data\n\n\nThere is no shortage of data to work with. Here we will use the NY Times COVID tracking data set which is updated daily. The package covid19nytimes lets us refresh the data on demand.\n\n# correlate deaths and cases by state\nlibrary(tidyverse)\nlibrary(covid19nytimes)\nlibrary(timetk)\nlibrary(lubridate)\nlibrary(broom)\nlibrary(knitr)\n\n# source https://github.com/nytimes/covid-19-data.git\nus_states_long <- covid19nytimes::refresh_covid19nytimes_states()\n\n# if link is broken\n#load(\"../data/us_states_long.rdata\")\n\n# use data from November 15 to stay consistent with text narrative\ncutoff_start <- as.Date(\"2020-03-15\") # not widespread enough until then\ncutoff_end <- max(us_states_long$date) -7 # discard last week since there are reporting lags\n\nus_states_long <- us_states_long %>% filter(date >= cutoff_start)\nus_states_long <- us_states_long %>% filter(date <= cutoff_end)\n# Remove tiny territories\nterritories <- c(\"Guam\",\"Northern Mariana Islands\")\nus_states_long <- us_states_long %>% filter(!(location %in% territories))\nsave(us_states_long,file=\"us_states_long.rdata\")\nus_states_long %>% head() %>% kable()\n\n\n\n\n\ndate\n\n\nlocation\n\n\nlocation_type\n\n\nlocation_code\n\n\nlocation_code_type\n\n\ndata_type\n\n\nvalue\n\n\n\n\n\n\n2020-11-28\n\n\nAlabama\n\n\nstate\n\n\n01\n\n\nfips_code\n\n\ncases_total\n\n\n244993\n\n\n\n\n2020-11-28\n\n\nAlabama\n\n\nstate\n\n\n01\n\n\nfips_code\n\n\ndeaths_total\n\n\n3572\n\n\n\n\n2020-11-28\n\n\nAlaska\n\n\nstate\n\n\n02\n\n\nfips_code\n\n\ncases_total\n\n\n31279\n\n\n\n\n2020-11-28\n\n\nAlaska\n\n\nstate\n\n\n02\n\n\nfips_code\n\n\ndeaths_total\n\n\n115\n\n\n\n\n2020-11-28\n\n\nArizona\n\n\nstate\n\n\n04\n\n\nfips_code\n\n\ncases_total\n\n\n322774\n\n\n\n\n2020-11-28\n\n\nArizona\n\n\nstate\n\n\n04\n\n\nfips_code\n\n\ndeaths_total\n\n\n6624\n\n\n\n\n\n\nThe NY Times data is presented in a “long” format. When we start modeling, long will suit us well but first we have to add features to help us and that will require pivoting to wide, adding features and then back to long. The daily data is so irregular the first features we will add are 7-day moving averages to smooth the series. We’ll also do a nation-level analysis first so we aggregate the state data as well.\n\n# Create rolling average changes\n# pivot wider\n# this will also be needed when we create lags\nus_states <- us_states_long %>%\n  # discard dates before cases were tracked.\n  filter(date > as.Date(\"2020-03-01\")) %>% \n  pivot_wider(names_from=\"data_type\",values_from=\"value\") %>% \n  rename(state=location) %>%\n  select(date,state,cases_total,deaths_total) %>%\n  mutate(state = as_factor(state)) %>% \n  arrange(state,date) %>% \n  group_by(state) %>%\n  #smooth the data with 7 day moving average\n  mutate(cases_7day = (cases_total - lag(cases_total,7))/7) %>%\n  mutate(deaths_7day = (deaths_total - lag(deaths_total,7))/7) %>%\n  {.}\n\n# national analysis\n# ----------------------------------------------\n# aggregate state to national\nus <- us_states %>%\n  group_by(date) %>% \n  summarize(across(.cols=where(is.double),\n                   .fns = function(x)sum(x,na.rm = T),\n                   .names=\"{.col}\"))\n\nus[10:20,] %>% kable()\n\n\n\n\n\ndate\n\n\ncases_total\n\n\ndeaths_total\n\n\ncases_7day\n\n\ndeaths_7day\n\n\n\n\n\n\n2020-03-24\n\n\n53906\n\n\n784\n\n\n6857.571\n\n\n95.28571\n\n\n\n\n2020-03-25\n\n\n68540\n\n\n1053\n\n\n8599.714\n\n\n127.28571\n\n\n\n\n2020-03-26\n\n\n85521\n\n\n1352\n\n\n10448.571\n\n\n162.85714\n\n\n\n\n2020-03-27\n\n\n102847\n\n\n1769\n\n\n12121.286\n\n\n213.14286\n\n\n\n\n2020-03-28\n\n\n123907\n\n\n2299\n\n\n14199.143\n\n\n277.00000\n\n\n\n\n2020-03-29\n\n\n142426\n\n\n2717\n\n\n15625.714\n\n\n322.85714\n\n\n\n\n2020-03-30\n\n\n163893\n\n\n3367\n\n\n17202.429\n\n\n398.42857\n\n\n\n\n2020-03-31\n\n\n188320\n\n\n4302\n\n\n19202.000\n\n\n502.57143\n\n\n\n\n2020-04-01\n\n\n215238\n\n\n5321\n\n\n20956.857\n\n\n609.71429\n\n\n\n\n2020-04-02\n\n\n244948\n\n\n6537\n\n\n22775.286\n\n\n740.71429\n\n\n\n\n2020-04-03\n\n\n277264\n\n\n7927\n\n\n24916.714\n\n\n879.71429\n\n\n\n\n\n\n\n\nExploratory Data Analysis\n\n\nWe might be tempted to simply regress deaths vs. cases but a scatter plot shows us that would not be satisfactory. As it turns out, the relationship of cases and deaths is strongly conditioned on date. This reflects the declining mortality rate as we have come to better understand the disease.\n\n# does a simple scatterplot tell us anything \n# about the relationship of deaths to cases? No.\nus %>% \n  ggplot(aes(deaths_7day,cases_7day)) + geom_point() +\n  labs(title = \"Not Useful\",\n       caption = \"Source: NY Times, Arthur Steinmetz\")\n\n\n\n\nWe can get much more insight plotting smoothed deaths and cases over time. It is generally bad form to use two different y axes on a single plot but but this example adds insight. A couple of observations are obvious. First when cases start to rise, deaths follow with a lag. Second, we have had three spikes in cases so far and in each successive instance the mortality has risen by a smaller amount. This suggests that, thankfully, we are getting better at treating this disease. It is NOT a function of increased testing because positivity rates have not been falling.\n\n#visualize the relationship between rolling average of weekly cases and deaths\ncoeff <- 30\nus %>% \n  ggplot(aes(date,cases_7day)) + geom_line(color=\"orange\") +\n  theme(legend.position = \"none\") +\n  geom_line(aes(x=date,y=deaths_7day*coeff),color=\"red\") +\n  scale_y_continuous(labels = scales::comma,\n                     name = \"Cases\",\n                     sec.axis = sec_axis(deaths_7day~./coeff,\n                                         name=\"Deaths\",\n                                         labels = scales::comma)) +\n  theme(\n    axis.title.y = element_text(color = \"orange\", size=13),\n    axis.title.y.right = element_text(color = \"red\", size=13)\n  ) +\n  labs(title =  \"U.S. Cases vs. Deaths\",\n       subtitle = \"7-Day Average\",\n       caption = \"Source: NY Times, Arthur Steinmetz\",\n       x = \"Date\")\n\n This illustrates a problem for any modeling we might do.It looks like the more cases surge, the less the impact on deaths. This is NOT a valid conclusion. A simple regression of deaths vs. cases and time shows the passage of time has more explanatory power than cases in predicting deaths so we have to take that into account.\n\n# passage of time affects deaths more than cases\nlm(deaths_7day~cases_7day+date,data=us) %>% tidy()\n## # A tibble: 3 x 5\n##   term           estimate   std.error statistic  p.value\n##   <chr>             <dbl>       <dbl>     <dbl>    <dbl>\n## 1 (Intercept) 76542.      10054.           7.61 5.15e-13\n## 2 cases_7day      0.00828     0.00112      7.41 1.86e-12\n## 3 date           -4.11        0.547       -7.52 9.11e-13\n\n\n\nBuild Some Models\n\n\nWe’ll approach this by running regression models of deaths and varying lags (actually leads) of cases. We chose to lead deaths as opposed to lagging cases because it will allow us to make predictions about the future of deaths given cases today. We include the date as a variable as well. Once we’ve run regressions against each lead period, we’ll chose the lead period that has the best fit (R-Squared) to the data.\n\n\nThe requires a lot of leads and a lot of models. Fortunately, R provides the tools to make this work very simple and well organized. First we add new columns for each lead period using timetk::tk_augment_lags. This one function call does all the work but it only does lags so we have to futz with it a bit to get leads.\n\n\nI chose to add forty days of leads. I don’t really think that long a lead is realistic and, given the pandemic has been around only nine months, there aren’t as many data points forty days ahead. Still, I want to see the behavior of the models. Once we have created the leads we remove any dates for which we don’t have led deaths.\n\n#create columns for deaths led 0 to 40 days ahead\nmax_lead <- 40\nus_lags <- us %>%\n  # create lags by day\n  tk_augment_lags(deaths_7day,.lags = 0:-max_lead,.names=\"auto\")\n  # fix names to remove minus sign\n  names(us_lags) <- names(us_lags) %>% str_replace_all(\"lag-|lag\",\"lead\")\n\n# use only case dates where we have complete future knowledge of deaths for all lead times.\nus_lags <- us_lags %>% filter(date < cutoff_end-max_lead)\n\nus_lags[1:10,1:7] %>% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndate\n\n\ncases_total\n\n\ndeaths_total\n\n\ncases_7day\n\n\ndeaths_7day\n\n\ndeaths_7day_lead0\n\n\ndeaths_7day_lead1\n\n\n\n\n\n\n2020-03-15\n\n\n3597\n\n\n68\n\n\n0.000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n\n\n2020-03-16\n\n\n4504\n\n\n91\n\n\n0.000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n\n\n2020-03-17\n\n\n5903\n\n\n117\n\n\n0.000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n\n\n2020-03-18\n\n\n8342\n\n\n162\n\n\n0.000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n\n\n2020-03-19\n\n\n12381\n\n\n212\n\n\n0.000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n\n\n2020-03-20\n\n\n17998\n\n\n277\n\n\n0.000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n\n\n2020-03-21\n\n\n24513\n\n\n360\n\n\n0.000\n\n\n0.00000\n\n\n0.00000\n\n\n55.57143\n\n\n\n\n2020-03-22\n\n\n33046\n\n\n457\n\n\n4204.714\n\n\n55.57143\n\n\n55.57143\n\n\n69.57143\n\n\n\n\n2020-03-23\n\n\n43476\n\n\n578\n\n\n5565.143\n\n\n69.57143\n\n\n69.57143\n\n\n95.28571\n\n\n\n\n2020-03-24\n\n\n53906\n\n\n784\n\n\n6857.571\n\n\n95.28571\n\n\n95.28571\n\n\n127.28571\n\n\n\n\n\n\n…etc up to 40\n\n\nNow we start the job of actually building the linear models and seeing the real power of the tidy modeling framework. Since we have our lead days in columns we revert back to long-form data. For each date we have a case count and 40 lead days with the corresponding death count. As will be seen below, the decline in the fatality rate has been non-linear, so we use a second-order polynomial to regress the date variable.\n\n\nOur workflow looks like this:\n\n\n\nCreate the lags using tk_augment_lag (above).\n\n\npivot to long form.\n\n\nnest the data by lead day and state.\n\n\nmap the data set for each lead day to a regression model.\n\n\nPull out the adjusted R-Squared using glance for each model to determine the best fit lead time.\n\n\n\nThe result is a data frame with our lead times, the nested raw data, model and R-squared for each lead time.\n\n# make long form to nest\n# initialize models data frame\nmodels <- us_lags %>% ungroup %>% \n  pivot_longer(cols = contains(\"lead\"),\n               names_to = \"lead\",\n               values_to = \"led_deaths\") %>% \n  select(date,cases_7day,lead,led_deaths) %>% \n  mutate(lead = as.numeric(str_remove(lead,\"deaths_7day_lead\"))) %>% \n\n  nest(data=c(date,cases_7day,led_deaths)) %>% \n  # Run a regression on lagged cases and date vs deaths\n  mutate(model = map(data,\n                     function(df) \n                       lm(led_deaths~cases_7day+poly(date,2),data = df)))\n\n# Add regression coefficient\n# get adjusted r squared\nmodels <- models %>% \n  mutate(adj_r = map(model,function(x) glance(x) %>% \n                       pull(adj.r.squared))\n         %>% unlist)\nmodels\n## # A tibble: 41 x 4\n##     lead data               model  adj_r\n##    <dbl> <list>             <list> <dbl>\n##  1     0 <tibble [218 x 3]> <lm>   0.164\n##  2     1 <tibble [218 x 3]> <lm>   0.187\n##  3     2 <tibble [218 x 3]> <lm>   0.212\n##  4     3 <tibble [218 x 3]> <lm>   0.241\n##  5     4 <tibble [218 x 3]> <lm>   0.272\n##  6     5 <tibble [218 x 3]> <lm>   0.307\n##  7     6 <tibble [218 x 3]> <lm>   0.343\n##  8     7 <tibble [218 x 3]> <lm>   0.383\n##  9     8 <tibble [218 x 3]> <lm>   0.424\n## 10     9 <tibble [218 x 3]> <lm>   0.467\n## # ... with 31 more rows\n\nTo decide the best-fit lead time we choose the model with the highest R-squared.\n\n# Show model fit by lead time\n# make predictions using best model\nbest_fit <- models %>% \n  summarize(adj_r = max(adj_r)) %>% \n  left_join(models,by= \"adj_r\")\n\nmodels %>%\n  ggplot(aes(lead,adj_r)) + geom_line() +\n  labs(subtitle = paste(\"Best fit lead =\",best_fit$lead,\"days\"),\n       title = \"Model Fit By Lag Days\",\n       x = \"Lead Time in Days for Deaths\",\n       caption = \"Source: NY Times, Arthur Steinmetz\",\n       y= \"Adjusted R-squared\")\n\n We can have some confidence that we are not overfitting the date variable because the significance of the case count remains. With a high enough degree polynomial on the date variable, cases would vanish in importance.\n\nbest_fit$model[[1]] %>% tidy()\n## # A tibble: 4 x 5\n##   term             estimate  std.error statistic  p.value\n##   <chr>               <dbl>      <dbl>     <dbl>    <dbl>\n## 1 (Intercept)      436.      38.0           11.5 4.21e-24\n## 2 cases_7day         0.0167   0.000993      16.8 5.45e-41\n## 3 poly(date, 2)1 -7306.     227.           -32.2 5.87e-84\n## 4 poly(date, 2)2  4511.     167.            26.9 1.02e-70\n\n\n\nMake Predictions\n\n\nThe best-fit lead time is 23 days but let’s use predict to see how well our model fits to the actual deaths.\n\n# ------------------------------------------\n# see how well our model predicts\n# Function to create prediction plot\nshow_predictions <- function(single_model,n.ahead){\n  predicted_deaths = predict(single_model$model[[1]],newdata = us)\n  date = seq.Date(from=min(us$date) + n.ahead,to=max(us$date) + n.ahead,by=1)\n  display = full_join(us,tibble(date,predicted_deaths))\n\n  gg <- display %>% \n    pivot_longer(cols = where(is.numeric)) %>% \n    filter(name %in% c(\"deaths_7day\",\"predicted_deaths\")) %>% \n    ggplot(aes(date,value,color=name)) + geom_line() +\n    labs(title=\"Actual vs. Predicted Deaths\",\n         x = \"Date\", \n         y = \"Count\",\n         caption = \"Source: NY Times, Arthur Steinmetz\")\n  gg\n}\n\nshow_predictions(best_fit,best_fit$lead)\n\n\n\n\nThis is a satisfying result, but sadly shows deaths about to spike. This is despite accounting for the improvements in treatment outcomes we’ve accomplished over the past several months. The 23-day lead time model shows a 1.7% mortality rate over the whole length of observations but conditioned on deaths falling steadily over time.\n\n\n\n\nDeclining Mortality Rate\n\n\nOnce we’ve settled on the appropriate lag time, we can look at the fatality rate per identified case. This is but one possible measure of fatality rate, certainly not THE fatality rate. Testing rate, positivity rate and others variables will affect this measure. We also assume our best-fit lag is stable over time so take the result with a grain of salt. The takeaway should be how it is declining, not exactly what it is.\n\n\nEarly on, only people who were very sick or met strict criteria were tested so, of course, fatality rates (on this metric) were much, much higher. To minimize this we start our measure at the middle of April.\n\n\nSadly, we see that fatality rates are creeping up again.\n\nfatality <- best_fit$data[[1]] %>% \n  filter(cases_7day > 0) %>%\n  filter(date > as.Date(\"2020-04-15\")) %>%\n  mutate(rate = led_deaths/cases_7day)\n\nfatality %>% ggplot(aes(date,rate)) + geom_line() + \n  geom_smooth() +\n  labs(x=\"Date\",y=\"Fatality Rate\",\n       title = \"Fatality Rates are Creeping Up\",\n       subtitle = \"Fatality Rate as a Percentage of Lagged Cases\",\n       caption = \"Source: NY Times, Arthur Steinmetz\") +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\nState-Level Analysis\n\n\nOne problem with the national model is each state saw the arrival of the virus at different times, which suggests there might also be different relationships between cases and deaths. Looking at a few selected states illustrates this.\n\n# ------------------------------------------\n# state by state analysis\n\nstate_subset <- c(\"New York\",\"Texas\",\"California\",\"Ohio\")\n\n# illustrate selected states\nus_states %>% \n  filter(state %in% state_subset) %>% \n  ggplot(aes(date,cases_7day)) + geom_line(color=\"orange\") +\n  facet_wrap(~state,scales = \"free\") +\n  theme(legend.position = \"none\") +\n  geom_line(aes(y=deaths_7day*coeff),color=\"red\") +\n  scale_y_continuous(labels = scales::comma,\n                     name = \"Cases\",\n                     sec.axis = sec_axis(deaths_7day~./coeff,\n                                         name=\"Deaths\",\n                                         labels = scales::comma)) +\n  theme(\n    axis.title.y = element_text(color = \"orange\", size=13),\n    axis.title.y.right = element_text(color = \"red\", size=13)\n  ) +\n  labs(title =  \"U.S. Cases vs. Deaths\",\n       subtitle = \"7-Day Average\",\n       caption = \"Source: NY Times, Arthur Steinmetz\",\n       x = \"Date\")\n\n\n\n\nIn particular we note New York, where the virus arrived early and circulated undetected for weeks. Testing was rare and we did not know much about the course of the disease so the death toll was much worse. Tests were often not conducted until the disease was in advanced stages so we would expect the lag to be shorter.\n\n\nIn Texas, the virus arrived later. There it looks like the consequences of the first wave were less dire and the lag was longer.\n\n\n\n\nRun State Models\n\n\nNow we can run the same workflow we used above over the state-by-state data. Our data set is much larger because we have a full set of lags for each state but building our data frame of list columns is just as easy.\n\n\nLooking at the lags by state shows similar results to the national model, on average, as we assume, but the dispersion is large. Early in the pandemic, in New York, cases were diagnosed only for people who were already sick so the lead time before death was much shorter.\n\n# create lags\nus_states_lags <- us_states %>%\n  # create lags by day\n  tk_augment_lags(deaths_7day,.lags = -max_lead:0,.names=\"auto\") %>% \n  {.}\n# fix names to remove minus sign\nnames(us_states_lags) <- names(us_states_lags) %>% str_replace_all(\"lag-\",\"lead\")\n\n# make long form to nest\n# initialize models data frame\nmodels_st <- us_states_lags %>% ungroup %>% \n  pivot_longer(cols = contains(\"lead\"),\n               names_to = \"lead\",\n               values_to = \"led_deaths\") %>% \n  select(state,date,cases_7day,lead,led_deaths) %>% \n  mutate(lead = as.numeric(str_remove(lead,\"deaths_7day_lead\"))) %>% \n  {.}\n\n# make separate tibbles for each regression\nmodels_st <- models_st %>% \n  nest(data=c(date,cases_7day,led_deaths)) %>% \n  arrange(lead)\n\n#Run a linear regression on lagged cases and date vs deaths\nmodels_st <- models_st %>% \n  mutate(model = map(data,\n                     function(df) \n                       lm(led_deaths~cases_7day+poly(date,2),data = df)))\n\n\n# Add regression coefficient\n# get adjusted r squared\nmodels_st <- models_st %>% \n  mutate(adj_r = map(model,function(x) glance(x) %>% \n                       pull(adj.r.squared))\n         %>% unlist)\n\nmodels_st %>%\n  filter(state %in% state_subset) %>% \n  ggplot(aes(lead,adj_r)) + geom_line() +\n  facet_wrap(~state) +\n  labs(title = \"Best Fit Lead Time\",\n       caption = \"Source: NY Times, Arthur Steinmetz\")\n\n\n\n\nTo see how the fit looks for the data set as a whole we look at a histogram of all the state R-squareds. We see many of the state models have a worse accuracy than the national model.\n\n# best fit lag by state\nbest_fit_st <- models_st %>% \n  group_by(state) %>% \n  summarize(adj_r = max(adj_r)) %>% \n  left_join(models_st)\n\nbest_fit_st %>% ggplot(aes(adj_r)) + \n  geom_histogram(bins = 10,color=\"white\") +\n  geom_vline(xintercept = best_fit$adj_r[[1]],color=\"red\") +\n  annotate(geom=\"text\",x=0.75,y=18,label=\"Adj-R in National Model\") +\n  labs(y = \"State Count\",\n       x=\"Adjusted R-Squared\",\n       title = \"Goodness of Fit of State Models\",\n       caption = \"Source:NY Times,Arthur Steinmetz\")\n\n\n\n\nThere are vast differences in the best-fit lead times across the states but the distribution is in agreement with our national model.\n\nbest_fit_st %>% ggplot(aes(lead)) + \n  geom_histogram(binwidth = 5,color=\"white\") +\n  scale_y_continuous(labels = scales::label_number(accuracy = 1)) +\n  geom_vline(xintercept = best_fit$lead[[1]],color=\"red\") +\n  annotate(geom=\"text\",x=best_fit$lead[[1]]+7,y=10,label=\"Lead in National Model\") +\n  labs(y = \"State Count\",\n    x=\"Best Fit Model Days from Case to Death\",\n    title = \"COVID-19 Lag Time From Cases to Death\",\n    caption = \"Source:NY Times,Arthur Steinmetz\")\n\n\n\n\n\n\nValidate with Individual Case Data from Ohio\n\n\nThis whole exercise has involved proxying deaths by time and quantity of positive tests. Ideally, we should look at longitudinal data which follows each individual. The state of Ohio provides that so we’ll look at just this one state to provide a reality check on the foregoing analysis. In our proxy model, Ohio shows a best-fit lead time of 31 days, which is much longer than our national-level model.\n\n# ----------------------------------------------------\nbest_fit_st %>% select(-data,-model) %>% filter(state == \"Ohio\") %>% kable()\n\n\n\n\n\nstate\n\n\nadj_r\n\n\nlead\n\n\n\n\n\n\nOhio\n\n\n0.7548416\n\n\n31\n\n\n\n\n\n\nThe caveat here is the NY Times data uses the “case” date which is presumably the date a positive test is recorded. The Ohio data uses “onset” date, which is the date the “illness began.” That is not necessarily the same as the test date.\n\n# source: https://coronavirus.ohio.gov/static/dashboards/COVIDSummaryData.csv\nohio_raw <- read_csv(\"https://coronavirus.ohio.gov/static/dashboards/COVIDSummaryData.csv\", \n                     col_types = cols(`Admission Date` = col_date(format = \"%m/%d/%Y\"), \n                                      `Date Of Death` = col_date(format = \"%m/%d/%Y\"), \n                                      `Onset Date` = col_date(format = \"%m/%d/%Y\")))\n\n# helper function to fix column names to best practice\nfix_df_colnames <- function(df){\n  names(df)<-names(df) %>% \n    str_replace_all(c(\" \" = \"_\" , \",\" = \"\" )) %>% \n    tolower()\n  return(df)\n}\n\n# clean up the data\nohio <- ohio_raw %>% \n  rename(death_count = `Death Due to Illness Count`) %>% \n  filter(County != \"Grand Total\") %>%\n  fix_df_colnames() %>% \n  # data not clean before middle of march\n  filter(onset_date >= cutoff_start)\n\nHow comparable are these data sets? Let’s compare the NY Times case count and dates to the Ohio “Illness Onset” dates.\n\n# create rolling average function\nmean_roll_7 <- slidify(mean, .period = 7, .align = \"right\")\n\ncomps <- ohio %>% \n  group_by(onset_date) %>% \n  summarise(OH = sum(case_count),.groups = \"drop\") %>%\n  mutate(OH = mean_roll_7(OH)) %>% \n  ungroup() %>% \n  mutate(state = \"Ohio\") %>% \n  rename(date=onset_date) %>% \n  left_join(us_states,by=c(\"date\",\"state\")) %>% \n  transmute(date,OH,NYTimes = cases_7day)\n\ncomps %>% \n  pivot_longer(c(\"OH\",\"NYTimes\"),names_to = \"source\",values_to = \"count\") %>%  \n  ggplot(aes(date,count,color=source)) + geom_line() +\n  labs(title =  \"Case Counts from Different Sources\",\n       caption = \"Source: State of Ohio, NY Times\",\n       subtitle = \"NY Times and State of Ohio\",\n       x = \"Date\",\n       y = \"Daily Case Count (7-day Rolling Average)\")\n\n We clearly see the numbers line up almost exactly but the Ohio data runs about 4 days ahead of the NY Times data.\n\n\nFor each individual death, we subtract the onset date from the death date. Then we aggregate the county-level data to statewide and daily data to weekly. Then take the weekly mean of deaths.\n\n# aggregate the data to weekly\nohio <- ohio %>% \n  mutate(onset_to_death = as.numeric(date_of_death - onset_date),\n         onset_year = year(onset_date),\n         onset_week = epiweek(onset_date))\n\n\nonset_to_death <- ohio %>%\n  filter(death_count > 0) %>% \n  group_by(onset_year,onset_week) %>%\n  summarise(death_count_sum = sum(death_count),\n            mean_onset_to_death = weighted.mean(onset_to_death,\n                                                death_count,\n                                                na.rm = TRUE)) %>%\n  mutate(date=as.Date(paste(onset_year,onset_week,1),\"%Y %U %u\")) %>%\n  {.}\n\nonset_to_death %>% ggplot(aes(date,death_count_sum)) + geom_col() +\n    labs(title =  \"Ohio Weekly Deaths\",\n       caption = \"Source: State of Ohio, Arthur Steinmetz\",\n       subtitle = \"Based on Illness Onset Date\",\n       x = \"Date of Illness Onset\",\n       y = \"Deaths\")\n\n When we measure the average lag, we find that it has been fairly stable over time in Ohio. Unfortunately, it differs substantially from our proxy model using untracked cases.\n\n# helper function to annotate plots \npos_index <- function(index_vec,fraction){\n  return(index_vec[round(length(index_vec)*fraction)])\n}\n\navg_lag <- round(mean(onset_to_death$mean_onset_to_death))\n\nonset_to_death %>% ggplot(aes(date,mean_onset_to_death)) + \n  geom_col() +\n  geom_hline(yintercept = avg_lag) +\n  annotate(geom=\"text\",\n           label=paste(\"Average Lag =\",round(avg_lag)),\n           y=20,x=pos_index(onset_to_death$date,.8)) +\n  labs(x = \"Onset Date\",\n       y = \"Mean Onset to Death\",\n       title = \"Ohio Days from Illness Onset Until Death Over Time\",\n       caption = \"Source: State of Ohio, Arthur Steinmetz\",\n       subtitle = paste(\"Average =\",\n                     avg_lag,\"Days\"))\n\n Note the drop off at the end of the date range. This is because we don’t yet know the outcome of the most recently recorded cases. Generally, while we have been successful in lowering the fatality rate of this disease, the duration from onset to death for those cases which are fatal has not changed much, at least in Ohio.\n\n\nSince we have the actual number of deaths associated with every onset date we can calculate the “true” fatality rate. As mentioned, the fatality rate of the more recent cases is not yet known. Also the data is too sparse at the front of the series so we cut off the head and the tail of the data.\n\nohio_fatality_rate <- ohio %>% \n  group_by(onset_date) %>% \n  summarize(case_count = sum(case_count),\n            death_count = sum(death_count),.groups=\"drop\") %>% \n  mutate(fatality_rate = death_count/case_count) %>% \n  mutate(fatality_rate_7day = mean_roll_7(fatality_rate)) %>% \n# filter out most recent cases we we don't know outcome yet\n  filter(onset_date < max(onset_date)-30) \n\nohio_fatality_rate %>% \n  filter(onset_date > as.Date(\"2020-04-15\")) %>% \n  ggplot(aes(onset_date,fatality_rate_7day)) + geom_line() +\n  geom_smooth() +\n  labs(x=\"Illness Onset Date\",y=\"Ohio Fatality Rate\",\n       caption = \"Source: State of Ohio, Arthur Steinmetz\",\n       title = \"Ohio Fatality Rate as a Percentage of Tracked Cases\") +\n  scale_y_continuous(labels = scales::percent,breaks = seq(0,0.12,by=.01))\n\n\n\n\nThe fatality rate in Ohio seems to have been worse than our national model but it is coming down. Again, this result comes from a different methodology than our proxy model.\n\n\n\n\nConclusion\n\n\nAmong the vexing aspects of this terrible pandemic is that we don’t know what the gold standard is for treatment and prevention. We are learning as we go. The good news is we ARE learning. For a data analyst the challenge is the evolving relationship of of all of the disparate data. Here we have gotten some insight into the duration between a positive test and mortality. We can’t have high confidence that our proxy model using aggregate cases is strictly accurate because the longitudinal data from Ohio shows a different lag. We have clearly seen that mortality has been declining but our model suggests that death will nonetheless surge along with the autumn surge in cases.\n\n\nWhat are the further avenues for modeling? There is a wealth of data around behavior and demographics with this disease that we don’t fully understand yet. On the analytics side, we might get more sophisticated with our modeling. We have only scratched the surface of the tidymodels framework and we might apply fancier predictive models than linear regression. Is the drop in the fatality rate we saw early in the pandemic real? Only people who were actually sick got tested in the early days. Now, many positive tests are from asymptomatic people. Finally, the disagreement between the case proxy model and the longitudinal data in Ohio shows there is more work to be done."
  },
  {
    "objectID": "posts/2023-01-24_Switching-to-Quarto/2023-1-24_switching-to-quarto.html",
    "href": "posts/2023-01-24_Switching-to-Quarto/2023-1-24_switching-to-quarto.html",
    "title": "Switching to Quarto from Blogdown",
    "section": "",
    "text": "It all started when I decided to change up my Hugo theme. Up until that point I was happily using the Blogdown add-in for RStudio to initiate new blog posts. At some point the default directory structure for Hugo blogs changed and when I tried to update my theme the whole web site got impossibly messy and finally broke. I managed to cobble it back together but I really had no idea what I was doing and became afraid of messing with it any more. This led to bloggers block.\nWhen Quarto came on the scene I was attracted to the idea of starting fresh but also intimidated by the thought of porting all my old content from R Markdown to Quarto. Ideally you should just be able to render the R Markdown code in Quarto and it will just work. For me, the problem was that many of my posts would no longer render in R because of package updates or out-of-date web links. Quarto solves that problem with the freeze option but it only works for content that has been rendered at least once with Quarto.\nI needn’t have fretted. It turns out porting old blog or web content is ridiculously easy though the existing Quarto guides don’t discuss this one simple trick. Here’s the TL;DR version: all you have to do is change the file extension of your fully rendered HTML files from .html to .qmd and Quarto will happily render them, wrapping its own HTML code around the old HTML but not otherwise messing with it. Any theming or subsequent changes in theming will be properly rendered. You can get rid of your old R Markdown files (though they will live in GitHub forever, right?).\nI never did figure out the directory structure of a Hugo web site. Fortunately, Quarto is much simpler. The main directory and subdirectories of your blog contains all the Quarto files. You can name them as you like. One special directory called _site contains the rendered HTML files. It will be created the first time you render your site. There are a few more things to do to bring it all together. Let’s go through them step-by-step.\n\nAssuming you Create New Project in RStudio a subdirectory will be created called posts. Create a subfolder in your posts folder to hold converted file. Use any name but I like the form yyyy-mm-dd_name-of-post. This will allow displaying the posts in date order.\nCopy old rendered HTML document (not the RMD markdown document) to the new folder.\nRename copied HTML file from <filename>.html to <filename>.qmd.\nLoad QMD file into RStudio for some light editing.\nBlogdown uses tags and categories in the YAML header while Quarto uses only categories. In the editor, combine the YAML header tags and categories into just categories.\nSo this:\ncategories:\n  - R\ntags:\n  - quarto\n  - blogging\nbecomes this:\ncategories:\n  - R\n  - quarto\n  - blogging\nOf course you want your code snippets to have a consistent theme. Quarto uses a different HTML class id to style code snippets but that’s easily fixed. In the editor Search/Replace all instances of class=\"r\" to class=\"sourceCode r\".\nMost of your posts will have images you added or images that were generated during the render. We need to put those old images in a place where Quarto can find them. Create subfolder below the one just created called img.\nCopy any image files from the old version over to the img directory we just made.\nIn the editor, fix path names of any image files to point to the img folder. A quick search-and-replace should do it. The HTML tags will look like:\n<img src=\"somwhere_else/my_old_folder/unnamed-chunk-13-1.png\" width=\"672\" />\nwhich you should change to:\n<img src=\"img/unnamed-chunk-13-1.png\" width=\"672\" />\nDoes your blog use any Javascript HTML widgets? In my case a post that used the the Plotly package created some. These will be found in your old site in the rmarkdown-libs folder. Copy this folder over to the base directory of your new blog. Back in the editor make sure any lines of HTML that contain rmarkdown-libs have a valid path to new location. As a side note, if you render the file as R code, Quarto will put those widgets in a directory called site-libs.\nSave and Render. Done!\n\nFrom this point you can go down the rabbit hole and play with all the theming and formatting options that Quarto allows. You’ll make most of these changes in your index.qmd and _quarto.yml files. Any changes you make will be reflected in you old posts. Easy peasy!\nThanks to Emily Robinson for jump starting me on this project. She pointed me to great resources on writing a Quarto blog and convinced me that “you can do it!”\nBonus: I really liked the Blogdown add-in for R Studio as it made initiating a new post very easy. Thomas Mock has made a start on this with a quick function to start a new Quarto post. Check it out."
  }
]