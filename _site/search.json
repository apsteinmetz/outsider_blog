[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "July Fourth by Grandma Moses\n\n\nI am not a data scientist. Others have coined good terms like “Data Nerd” and “Citizen Data Scientist.” I’ll coin another: “Outsider Data Scientist.” I would style myself in the likeness of an “outsider” artist, Grandma Moses. She was an American artist who didn’t pick up a brush between childhood and old age, and had no formal training. The works she produced would never be mistaken for the old masters’ but they had a certain charm. Perhaps I might strive for that. I am also getting on in years.\nI play around with R for fun. I enjoy thinking up ways to present complex information in a simple, compelling way. I attended an R conference where an axiom was presented by Dave Robertson that the value of information “still” on your computer is approximately zero and the value of information out in the world is infinitely more. Even if it is small that’s infinitely more than zero, right? That emboldened me to put stuff “out there.” Perhaps someone else might find it interesting. At a minimum this blog forces more rigor in my own thinking.\n“Outsider” is a bit of a misnomer. I’m sure Grandma Moses saw an another painting or two in her life. Painting was pretty mature before Ms. Moses picked up her brush, but this whole data science thing has exploded in the last few years. It is very exciting to even be on the periphery of the event horizon. I am indebted to the R community for all the examples they have shared through R-Bloggers, Stack Overflow and Twitter. The tools provided by Posit (formerly RStudio) are the bomb!\nThis is a personal side project in no way associated with any organization I am affiliated with. My opinions here are mine alone and any data I present here is neither proprietary nor is it warranted to be correct or accurate. Nothing I say here should be construed as investment advice.\nI used to work at OppenheimerFunds Inc. before it was acquired by Invesco, first as a portfolio manager of global macro fixed income and ultimately as CEO. I am the former board chair at the National Museum of Mathematics, MoMath.org, where I rubbed shoulders (though not in creepy way) with people who are really, really smart. Visit the museum when you are in NYC. We are making math cool! Finally, I am on the board of “Rock the Street, Wall Street” which brings financial literacy programs into high school classrooms to encourage girls to become interested in finance. We need more diversity in finance.\n\nArt Steinmetz\n\nemail: apsteinmetz@yahoo.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "outsider_blog",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSwitching to Quarto from Blogdown\n\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2022\n\n\nArt Steinmetz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRick and Morty Palettes\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2019\n\n\nArt Steinmetz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs Free Pre-K in NYC Favoring the Rich?\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2018\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nNew Winter Sports for New Countries\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2018\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nLive Fast, Die Young, Stay Pretty?\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2018\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nPlumbing the Depths of My Soul (in Facebook)\n\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2017\n\n\nArt Steinmetz\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence.html",
    "href": "posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence.html",
    "title": "Plumbing the Depths of My Soul (in Facebook)",
    "section": "",
    "text": "First post! Let’s start out nice and easy. No big data machine learning or heavy stats. This post will merely explore the depths of my soul through a meta-analysis of every one of my Facebook posts. Meta-navel gazing, if you will.\nPerhaps you are not all that interested in the plumbing the depths of my soul. Still, you may be interested in seeing how you can do an analyis of your own Facebook life in the comfort of your own home. If so, read on!\nWe will (lightly) cover web scraping, sentiment analysis, tests of significance and visualize it with a generous helping of ggplot. Note I use the tidyverse/dplyr vernacular. This is fast becoming a dialect of R. I quite like it but its syntax is different than traditional R. It produces sometimes slower, but much more readable, code. Basically, you “pipe” data tables through action verbs using the pipe operator (“%>%”).\nLet’s go do some outsider data science!\nStart by loading needed packages.\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(wordcloud)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggplot2)\nlibrary(zoo)\nlibrary(reshape2)\nlibrary(lubridate)\n\n\n#make explicit so kableExtra doesn't complain later\noptions(knitr.table.format = \"html\") \n\n\nFetch and clean all the words in my Facebook posts\nFacebook lets you download a log of all your activity at https://Facebook.com/settings. Look toward the bottom of the page for the download link. You will get an email with a link to a zipped set of html files. These are what I’ll be using for the analysis.\nFirst let’s get all my comments since the dawn of my Facebook existence.\n\npath='data/'\nraw_timeline<- read_html(paste0(path,\"timeline.htm\"),encoding=\"UTC-8\")\n\nNow that we have the raw data we need to extract the just the text of the comments. Visually inspecting the raw html file reveals that all of the comments I wrote have the tag <div class=\"comment\"> so I construct an xpath selector to grab those nodes then get the text in them. This is what the raw html looks like:\n</p><p><div class=\"meta\">Thursday, November 16, 2017 at 1:17pm EST</div> <div class=\"comment\">I’m sure you are all thinking “what does this mean for Al Franken?”</div> </p><p> <div class=\"meta\">Thursday, November 16, 2017 at 10:44am EST</div> Art Steinmetz shared a link. </p><p>\nThe challenge here is that we want to get the date also which appears BEFORE the comment and has the tag <div class=\"meta\">. Unfortunately, as we see above, merely sharing a link generates this tag without any comment or a different tag class so there are more meta classes than comment classes. Facebook should create a separate XML record for each log activity, but they don’t.\nThe code below seems inelegant to me. for loops in R are non-idiomatic and indicate somebody was steeped in a non vectorized language (like me). I tried without success to craft an xpath expression that would walk backwards when it sees a comment class to get the date. In the end I resorted to the devil I know, a loop.\n\ntimeline_post_nodes <- raw_timeline %>% \n  html_nodes(xpath=\"//div[@class ='comment'] | //div[@class='meta']\")\n\ntimeline_posts1<-NULL\n#the bit below is the slowest part of our project. \n#If you post multiple times a day over years it could take a while.\nfor (n in 1:length(timeline_post_nodes)){\n  if ( html_attr(timeline_post_nodes[n],\"class\")==\"comment\"){\n    post= html_text(timeline_post_nodes[n])\n    date= html_text(timeline_post_nodes[n-1])\n    timeline_posts1<-timeline_posts1 %>% bind_rows(tibble(date,post))\n  }\n}\n\nThe time stamps we extracted are just character strings with no quantitative meaning. Let’s convert the dates in the form of “Saturday November 18 2017 11:12am EST” to a day of the week and a POSIX date/time format that other R functions will understand. First we pull out the day of the week using the comma as a separator but this also separates the month and day from the year, which we don’t want, so we put those back together.\nThis begs the question of whether we should have used a tricker “regular expression” to accomplish this in one step. RegExes are a dark art that I have a lot of admiration for, even if I am a rank neophyte. In this exercise I didn’t think it was worth the time to figure out a “proper” solution when a “simple” one sufficed. Other times I like the puzzle challenge of coming up with a powerful RegEx. There are web sites that are a great help in building them. Try http://regex101.com, for one.\nWith a good date string in hand we can use parse_date() to convert it. Notice the format string we use to accomplish this.\n\ntimeline_posts<-timeline_posts1 %>%\n  mutate(date=sub(\"at \",\"\",date)) %>%\n  separate(date,c(\"doy\",\"date\",\"yeartime\"),sep=\", \") %>%\n  transmute(doy=doy,date=paste(date,yeartime),post=post)\n\n# Now that we've pulled out the day of the week, let's make sure they show in order in plots\n# by making doy and ordered factor.\nday_order<-c(\"Monday\",\"Tuesday\",\"Wednesday\",\n            \"Thursday\",\"Friday\",\"Saturday\", \n            \"Sunday\")\n\ntimeline_posts$doy<-factor(timeline_posts$doy,levels = day_order)\n\ntimeline_posts<-timeline_posts %>% \n  mutate(date = str_remove(date,\" EST| EDT\")) |> \n  mutate(date = parse_datetime(date,\n                               format=\"%B %d %Y %I:%M%p\",\n                               locale = locale(tz = \"US/Eastern\")))\n\nkable(head(timeline_posts[1:2,])) %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    doy \n    date \n    post \n  \n \n\n  \n    Saturday \n    2017-11-18 11:12:00 \n    I feel cheated. When I read the fine print I see these guys haven't won the \"Uniformity of Granulation\" award since 1894.  I want the oatmeal that won last year! \n  \n  \n    Saturday \n    2017-11-18 10:41:00 \n    I had a chance to visit Shenzhen this year.  The hardware scene is reminiscent of Blade Runner as you'll see.  This guy prowls the markets to make his own iPhone from scratch. \n  \n\n\n\n\n\nWe now have over 2000 text strings, each representing one post. Since we are working at the word level we need to break up each post into its constituent words.\nFor much of this analysis I am following the example shown at https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html.\nThe unnest_tokens function from the ‘tidytext’ package lets us convert a dataframe with a text column to be one-word-per-row dataframe. How many words are there?\n\nmy_post_words<-  timeline_posts %>%\n  unnest_tokens(word, post)\n\nnrow(my_post_words)\n\n[1] 51347\n\n\nSo we have over fifty thousand words. A lot of them are going to be uninteresting. Although, given that Facebook posts are an excercise in narcissim, you might say all of them are uninteresting to anybody but me.\nAnyway, lets press on. We can use the stop_words data set included with tidytext to to strip out the superfluous words. Note this includes words like ‘accordingly’ which convey little meaning but might be useful in revealing idiosyncratic writting patterns, much like people punctuate their speech with vocal pauses like “like” and “right.” How many words are left after that?\n\ndata(\"stop_words\")\ncleaned_post_words <- my_post_words %>%\n  anti_join(stop_words,by='word')\n\nnrow(cleaned_post_words)\n\n[1] 22465\n\n\n\n\nLook at the most common words\nSo now our data set is clean and tidy. Let’s answer some questions. What are the most common words I use in posts.\n\npopular_words<-cleaned_post_words %>%\n  count(word, sort = TRUE)\nkable(popular_words[1:10,]) %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    word \n    n \n  \n \n\n  \n    day \n    111 \n  \n  \n    â \n    101 \n  \n  \n    people \n    97 \n  \n  \n    time \n    92 \n  \n  \n    kids \n    84 \n  \n  \n    love \n    69 \n  \n  \n    carrie \n    60 \n  \n  \n    guy \n    59 \n  \n  \n    friends \n    56 \n  \n  \n    art \n    48 \n  \n\n\n\n\n\nI don’t know where that “a-hat” character comes from but let’s get rid of it.\n\ncleaned_post_words<- cleaned_post_words%>% \n  mutate(word=str_replace(word,\"â\",\"\")) %>% \n  filter(str_length(word)>0)\npopular_words<-cleaned_post_words %>%\n  count(word, sort = TRUE)\n\nAfter we strip out stop words we have less then 10,000 “real” words left.\nGood to see that my wife’s name is one of my most used words. “Kids,” “friends,” and “love” are no surprise. What’s a good way to visualize this? Word cloud!\nI love word clouds! We can easily display the most used words this way using the wordcloud package.\n\n# We love wordclouds!\n#scalefactor magnifies differences for wordcloud\nscaleFactor=1.3\nmaxWords = 200\n\n\nwordcloud(words = popular_words$word, \n          freq = popular_words$n^scaleFactor,\n          max.words=maxWords, \n          random.order=FALSE,rot.per=0.35, \n          colors=brewer.pal(8, \"Dark2\"),\n          scale = c(3,.3))\n\n\n\n\nI mentioned “Obama” about as often as I mentioned “beer.”\n\n\nDo some sentiment analysis\nI used to be jerk. But, given my age, I am entitled to call myself a curmudgeon instead. That sounds nicer somehow, and excuses my negative reaction to everything. However, given how internet discourse easily sinks into a tit-for-tat of profane hatred, I try to go against type, accentuate the positive and say nothing if I can’t say something nice. That’s the idea. How does my sour nature interact with my better intentions? We can use sentiment analysis to find out. The tidytext package also has serveral lexicons with thousands of words coded by their sentiment. Refer to http://tidytextmining.com for an excellent tutorial on this. Obviously, the isolated word approach has limitations. Context matters and by taking one word at a time we don’t capture that. So, with that caveat, how much of a downer am I?\nFirst, let’s look at the sentiment of my posts on a binary basis. Is the word positive or negative? The “bing” lexicon scores thousands of words that way. Obviously, not all the words we used are in the data set. About a third are, though.\n\ncleaned_post_words %>% \n  inner_join(get_sentiments('bing'),by=\"word\") %>% \n  group_by(sentiment) %>% \n  summarize(count=n()) %>% \n  kable() %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    sentiment \n    count \n  \n \n\n  \n    negative \n    1678 \n  \n  \n    positive \n    1412 \n  \n\n\n\n\n\nWell, then. So I am a downer, on a net basis, but not terribly so.\nWe can make this into a word cloud, too! Here are the words I used divided by sentiment.\n\ncleaned_post_words %>%\n  inner_join(get_sentiments('bing'),by=\"word\") %>%\n  count(word, sentiment, sort = TRUE) %>%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %>%\n  comparison.cloud(colors = c(\"#F8766D\", \"#00BFC4\"),\n                   max.words = 100)\n\n\n\n\nWait a minute! “Trump” is scored as a positive sentiment word! Is this a hidden statement by the author of the lexicon?! Doubtful. It’s “trump,” as in “spades trumps clubs,” not as a proper name. And why is “funny” a negative word? I guess it’s “funny strange,” not “funny ha-ha.” It shows the limitations of this kind of thing.\nA different lexicon scores each word’s sentiment on a scale of minus to positive five. This seems pretty subjective to me but has the benefit of letting us add up the numbers to get a net score. What is my sentiment score over all words I’ve ever written on Facebook (not all, the log doesn’t include comments to other’s posts).\n\nsentiment_score<-cleaned_post_words %>% \n  inner_join(get_sentiments('afinn'),by=\"word\") %>% \n  pull(value) %>% \n  mean()\nsentiment_score\n\n[1] 0.1610233\n\n\nWell, this draws an slightly different conclusion. The net score of my sentiment is +0.16 out of range of -5 to +5. Just barely happy. While I may use more negative than positive words, my positive words are more positive. I suspect the word “love” which we already saw is frequently used (though it is “only” a “3”) accounts for this.\nWhat were my most negative words?\n\nword_scores<-cleaned_post_words %>% \n  inner_join(get_sentiments('afinn'),by=\"word\") %>% \n  group_by(word,value) %>% summarise(count=n())\n\n`summarise()` has grouped output by 'word'. You can override using the\n`.groups` argument.\n\n\n\nword_scores %>%\n        arrange((value)) %>%\n        ungroup() %>%\n        .[1:10,] %>%\n        kable() %>%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\nThis is a family blog so I comment out the code that displays the worst words. Suffice it to say, they are the usual curse words and forms thereof. I am cringing right now. Did I say those things? Yes, well not often, at least, once or twice is typical for each.\nAs I mentioned above, the limitation of this analysis is that it lacks context. For instance, did I call someone a slut? I was briefly horrified when I saw that word. Here is the word in context from 2014: “Less slut-shaming and more perp-jailing.”\nAll these negative words carry more power for me, an old-geezer, than for kids today (kids today!) who let f-bombs roll off their tongues with uncomfortable (to me) ease. Get off my lawn!\nWhat were my positive words?\n\nword_scores %>% arrange(desc(value)) %>% \n  ungroup() %>%\n  .[1:10,] %>% \n  kable() %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    word \n    value \n    count \n  \n \n\n  \n    breathtaking \n    5 \n    1 \n  \n  \n    outstanding \n    5 \n    1 \n  \n  \n    thrilled \n    5 \n    3 \n  \n  \n    amazing \n    4 \n    19 \n  \n  \n    awesome \n    4 \n    19 \n  \n  \n    brilliant \n    4 \n    5 \n  \n  \n    fabulous \n    4 \n    1 \n  \n  \n    fantastic \n    4 \n    2 \n  \n  \n    fun \n    4 \n    45 \n  \n  \n    funnier \n    4 \n    1 \n  \n\n\n\n\n\nWhew! I feel better now. Everything is awesome!\nDid I get happier or sadder over time? We’ll answer that question in a minute.\n\n\nTime Patterns\nThe foregoing analysis just includes posts on my timeline where I made a comment. If we want to know things like when I’m active on Facebook we need to look at all activity. Again, Facebook doesn’t separately tag different activities. Let’s go back over all the activity to pull out just the timestamps, but all of them this time.\n\nactivity_times <- tibble(date = raw_timeline %>% \n                             html_nodes(xpath=\"//div[@class='meta']\") %>% \n                             html_text()\n                              ) %>%\n  mutate(date=sub(\"at \",\"\",date)) %>%\n  separate(date,c(\"doy\",\"date\",\"yeartime\"),sep=\", \") %>%\n  transmute(doy=doy,date=paste(date,yeartime)) %>%\n  mutate(date = str_remove(date,\" EST| EDT\")) |> \n  mutate(date = parse_datetime(date,\n                               format=\"%B %d %Y %I:%M%p\",\n                               locale = locale(tz = \"US/Eastern\")))\n\nactivity_times$doy<-factor(activity_times$doy,levels = day_order)\n\nLet’s ask a couple questions. What day of the week am I most active on Facebook?\n\n#make sure days of week are in sequential order. Monday first\n\nactivity_times %>% ggplot(aes(doy))+geom_bar()+\n  labs(title='Facebook Activity', x='Weekday',y='Posts')\n\n\n\n\nMonday stands out. I didn’t realize this. Perhaps I come to work Monday morning and catch up with the news which prompts me to post.\nAm I more cranky on different days?\n\n#cleaned_post_words$doy<-factor(cleaned_post_words$doy,levels = day_order)\n\nword_scores_by_weekday<-cleaned_post_words %>% \n  inner_join(get_sentiments('afinn'),by=\"word\") %>% \n  group_by(doy)\n\nword_scores_by_weekday %>%\n  summarise(mood=mean(value)) %>% \n  ggplot(aes(x=doy,y=mood))+geom_col()+labs(x=\"Weekday\",y=\"Mood Score\")\n\n\n\n\nThis is interesting! I am in a relatively good mood on Monday! It’s the middle of the week when I tend to use more negative words. Then I pick up going into the weekend.\nRemember though, these are numbers of small magnitude. Are the variations statistically significant? Let’s compare Tuesday to Sunday and (which have the most extreme differences). First visually then with a t-test to see if the differences are significant. For our hypothesis we assume the the true difference in the average mood on Monday is no different than the average mood on Sunday. Based on the differences we see, can we reject this hypothesis?\n\nsunday_moods<-word_scores_by_weekday %>% \n  filter(doy==\"Sunday\") %>% \n  group_by(doy,date) %>% \n  summarise(mood=mean(value)) %>% \n  select(doy,mood)\n\n`summarise()` has grouped output by 'doy'. You can override using the `.groups`\nargument.\n\ntuesday_moods<-word_scores_by_weekday %>% \n  filter(doy==\"Tuesday\") %>% \n  group_by(doy,date) %>% \n  summarise(mood=mean(value)) %>% \n  select(doy,mood)\n\n`summarise()` has grouped output by 'doy'. You can override using the `.groups`\nargument.\n\nbind_rows(tuesday_moods,sunday_moods) %>% ggplot(aes(mood,fill=doy))+geom_density(alpha=0.7)\n\n\n\n\n\nt.test(tuesday_moods$mood,sunday_moods$mood)\n\n\n    Welch Two Sample t-test\n\ndata:  tuesday_moods$mood and sunday_moods$mood\nt = -0.97824, df = 332.11, p-value = 0.3287\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.6292549  0.2112694\nsample estimates:\n mean of x  mean of y \n0.06088435 0.26987711 \n\n\nRats! It looks like our “interesting” observation is not interesting. The p-value of 0.32 is below 2, so we can’t reject our hypothesis. The difference in mean sentiment for Sunday and Tuesday would have to be beyond the confidence interval to give us acceptable certainty that I am most cranky on Tuesday.\nWe can’t get too excited by the density plot, either. My posts are bi-modally distributed but, given the relatively short length of my posts, chances are I use just one sentiment-loaded word and that skews the distribution. Again, small sample sizes are the problem. Pretty picture, though!\nWhat times am I most active?\n\nhours<-cleaned_post_words %>% mutate(hour=hour(date))\n\nhours %>% ggplot(aes(hour))+geom_bar()+\n  labs(title='Facebook Activity', x='Hour',y='Posts')\n\n\n\n\n#Trends over Time\nIs there any trend to my Facebook activity over time? Let’s bucket the posts by month and look for a pattern.\n\nactivity_times <- activity_times %>% \n  #filter dates before I joined as bad data\n  filter(date>as.Date(\"2008-01-01\")) %>% \n  mutate(month=as.yearmon(date))\n\nactivity_times %>% \n  ggplot(aes(as.Date(month))) + geom_bar() +labs(x=\"Month\",y=\"Posts\")\n\n\n\n\nWhat’s up with the beginning of 2013 and December in 2015? Looking at the raw activity log I see that I briefly let Spotify tell you what I was listening to via Facebook. That generated a lot of activity. I turned it off after a couple weeks. In late 2016 around the election we also see an uptick in activity. Otherwise there have been pretty mild ebbs and flows, averaging about 30 posts per month.\n\nactivity_times %>% \n  group_by(month) %>% \n  summarise(n=n()) %>% \n  summarise(avg_per_month=mean(n)) %>% \n  kable() %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    avg_per_month \n  \n \n\n  \n    29.42105 \n  \n\n\n\n\n\n\n\nDoes my mood change over time?\nWe can repeat the sentiment analysis from above but bucket it by month.\n\nword_scores_by_month<-cleaned_post_words %>% \n  inner_join(get_sentiments('afinn'),by=\"word\") %>% \n  select(date, word,value) %>% \n  mutate(yearmonth=as.yearmon(date)) %>% \n  group_by(yearmonth) %>% summarise(mood=sum(value))\n\nword_scores_by_month %>%  \n  ggplot(aes(x=as.Date(yearmonth),y=mood))+geom_col()+geom_smooth()+labs(x=\"Month\",y=\"Mood Score\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nA trend is not very evident. Month-to-month variation is very high. Is it that we don’t have a good sample size or do my moods swing wildly? The most extreme gyration is around the 2016 presidential election. Optimism followed by despair? Perhaps.\n\n\nPolitics Rears Its Ugly Head\nI try to avoid too much talk about politics on Facebook but, like most of us, it was tough in an election year. This gives us an opportunity to dig into a specific topic within the posting corpus.\nLet’s start by seeing how often I mentioned politicians names.\n\npoliticos=c(\"obama\",\"trump\",\"hillary\",\"clinton\",\"johnson\",\"kasich\",\"bush\",\"sanders\",\"romney\",\"mccain\",\"palin\")\n\ngg<-cleaned_post_words %>% \n  filter(word %in% politicos) %>%\n  mutate(word=str_to_title(word)) %>% \n  count(word, sort = TRUE) %>% \n  \n  ggplot(aes(x=reorder(word,n),y=n))+geom_col()+coord_flip()+labs(y=\"Mentions\",x=\"\")\n\ngg\n\n\n\n\n“Johnson”” is Gary Johnson, in case you forgot. Unlike the the “lamestream media,” I gave much more attention to the Libertarian candidate in my posts. Oddly, that didn’t help his success in the election.\n“Hillary”” is the only first name in the list. Using a woman’s first name when equally familiar men are referred to with their last name is often sexist. In my defense, during the election it was important to distinguish between her and husband Bill so that’s why I used “Hillary.” We are not on a first name basis. On the other hand, “Bill” is a common enough name (and noun) so it’s likely that many posts using it don’t refer to the politician. Just to get an idea let’s look at the words bracketing “bill”:\n\ncleaned_post_words %>% \n  mutate(phrase=paste(lag(word),word,lead(word))) %>% \n  filter(word=='bill') %>% \n  select(date,phrase) %>% \n  kable() %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n \n  \n    date \n    phrase \n  \n \n\n  \n    2017-09-20 08:53:00 \n    morning bill protection \n  \n  \n    2017-07-23 12:27:00 \n    water bill extra \n  \n  \n    2017-03-11 14:15:00 \n    stitch bill doctor \n  \n  \n    2017-03-11 13:51:00 \n    pass bill nancy \n  \n  \n    2017-03-05 10:45:00 \n    front bill maudlin \n  \n  \n    2016-10-18 13:13:00 \n    friends bill fast \n  \n  \n    2016-09-28 21:46:00 \n    johnson bill weld \n  \n  \n    2016-09-28 21:46:00 \n    idealist bill policy \n  \n  \n    2016-07-28 14:28:00 \n    delegates bill clinton \n  \n  \n    2012-08-02 12:59:00 \n    governor bill haslam \n  \n  \n    2012-05-03 18:43:00 \n    jobs bill money \n  \n  \n    2012-05-03 18:43:00 \n    talking bill drummond \n  \n  \n    2012-04-21 17:45:00 \n    xtc bill lot \n  \n  \n    2010-10-20 21:12:00 \n    betty bill bartlett \n  \n  \n    2009-07-02 16:57:00 \n    disco bill nelson \n  \n  \n    2009-03-09 08:36:00 \n    pay bill 20 \n  \n  \n    2009-02-27 18:17:00 \n    kill bill banks \n  \n\n\n\n\n\nIt looks like just one of the mentions of “Bill” is Bill Clinton, so we can ignore him (for this project, anyway).\nWas the uptick we saw above in posting activity around the election due to political activty? We asssume, but let’s look just the posts containing the names of the politcians I identified earlier. This does not include links I shared containing names but did not comment upon, as Facebook won’t tell me what the link was.\n\ncleaned_post_words %>% \n  mutate(month=as.yearmon(date)) %>% \n  filter(word %in% politicos) %>% \n  mutate(word=str_to_title(word)) %>% \n  group_by(month,word) %>% \n  summarize(n=n()) %>% \n  ggplot(aes(as.Date(x=month),y=n,fill=word)) + geom_col() +labs(x=\"Month\",y=\"Mentions\")\n\n`summarise()` has grouped output by 'month'. You can override using the\n`.groups` argument.\n\n\n\n\n\nYup, I guess so. Unlike some of my wonderful Facebook friends I have been able to let go of the emotions around the election, as my posts naming politicians have dropped back to their baseline levels. Naturally, the names have changed!\nCan you tell my political leanings from this data? Well, duh! You could just read the posts but where’s the fun in that? More practically, if we are analyzing big data sets it would be tough to glean the overall sentiment of many people from reading a feasible sample of posts. Let’s try running the sentiment analysis on all of the posts containing the name of a politician to see of anything emerges.\nNote, I often mention more than one name in a single post and this method won’t distinguish which words apply to which name.\n\npol_sentiment<- NULL\npol_sent<- NULL\nfor (pol in politicos) {\n  sent_words<-cleaned_post_words %>% \n    filter(word == pol) %>%\n    select(date) %>% \n    unique() %>% \n    inner_join(cleaned_post_words,by='date') %>%\n    select(word) %>% \n    inner_join(get_sentiments('afinn'),by=\"word\")\n  #did we get anything?\n  if(nrow(sent_words)>0){\n    avg_score<-summarise(sent_words,opinion=mean(value),post_count=n())\n    pol_sentiment<-bind_rows(bind_cols(Politician=str_to_title(pol),avg_score),pol_sentiment)\n    pol_sent<-bind_rows(tibble(Politician=str_to_title(pol),\n                                   opinion=avg_score$opinion,\n                                   post_count=avg_score$post_count,\n                                   sent_words=list(sent_words$value)\n                                   ),\n    pol_sent)\n    \n  }\n\n}\npol_sent[,1:3]\n\n# A tibble: 8 × 3\n  Politician opinion post_count\n  <chr>        <dbl>      <int>\n1 Romney     -1.75            4\n2 Sanders     0.0333         30\n3 Bush       -0.410          39\n4 Johnson     0.226          84\n5 Clinton     0.0345         29\n6 Hillary     0.0108         93\n7 Trump       0.171          82\n8 Obama      -0.112          98\n\n\nFirst off, Palin and McCain don’t appear in this list beacuse I apparently didn’t use any words from the “afinn” lexicon in my posts mentioning them. Second, Romney is only mentioned in four posts so I don’t think we have a valid sample size. Notice that we store the list of sentiment scores for each politician in sent_words. We’ll use that in a minute.\nTaking what’s left, let’s view a chart.\n\npol_sent %>% filter(Politician != \"Romney\") %>% \n  ggplot(aes(x=reorder(Politician,opinion),y=opinion))+geom_col()+\n  coord_flip()+labs(y=\"Sentiment\",x=\"\")\n\n\n\n\nI’m sad to say this is far from how I would rank order my feelings about each of these names. I was a Gary Johnson supporter but, beyond that, this list might as well be random. Sample size is an issue. I am not that prolific a poster and the words I have in common with the sentiment lexicon is fewer still. Also, remember the sentiment ranges run from -5 to +5. We are talking small magnitudes here. Here is the same chart with the maximum allowable range shown.\n\npol_sent %>% filter(Politician != \"Romney\") %>% \n  ggplot(aes(x=reorder(Politician,opinion),y=opinion))+geom_col()+\n  coord_flip()+labs(y=\"Sentiment\",x=\"\")+ylim(c(-5,5))\n\n\n\n\nLet’s subject this to a test of significance. The “null hypothesis” we want to test is that the mean sentiment score expressed for “Hillary” is not statistically different from the “Trump” score.\n\ntrump_opinion<-pol_sent %>% filter(Politician==\"Trump\") %>% pull(sent_words) %>% unlist()\nhillary_opinion<-pol_sent %>% filter(Politician==\"Hillary\") %>% pull(sent_words) %>% unlist()\nt.test(trump_opinion,hillary_opinion)\n\n\n    Welch Two Sample t-test\n\ndata:  trump_opinion and hillary_opinion\nt = 0.46581, df = 170.76, p-value = 0.6419\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5179631  0.8379211\nsample estimates:\n mean of x  mean of y \n0.17073171 0.01075269 \n\n\nLike before, we can’t reject our hypothesis, I liked Donald Trump as much as I liked Hillary Clinton or, rather, you can’t prove a thing!\nThis analysis shows I didn’t really express strong opinions. It was deliberate. During the election things got pretty heated, as you may recall. I have Facebook friends on all sides of the the political divides. Out of respect, I tried very hard not to trash anybody’s candidate and instead tried to accentuate the positive.\n#Bonus Friend Analysis!\nOne more thing. We’ve only looked at the ‘timeline.htm’ file but Facebook’s data dump includes a lot of other stuff including a list of your friends and the date they were added. We can look at a timeline and summary statistics for for this too.\n\nraw_friends<- read_html(paste0(path,\"friends.htm\"),encoding=\"UTC-8\")\n\nHere the <h2> tag separates the labels for the different friend interactions. What are the categories of interactions Facebook gives us?\n\nfriend_nodes <- raw_friends %>% \n  html_nodes(\"h2\") %>% html_text()\n\nfriend_nodes\n\n[1] \"Friends\"                  \"Sent Friend Requests\"    \n[3] \"Received Friend Requests\" \"Deleted Friend Requests\" \n[5] \"Removed Friends\"          \"Friend Peer Group\"       \n\n\nThe actual items are in lists separated by the <ul> tag. Let’s traverse the list, extracting the category, name and date for each. The first and the last lists do not contain relevant info so we’ll take just the middle five. We’ll also rename the “Friends” category to the more descriptive “Friends Added.”\n\nfriend_nodes[1]<-\"Friends Added\"\nlist_nodes <- raw_friends %>% \n  html_nodes(\"ul\") %>% \n  .[2:6]\n\nfriend_table<- NULL\nfor (node in 1:length(list_nodes)){\n  category<-friend_nodes[node]\n  items<- list_nodes[node] %>% \n    html_nodes(\"li\") %>% \n    html_text() %>% \n    tibble(item=.) %>% \n    separate(item,c(\"Name\",\"Date\"),sep=\"\\\\(\",remove=TRUE) %>% \n    mutate(Date=str_replace(Date,\"\\\\)\",\"\"))\n  \n    friend_table<-cbind(Category=category,items,stringsAsFactors=FALSE) %>% \n      bind_rows(friend_table) %>% \n      as_tibble()\n}\n\nHow many Facebook friends do I have? Not many, by Facebook standards. This is, of course, the question that measures our entire self-worth. I’m very discriminating about who I friend . Well, that plus I’m not a very likeable person. Only four people have become so annoying that I unfriended them (both political extremes). There are more that I unfollowed (too many cat videos) but Facebook doesn’t include them in the log for some reason. Facebook also won’t tell me who unfriended me. But I’m sure that no one would do THAT.\n\nfriend_table %>% group_by(Category) %>% summarize(Number=n())\n\n# A tibble: 5 × 2\n  Category                 Number\n  <chr>                     <int>\n1 Deleted Friend Requests      67\n2 Friends Added               167\n3 Received Friend Requests      7\n4 Removed Friends               4\n5 Sent Friend Requests          3\n\n\nOnce again we have a character string for the date which we need to turn into a proper date. The wrinkle here is dates in the current year don’t specify the year in the log. We have to manually add it. The data is at a daily resolution, which is too granular for a clear picture at my level of activity. Let’s make it quarterly.\n\nfriend_table2 <- friend_table %>% \n  mutate(Date=if_else(str_length(Date)<7,paste0(Date,\", \",year(Sys.Date())),Date)) %>%\n  mutate(Date=parse_date(Date,format=\"%b %d, %Y\")) %>% \n  select(Category,Date) %>%\n  mutate(yearquarter=as.yearqtr(Date)) %>%\n  group_by(yearquarter)\ngg<-friend_table2 %>% ggplot(aes(x=as.Date(yearquarter),fill=Category))+ geom_bar(width=70)\ngg<-gg +labs(title=\"Facebook Friending Activity\",y=\"Count\",x=\"Quarterly\")\ngg\n\n\n\n\nNot too surprising. There was a lot of friending happening when I first joined Facebook. Perhaps a bit more curious is the recent renewed uptick in friending. There has been an influx of renewed high school aquaintances.\nSometimes it is useful too look at the balance of opposites. For example, we can see the balance of the number of friendings vs. the number of delete friend requests by assigning a negative number to deletions. There is no simple way to do this with native dplyr functions, though there should be. Base R is actually better at transforming just certain elements in a column based on some condition. Fortunately, I found a super-useful bit of code on Stack Overflow, mutate_cond(), that does exactly what we need.\n\nmutate_cond <- function(.data, condition, ..., envir = parent.frame()) {\n  #change elements of a column based on a condition\n  #https://stackoverflow.com/questions/34096162/dplyr-mutate-replace-on-a-subset-of-rows/34096575#34096575\n  condition <- eval(substitute(condition), .data, envir)\n    condition[is.na(condition)] = FALSE\n    .data[condition, ] <- .data[condition, ] %>% mutate(...)\n    .data\n}\n\n# tabulate sums of categories by quarter\nfriend_table3 <- friend_table %>% \n  mutate(Date=if_else(str_length(Date)<7,paste0(Date,\", \",year(Sys.Date())),Date)) %>%\n  mutate(Date=parse_date(Date,format=\"%b %d, %Y\")) %>% \n  mutate(yearquarter=as.yearqtr(Date)) %>%\n  select(Category,yearquarter) %>%\n  group_by(Category,yearquarter) %>% \n  summarise(count=n())\n\n`summarise()` has grouped output by 'Category'. You can override using the\n`.groups` argument.\n\n#make deleted requests negative\ngg<-friend_table3 %>% \n  mutate_cond(Category==\"Deleted Friend Requests\",count=-count) %>% \n  filter(Category==\"Deleted Friend Requests\" | Category==\"Friends Added\") %>% \n  ggplot(aes(x=as.Date(yearquarter),y=count,fill=Category))+ geom_col() \n\ngg<-gg +labs(title=\"Facebook Friending Activity\",y=\"Count\",x=\"Quarterly\")\ngg\n\n\n\n\nIt seems that adding friends is associated with deleted requests. I’ll surmise that when I show up in a new friend’s network that will spark some friend requests from their network. Some, maybe most, will be from people I don’t actually know and I will reject. There are spikes in rejections because I let them stack up before I notice them.\n\n\nWrapping Up\nWell that was cool. We got to try a lot of things. HTML parsing, date functions, sentiment analysis, text mining and lots of dplyr manipulations. Like a lot of projects, once I got going I thought of many things to try beyond the initial scope. That’s where the fun is when you’re not on deadline and deliverables. Thanks for making it all the way through. Hopefully this gives you some ideas for your own explorations. Now you try!\n#Double Bonus! Making the cool speedometer at the top of this post\nBecause we love cool visualizations, let’s show my mood on a silly gauge. I won’t show the low-level code I used to generate it because it’s basically a copy of what you can find here: http://www.gastonsanchez.com/. Gaston is a visualization guru extrordinaire. This shows how far you can take base R graphics.\nWhat do you think your gauge would look like? If it’s in the red call the suicide prevention hotline.\nThe animation is created using the magick package. I am thrilled that this recent release brings the image magick program inboard to R so we no longer have to run an external program to render animated files like GIFs (which I insist on pronouncing with a hard ‘G’, BTW.)\n\n# create animated mood gif for top of notebook.\nlibrary(magick)\nmy_days_moods<-word_scores_by_weekday %>%\n  summarise(mood=mean(value))\n\n#interpolate to create more points for a smooth animation.\n# the trick is to create a series where the mood stays constant for a number of frames\n# then transitions smoothly to the next mood value. Examine interp_moods to see how.\ninterp_moods<-tibble(doy=unlist(lapply(levels(my_days_moods$doy),rep,10)),\n                         mood_label=round(unlist(lapply(my_days_moods$mood,rep,10)),2),\n                         mood=approx(x=1:14,unlist(lapply(my_days_moods$mood,rep,2)),n=70)$y)\n\n\ninterp_moods$mood_label<- paste(ifelse(interp_moods$mood_label>0,\"+\",\"\"),interp_moods$mood_label)\n\n# I'll spare you the details of the low-level code.\n# see it at http://www.gastonsanchez.com/.\nsource(\"g_speedometer.r\")\n\nimg <- image_graph(600, 400, res = 96)\nfor(n in 1:nrow(interp_moods)){\n  plot_speedometer(label=interp_moods$doy[n],\n                   value=round(interp_moods$mood[n],2),\n                   bottom_label=interp_moods$mood_label[n],\n                   min=-0.5,\n                   max=0.5)\n  text(-0.1,1.0,\"Faceboook Mood-o-Meter\",cex=1.3)\n}\ndev.off()\nimg <- image_background(image_trim(img), 'white')\nanimation <- image_animate(img, fps = 10)\nimage_write(animation, path = \"moods.gif\", format = \"gif\")"
  },
  {
    "objectID": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html",
    "href": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html",
    "title": "Live Fast, Die Young, Stay Pretty?",
    "section": "",
    "text": "Live fast, die young, stay pretty? That’s the stereotype for rockers, or it was. We only need to look at Keith Richards, over 70 and going strong, to find a major counterexample. Do rockers die young? What do they die of? How does that compare to the broader population (in the U.S., anyway). It turns out there are some suprising answers to those questions.\n\n\nAlong the way we’ll learn something about web scraping, html parsing and some ggplot2 tricks. We use the tidyverse dialect throughout, just so you know."
  },
  {
    "objectID": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html#a-note-on-pointers.",
    "href": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html#a-note-on-pointers.",
    "title": "Live Fast, Die Young, Stay Pretty?",
    "section": "\nA note on “pointers.”\n",
    "text": "A note on “pointers.”\n\n\nR and the rvest package have some great functions for converting html <table>s into data frames. rvest is a very powerful package but one thing I learned is that it works with pointers to the data rather than the actual data. C programmers and old geezers like me will be familiar with this. I remember pop and push and stacks and all that stuff from the old days. R generally doesn’t pass values “by reference.” It passes “by value.” That’s why when you modify data in the scope of a function it doesn’t affect the value of the data outside unless you assign it with return <data>. Using pointers in rvest functions means modifications to html data happen without an explicit assigment.\n\n\nConsider a trival example:\n\n#usual R behavior\nmy_function<- function(x){return (x+1)}\ndata=3\n`#this doesn't change data but it would if data was passed by reference\nmy_function(data)\n`# [1] 4\ndata\n`# [1] 3\n`#this does change data in the usual R way\ndata<-my_function(data)\ndata\n`# [1] 4\n\nIf we were passing values “by reference” my_function(data) would change data without the need to assign it back to data. That’s how rvest works.\n\n\nWe use this behavior to combine the tables in the two Wikipedia articles into one html page by extracting the tables in the second wiki article and making them xml siblings of the tables in the first.\n\n\nAlternatively, we could load the two pages, extract the tables separately and combine them later but this is trickier!\n\n#join pre-2010 to post 2010\ndeath_page<-death_page1\ndeath_page_child<-death_page1 %>% xml_children() %>% .[2]\ndeath_page2_child<-death_page2 %>% xml_children() %>% .[2]\n#create one big web page by adding all the first level children from the second\n#page to the first.\n#This modifies death_page by changing the list of pointers associated with it.\nxml_add_sibling(death_page_child,death_page2_child)\nwrite_html(death_page,file=\"death_page.html\")"
  },
  {
    "objectID": "posts/2018-02-26-new-winter-sports-for-new-countries/2018-02-26-new-winter-sports-for-new-countries.html",
    "href": "posts/2018-02-26-new-winter-sports-for-new-countries/2018-02-26-new-winter-sports-for-new-countries.html",
    "title": "New Winter Sports for New Countries",
    "section": "",
    "text": "Norway is a tiny country that punches way above its weight in the Winter Olympic medal count. We are not surprised as those folks are practically born on skis. At the same time, toussle-haired surfer dudes and dudettes from the US seem to be all over the hill when snowboards are involved. Notably, the sports where the US is most visible are sports which arose fairly recently. Is there a pattern here? Let’s do a quick hit to see if we can visualize the dominance of countries, not by event, but by vintage of a sport’s introduction to the games.\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(knitr)"
  },
  {
    "objectID": "posts/2018-02-26-new-winter-sports-for-new-countries/2018-02-26-new-winter-sports-for-new-countries.html#a-digression",
    "href": "posts/2018-02-26-new-winter-sports-for-new-countries/2018-02-26-new-winter-sports-for-new-countries.html#a-digression",
    "title": "New Winter Sports for New Countries",
    "section": "\nA Digression\n",
    "text": "A Digression\n\n\nA best practice with tidy data is to have every observation and every variable in a single data table. Where we want to use the data in a related table we use _join to add the data to the main table. This runs contrary to best practice in the early days of PC databases where “relational” was a big part of data manipulation. The data tables were kept separate and linked by keys. Keys are still how _join works, of course, but we just make one humongous table rather than look up the related fields on the fly. This is faster but uses more memory and/or storage. Back in the day when a couple megabytes of RAM was a lot, we cared about those things, even for small data projects. Now, we use local million-row tables with nary a blink of the eye. You kids don’t know how tough it was!"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "",
    "text": "A hallmark of mayoral administration of NYC Mayor Bill DeBlasio has been free pre-K for all New York families. When the program was initially rolled out there were complaints in some quarters that upper-income neighborhoods were getting more slots.\n\n\nThis is an exploration comparing income to pre-K seats by neighborhoods. It was done mainly to help me practice with the whole workflow of data gathering, document parsing, and data tidying - plus making cool bi-variate choropleth maps! I had to invent a novel method in R to get a good looking bivariate legend onto the chart.\n\n\nThanks to Joshua Stevens for the inspiration and color theory of bi-variate maps (http://www.joshuastevens.net/cartography/make-a-bivariate-choropleth-map/). Thanks to Ari Lamstein for the awesome suite of choropleth packages (http://www.arilamstein.com/).\n\n\nIn my original version I use an outside program, PDFTOTEXT.EXE, to get parseable text out of the PDF documents at the NYC.gov web site. I share the commented out code for this here but skip the step in the notebook to save run time. Instead, I load the raw converted text files to illustrate the parsing.\n\n\nA further complication is to directly grab the income and population data from the census bureau requires an API key. You’ll have to get your own here: . I comment out the relevant lines but instead provide the raw downloaded data sets to illustrate how they get manipulated.\n\n\nNOTE: This analysis was originally done back in 201. The data is from that time. The URLs for city’s directories have changed and so too have the formats. The web scraping routines need to be modified accordingly.\n\n\nYou can find the raw data in CSV format at https://github.com/apsteinmetz/PreK."
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#use-the-acs-package-to-construct-the-queries-for-census-api",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#use-the-acs-package-to-construct-the-queries-for-census-api",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nUse the acs package to construct the queries for census api\n",
    "text": "Use the acs package to construct the queries for census api\n\n# -----------------------------------------------------------------------\n# get census data on children and income\n#census api key\n#see acs package documentation\n#api.key.install('your key here')\n\n# NYC county codes\nnyc_fips = c(36085,36005, 36047, 36061, 36081)\n#get the zips for all nyc counties\ndata(\"zip.regions\")\nnyc_zips<-data.frame(county.fips.numeric=nyc_fips)%>%inner_join(zip.regions)%>%select(region)%>%t\n# make an ACS geo set\nnycgeo<- acs::geo.make(zip.code = nyc_zips)\n\n##Connect to census.gov Requires an API key. You can uncomment the lines below if you have a key. Otherwise skip to the next section to load the raw csv files which were prepared for this notebook.\n\n\n# Household Household income is table 190013, per capita income is 19301\n#income<-acs::acs.fetch(endyear=2011,geography=nycgeo,table.number=\"B19013\")\n# #get relevant data into a data frame format\n#inc<-cbind(acs::geography(income),acs::estimate(income))\n# kidsUnder3<-acs::acs.fetch(endyear=2011,geography=nycgeo,table.number=\"B09001\",keyword = \"Under 3\")\n# kids<-cbind(acs::geography(kidsUnder3),acs::estimate(kidsUnder3))\n# totalPop<-acs.fetch(endyear=2011,geography=nycgeo,table.number=\"B01003\")\n# pop<-cbind(geography(totalPop),estimate(totalPop))\n\n##Alternatively, load from csv files …the data we would have otherwise gotten from census.gov. Comment this chunk out if you fetch the census data directly.\n\n#if we can't connect to census.gov\ninc<-read_csv('data/NYCincome.csv',col_types = \"ccd\")\nkids<-read_csv('data/NYCkids.csv',col_types = \"ccd\")\npop<-read_csv('data/NYCpopulation.csv',col_types = \"ccd\")\n\n##Massage the census data\n\nnames(inc)<-c(\"NAME\",\"zip\",\"HouseholdIncome\")\n#needs some cleanup of dupes. I don't know why\ninc<-distinct(select(inc,zip,HouseholdIncome))\n\n#kids under 3 in 2011 should approximate Pre-K kids in 2015\nnames(kids)<-c(\"NAME\",\"zip\",\"kidsUnder3\")\nkids<-distinct(select(kids,zip,kidsUnder3))\nkids<-kids %>% select(zip,kidsUnder3) %>% distinct() %>% filter(kidsUnder3!=0 | !is.na(kidsUnder3))\n\nnames(pop)<-c(\"NAME\",\"zip\",\"totPop\")\npop<-pop%>%select(zip,totPop)%>%distinct()%>%filter(totPop!=0)\n\ncensus<-pop%>%inner_join(kids)%>%inner_join(inc)%>%mutate(zip=as.character(zip))"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#look-at-some-preliminary-pictures-from-the-census",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#look-at-some-preliminary-pictures-from-the-census",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nLook at some preliminary pictures from the census\n",
    "text": "Look at some preliminary pictures from the census\n\n\nSo now we have some census data. We can use the ‘chorplethr’ package to easily create some meaningful maps. Let’s look at where the kids are and what incomes are in NYC Zip codes. Note that the ‘choroplethr’ package requires the inputs to be in a data frame where the geographic identifier is labeled “region” and the data to be displayed is labeled “value.”\n\n#where are zips with the most rugrats?\nkidsChor <- census %>% \n  transmute(region = zip, value = kidsUnder3 / totPop * 100)\nzip_choropleth(kidsChor, \n               zip_zoom = nyc_zips, \n               title = \"Percentage of Kids Under 3 in 2011\")\n\n\n\nincomeChor <- census %>% \n  transmute(region = zip, \n            value = HouseholdIncome)\nzip_choropleth(incomeChor, \n               zip_zoom = nyc_zips, \n               title = \"Household Income 2011\")\n## Warning in self$bind(): The following regions were missing and are being\n## set to NA: 10174, 10119, 11371, 10110, 10271, 10171, 10177, 10152, 10279,\n## 10115, 11430, 10111, 10112, 10167, 11351, 11359, 11424, 11425, 11451,\n## 10169, 10103, 10311, 10153, 10154, 10199, 10165, 10168, 10278, 10020,\n## 10173, 10170, 10172"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#download-pdfs-from-nyc.gov",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#download-pdfs-from-nyc.gov",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nDownload PDFs from NYC.gov\n",
    "text": "Download PDFs from NYC.gov\n\n\nDownload the PDFs then convert to text using an outside program, PDFTOTEXT.EXE (http://www.foolabs.com/xpdf/home.html).\n\n\n# # -----------------------------------------------------------------------\n# # get NYC data on pre-K programs\n# # scan seat directory pdfs and put into a data frame by zip code\n# #DOE pre-k directories\n# urls<- c(\"http://schools.nyc.gov/NR/rdonlyres/1F829192-ABE8-4BE6-93B5-1A33A6CCC32E/0/2015PreKDirectoryManhattan.pdf\",\n#          \"http://schools.nyc.gov/NR/rdonlyres/5337838E-EBE8-479A-8AB5-616C135A4B3C/0/2015PreKDirectoryBronx.pdf\",\n#          \"http://schools.nyc.gov/NR/rdonlyres/F2D95BF9-553A-4B92-BEAA-785A2D6C0798/0/2015PreKDirectoryBrooklyn.pdf\",\n#          \"http://schools.nyc.gov/NR/rdonlyres/B9B2080A-0121-4C73-AF4A-45CBC3E28CA3/0/2015PreKDirectoryQueens.pdf\",\n#          \"http://schools.nyc.gov/NR/rdonlyres/4DE31FBF-DA0D-4628-B709-F9A7421F7152/0/2015PreKDirectoryStatenIsland.pdf\")\n# \n# #assumes pdftotext.exe is in the current directory.  Edit as necessary\n# exe <- \"pdftotext.exe\"\n# \n# #regex to parse address line\n# pkseattokens <-\"(Address: )([.[:alnum:]- ()]+),+ ([0-9]{5})([a-zA-Z .()-:]+) ([0-9]{1,4}) (FD|HD|AM|PM|5H)\"\n# \n# # each of the PDF directories have 27 pages of intro material. Skip it. This might change for different years. Check PDFs\n# firstPage = 28\n# \n# dests <- tempfile(str_match(urls,\"Directory(\\\\w.+).pdf\")[,2],fileext = \".pdf\")\n# txt<- NULL\n# for (i in 1:length(urls)) {\n#   download.file(urls[i],destfile = dests[i],mode = \"wb\")\n#   # pdftotxt.exe is in current directory and convert pdf to text using \"table\" style at firstpage\n#   result<-system(paste(exe, \"-table -f\", firstPage, dests[i], sep = \" \"), intern=T)\n#   # get txt-file name and open it  \n#   filetxt <- sub(\".pdf\", \".txt\", dests[i])\n#   txt <- append(txt,readLines(filetxt,warn=FALSE))\n# }"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#alternatively-import-and-combine-the-already-converted-text-files.",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#alternatively-import-and-combine-the-already-converted-text-files.",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nAlternatively, import and combine the already converted text files.\n",
    "text": "Alternatively, import and combine the already converted text files.\n\nboroughList <- c('Manhattan','Bronx','Brooklyn','Queens','Staten')\ntxt<-NULL\nfor (borough in  boroughList){\n  # get txt-file name and open it  \n  filetxt <- paste(\"data/\",borough, \".txt\", sep='')\n  txt <- append(txt,readLines(filetxt,warn = FALSE))\n}"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#extract-relevant-info-from-text-files",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#extract-relevant-info-from-text-files",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nExtract relevant info from text files\n",
    "text": "Extract relevant info from text files\n\n\nPull out the Zip, seat count and day length of each school. Note the pretty heroic (for me, anyway) regular expression, “pkseattokens.”\"\n\n# find address line which contains zip and seat count\ntxt2<-txt[grep(\"Address:\",txt)]\n# strip chars that will mess up regex\npkseattokens <-\"(Address: )([.[:alnum:]- ()]+),+ ([0-9]{5})([a-zA-Z .()-:]+) ([0-9]{1,4}) (FD|HD|AM|PM|5H)\"\ntxt2<-sub(\"'\",\"\",txt2)\nschools<-as_data_frame(str_match(txt2,pkseattokens))[,c(4,6,7)]\nnames(schools)<-c(\"zip\",\"seats\",\"dayLength\")\n#have to convert from factor to character THEN to integer.  Don't know why\nschools$seats<-as.integer(as.character(schools$seats))\n\n# aggregate seat count by zip code\nsumSeats <- schools %>% \n  group_by(zip) %>% \n  summarise(count = n(), \n            numSeats = sum(seats, na.rm = TRUE))\n  names(sumSeats)<-c(\"zip\",\"schools\",\"numSeats\")\n\nSo we go from this: \n\n\nthen to this:\n\ntxt[1:3]\n## [1] \"    District 1: Full-Day Pre-K Programs                                                                                      You may apply to these programs online, over the phone, or at a Family Welcome Center.\"\n## [2] \"\"                                                                                                                                                                                                                   \n## [3] \"    Bank Street Head Start (01MATK)                                                                                                            Other School Features         2015   2014 Lowest\"\n\nand then to this:\n\ntxt2[1:3]\n## [1] \"    Address: 535 East 5th Street, 10009 (East Village)                               Phone:    212-353-2532                                    Breakfast/Lunch/Snack(s)      40 FD  N/A\"\n## [2] \"    Address: 280 Rivington Street, 10002 (Lower East Side)                           Phone:    212-254-3070                                    Breakfast/Lunch/Snack(s)      40 FD  N/A\"\n## [3] \"    Address: 180 Suffolk Street, 10002 (Chinatown)                                   Phone:    212-982-6650                                    Breakfast/Lunch/Snack(s)      29 FD  N/A\"\n\n…and finally to this:\n\nschools[1:3,]\n## # A tibble: 3 x 3\n##   zip   seats dayLength\n##   <chr> <int> <chr>    \n## 1 10009    40 FD       \n## 2 10002    40 FD       \n## 3 10002    29 FD\n\nMan, I love when the regex works! Magic!"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#look-at-some-preliminary-pictures-from-the-pre-k-data",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#look-at-some-preliminary-pictures-from-the-pre-k-data",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nLook at some preliminary pictures from the pre-K data\n",
    "text": "Look at some preliminary pictures from the pre-K data\n\n\nNot all the programs are full day. Are there a lot of schools offering shorter programs? We won’t use this data further in our analysis, but lets look at how many seats are full day vs. something else. Full day is the overwhelming majority.\n\n#how do the programs break out in terms of day length?\nsumDayLength<-schools%>%group_by(dayLength)%>%summarise(NumSchools=n(),NumSeats=sum(seats,na.rm=TRUE))\nggplot(sumDayLength,aes(x=dayLength,y=NumSeats)) + geom_col() +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\nWhere are the most schools? Where are the most seats? We might assume this pictures look the same, and they do.\n\n# some preliminary pictures\nsumSeats %>% transmute(region = zip, value = schools) %>%\n  zip_choropleth(zip_zoom = nyc_zips, \n                 title = \"Number of Schools\")\n\n\n\nsumSeats %>% transmute(region=zip,value=numSeats) %>% \n  zip_choropleth(zip_zoom = nyc_zips,\n                 title = \"Number of Pre-K Seats\")\n## Warning in super$initialize(zip.map, user.df): Your data.frame contains the\n## following regions which are not mappable: 11249, 11376, NA\n## Warning in self$bind(): The following regions were missing and are being\n## set to NA: 10464, 11040, 10280, 10174, 10017, 10119, 11371, 10110, 10271,\n## 11003, 11370, 10171, 10069, 10162, 10177, 10152, 10279, 10115, 10005,\n## 10111, 10112, 10167, 11351, 11359, 11424, 11425, 11451, 10006, 10169,\n## 10103, 10311, 10153, 10154, 10199, 10165, 10168, 10278, 10020, 10173,\n## 10170, 10172, 11005"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#create-the-custom-legend.",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#create-the-custom-legend.",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nCreate the custom legend.\n",
    "text": "Create the custom legend.\n\n\nTo create the legend we ‘simply’ create a heat map of the 3x3 bins in the map and label the axes appropriately. Then, using ‘cowplot’, shove it into a corner of the map. There are other ways we could use, but they don’t look nearly as nice.\n\n#first create a legend plot\nlegendGoal = melt(matrix(1:9, nrow = 3))\nlg <- ggplot(legendGoal, aes(Var2, Var1, fill = as.factor(value))) + geom_tile()\nlg <- lg + scale_fill_manual(name = \"\", values = bvColors)\nlg <- lg + theme(legend.position = \"none\")\nlg <- lg + theme(axis.title.x = element_text(size = rel(1), color = bvColors[3])) + \n  xlab(\" More Income -->\")\nlg <- lg + theme(axis.title.y = element_text(size = rel(1), color = bvColors[3])) + \n  ylab(\"   More Seats -->\")\nlg <- lg + theme(axis.text = element_blank())\nlg <- lg + theme(line = element_blank())\nlg\n\n\n\n\nAbove we see the legend as a custom rolled heat map. There is no data in it, just a matrix corresponding to the bin indices in the zip code map. We assign colors to match."
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#put-both-plots-on-a-grid",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#put-both-plots-on-a-grid",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nPut both plots on a grid\n",
    "text": "Put both plots on a grid\n\n\nNow we have the map in the ‘gg’ variable and the legend in the ‘lg’ variable. ‘ggdraw()’ and ‘draw_plot()’ are the ‘cowplot’ functions that let us create the canvas. We tweak the location and size parameters for rendering the legend element until it looks nice inset with the map.\n\n# put the legend together with the map\n# further annotate plot in the ggplot2 environment\n#strip out the ugly legend\ngg<-bvc$render()  + theme(legend.position=\"none\")\nggdraw() + draw_plot(lg,0.2,0.5,width=0.2,height=0.35) + \n  draw_plot(gg)\n\n\n\n\nThis map shows clearly where the low income, well served areas of the city are and that the swanky manhattan zip codes have the fewest free pre-K seats per child."
  },
  {
    "objectID": "posts/2019-02-04-rick-and-morty-palettes/2019-02-04-rick-and-morty-palettes.html",
    "href": "posts/2019-02-04-rick-and-morty-palettes/2019-02-04-rick-and-morty-palettes.html",
    "title": "Rick and Morty Palettes",
    "section": "",
    "text": "This was just a fun morning exercise. Let’s mix multiple images to make a palette of their principal colors using k-means. We’ll also use the totally awesome list-columns concept to put each image’s jpeg data into a data frame of lists that we can map to a function that turns the jpeg data into a list of palette colors in a new data frame.\n\n\nThis more-or-less copies http://www.milanor.net/blog/build-color-palette-from-image-with-paletter/ with the added twist of using multiple images before creating the palette. We’ll also get into the weeds a bit more with dissecting the images. I wanted to see if some cartoon show palettes using this method matched those in the ggsci package. Did the authors use the algorithmic approach I will use here? Will my approach look any better? Don’t know. I decided to use “Rick and Morty” because my kids like it. I would certainly never watch such drivel. I’m a scientist.\n\n\nFor the record, the one pop culture derived palette I really like is the Wes Anderson palette and on CRAN. These are presumably lovingly curated and created, not like the ones created by the stupid robot I use here.\n\n\nThe drawback to using K-means to create palettes from images is that it’s likely that none of the colors created are actually in the image. They just represent the mathematical centers of the clusters of colors.\n\n\nLoad libraries.\n\nlibrary(tidyverse)\nlibrary(jpeg) #import images\nlibrary(scales) #just for for the show_col() function\nlibrary(ggsci) #to compare my palettes to its palettes\nlibrary(ggfortify) #to support kmeans plots\nlibrary(gridExtra) #multiple plots on a page\n\nLoad mulitple images. They are all Google image search thumbnails so the size is the same. This matters since we are combining images. A larger image would have a disproportional weight in our analysis.\n\n\nI first thought that, since I am combining multiple images to get one palette, I needed to tile the images then process. No. We just care about the pixel color values so it really doesn’t matter what position they are in. The most efficient approach is to just chain all the RGB values together. Duh. Still we want to do some work with the individual images so let’s label them.\n\nrm_list<-list()\nfor (n in 1:6){\n  img<-jpeg::readJPEG(paste0(\"img/rm\",n,\".jpg\"))\n  R<-as.vector(img[,,1])\n  G<-as.vector(img[,,2])\n  B<-as.vector(img[,,3])\n  rm_list<-bind_rows(data_frame(img=n,R,G,B),rm_list) %>% \n    arrange(img)\n}\n\nrm_list <- left_join(rm_list,\n                     data_frame(\n                     img = c(1, 2, 3, 4, 5, 6),\n                     name = c(\"Schwifty\",\"Portal\",\"Cable\",\n                     \"Family\", \"Outdoor\", \"Wedding\")\n                     ))\n\n\nShow Me What You Got\n\n\nI chose the images from Google image search to be representative of varying but typical scenes.\n\n\n Cable\n\n\n Family\n\n\n Wedding\n\n\n Outdoor\n\n\n Portal\n\n\n Schwifty\n\n\nFor fun let’s do some density plots of the color values.\n\n#make data tidy first\nrm_tidy <- rm_list %>% gather(\"color\",\"level\",-img,-name)\nggplot(rm_tidy,aes(x=level,fill=color))+\n  geom_density(alpha=0.7) + \n  scale_fill_manual(values=c(\"blue\",\"green\",\"red\")) + \n  theme_void()\n\n\n\n\nWe can see some evidence of bimodality, a preference for very bright and very dark hues. Red is more often cranked to the max, while blue is much more evenly distributed. Perhaps that is typical of the limited palette of cartoons or just a function of the small number of frames I chose.\n\nggplot(rm_tidy,aes(x=level,fill=color))+\n  geom_density(alpha=0.7) + \n  scale_fill_manual(values=c(\"blue\",\"green\",\"red\")) + \n  facet_wrap(~name)+\n  theme_void()\n\n\n\n\nIt’s interesting to compare “Cable” with “Family.” Both images share the same backdrop but “Family” is much darker.\n\n\n\n\nMake the Palettes\n\n\nWhen I was a kid with watercolors I wanted to come up with a name for the filthy color that resulted when I mixed all the colors together. I called it (trigger warning) “Hitler” (but, really, brown). What is the color that results when we average all the RGB values? What named R colors resemble it? It looks to me like it’s between “cornsilk4”\" and “darkkhaki.”\"\n\nblend_color<-rm_list %>% \n  summarise(R=mean(R),G=mean(G),B=mean(B)) %>% \n  rgb()\n\nshow_col(c(\"cornsilk4\",blend_color,\"darkkhaki\"))\n\n\n\n\nLet’s call it “desertkhaki” which, hopefully, is not a trigger word.\n\n\nNow, for the fun part. In the Wes Anderson palette set, each movie get’s a different palette. Let’s make palettes for each of the images, which I chose for their distinctiveness.\n\n\nFor me, the good thing about open source is that I can stand on the shoulders of giants in the community. R also makes very muscular analysis trivally simple. On the other hand, it makes “script kiddies” like me potentially dangerous. I can only describe k-means in the most general terms but can run it in a snap.\n\nnum_colors = 16\npal_schwifty <- rm_list %>% \n  filter(name==\"Schwifty\") %>% \n  select(R,G,B) %>% \n  kmeans(centers = num_colors, iter.max = 30) %>% \n  .$centers %>% \n  rgb()\n\nshow_col(pal_schwifty)\n\n\n\n\nFor data plotting the separation between some of these colors is too small. I think 9 colors will suffice.\n\nnum_colors = 9\npal_schwifty <- rm_list %>% \n  filter(name==\"Schwifty\") %>% \n  select(R,G,B) %>% \n  kmeans(centers = num_colors, iter.max = 30) %>% \n  .$centers %>% \n  as.tibble() %>% \n  {.}\n## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics).\n## This warning is displayed once per session.\nshow_col(rgb(pal_schwifty))\n\n\n\n\nFor plotting purposes I would like use these colors in order of intensity. Sorting colors is a topic in itself but here we’ll do it quick and simple.\n\npal_schwifty %>% \n  mutate(saturation=rowSums(.[1:3])) %>% \n  arrange(saturation) %>% \n  rgb() %>% \n  show_col()\n\n\n\n\nThat’s about right. Let’s put it all together. Go through all the images to create a series of palettes.\n\n\n#function to turn a table of RGB values to an ordered list of colors\ngen_pal <- function(rgb_table) {\n  num_colors = 9\n  pal <- rgb_table %>%\n  select(R, G, B) %>%\n  kmeans(centers = num_colors, iter.max = 30) %>%\n  .$centers %>%\n  as.tibble() %>%\n  mutate(saturation = rowSums(.[1:3])) %>%\n  arrange(saturation) %>%\n  rgb()\n  return(pal)\n}\n#now make list columns, which are totally awesome, for each palette\npalette_rick<-rm_list %>% \n  group_by(name) %>% \n  select(-img) %>% \n  nest(.key=\"rgb\") %>% \n  transmute(name=name,pal= map(rgb,gen_pal))\npalette_rick\n## # A tibble: 6 x 2\n##   name     pal      \n##   <chr>    <list>   \n## 1 Schwifty <chr [9]>\n## 2 Portal   <chr [9]>\n## 3 Cable    <chr [9]>\n## 4 Family   <chr [9]>\n## 5 Outdoor  <chr [9]>\n## 6 Wedding  <chr [9]>\n#a function to extract the individual palettes, given a name.\n\nextract_pal<-function(palette_list,pal_name){\n  pal<-palette_list %>% filter(name==pal_name) %>% \n    select(pal) %>% \n    unlist() %>% \n    as.vector()\n  return(pal)\n}\nplot_one<-function(pal_name){\n  tmp <- palette_rick %>% unnest() %>% filter(name==pal_name)\n  g<- ggplot(tmp,aes(pal,fill=pal)) + geom_bar() + \n  scale_fill_manual(values=tmp$pal,guide=F) +\n  theme_void()+ggtitle(pal_name)\n  return (g)\n  \n}\n\nlapply(palette_rick$name,plot_one) %>% \n  grid.arrange(grobs=.)\n\n\n\n\nFinally, let’s do what we said we’d do at the beginning, put all these images together and add it to our list column of palettes.\n\nmulti_img_pal <- gen_pal(rm_list)\npalette_rick<-data_frame(name=\"all\",pal=list(multi_img_pal)) %>% bind_rows(palette_rick)\nshow_col(multi_img_pal)\n\n\n\n\nNot too bad. I’m glad something resembling Rick’s hair makes it into the list. Compare it to the ggsci package Rick and Morty palette. Here we see the weaknesses of an algorithmic approach. ggsci is more interesting since it has more color diversity and vividness. I assume they were hand selected. You can see Rick’s hair and Morty’s shirt color.\n\nshow_col(ggsci::pal_rickandmorty()(9))\n\n\n\n\nSince the (rather flimsy) point of this excercise is to make palettes for data graphics, let’s make some plots.\n\n#use the example in help for dplyr::gather\nstocks <- data.frame(\n  time = as.Date('2009-01-01') + 0:9,\n  W = rnorm(10, 0, 1),\n  X = rnorm(10, 0, 1),\n  Y = rnorm(10, 0, 2),\n  Z = rnorm(10, 0, 4)\n)\nstocksm <- stocks %>% gather(stock, price, -time)\n\nggplot(stocksm,aes(time,price,color=stock))+geom_line(size=2)+\n  scale_color_manual(values = multi_img_pal) + theme_minimal()\n\n\n\nggplot(stocksm,aes(time,price,color=stock))+geom_line(size=2) +\n  theme_minimal() +\n  scale_color_manual(values = extract_pal(palette_rick,\"Wedding\"))\n\n Arguably, the perceptual differnces among the colors are less than ideal, even if the colors are pleasing. We might take the additional step of hand-selecting colors from a larger generated palette that are more suitable for plots.\n\n\n\n\nOne more thing…\n\n\nBack to the k-means analysis. When we created these palettes we were really assigning colors to the centers of the clusters of near neigbors in the a 2D space. This is a form of principal components analysis (PCA). Let’s visualize those clusters. The ggplot::autoplot() function makes this trivally easy. While we are at it, let’s crank up the number of colors to 20.\n\nnum_colors = 20\n#assign each pixel to a cluster\nkm <-  rm_list[c(\"R\",\"G\",\"B\")] %>% kmeans(centers = num_colors, iter.max = 30)\nrm_PCA<-prcomp(rm_list[c(\"R\",\"G\",\"B\")])\n\nrm_list <- rm_list %>% mutate(cluster=as.factor(km$cluster))\nautoplot(rm_PCA, x=1,y=2,data = rm_list, colour = \"cluster\",\n         loadings = TRUE, loadings.colour = 'blue',\n         loadings.label = TRUE, loadings.label.size = 10) +\n  scale_color_manual(values=rgb(km$centers),guide=FALSE)+\n  theme_classic()\n\n This is every pixel colored by it’s cluster assignment and plotted. It’s clear that the x-dimension, which happens to explain 74% of the color variance, is luminosity, with darker shades on the right. The other dimension seems to be related to hue.\n\n\nWe can make it clear by plotting the second and third principal component.\n\nrm_list <- rm_list %>% mutate(cluster=as.factor(km$cluster))\nautoplot(rm_PCA, x=2,y=3,data = rm_list, colour = \"cluster\",\n         loadings = TRUE, loadings.colour = 'blue',\n         loadings.label = TRUE, loadings.label.size = 10) +\n  scale_color_manual(values=rgb(km$centers),guide=F)+\n  theme_classic()\n\n\n\n\nNow it’s quite clear that the second and third principal components map to the color space even though this explains only about 25% of the variation in the data.\n\n\nFeel free to get schwifty with these palettes!"
  },
  {
    "objectID": "posts/2023-01-24_Switching-to-Quarto/2023-1-24_switching-to-quarto.html",
    "href": "posts/2023-01-24_Switching-to-Quarto/2023-1-24_switching-to-quarto.html",
    "title": "Switching to Quarto from Blogdown",
    "section": "",
    "text": "Create subfolder in our ‘posts’ folder to hold converted file. Use any name but I like the form yyyy-mm-dd_name-of-post.\nCreate subfolder below the one just created called ‘img’.\nCopy any image files from the old version over to the ‘img’ directory we just made.\nCopy old rendered HTML document (not the RMD markdown document) to the new folder.\nRename copied HTML file from <filename>.html to <filename>.qmd.\nLoad QMD file into RStudio.\nSearch/Replace all instances of `class=“r”` to `class=“sourceCode r”`.\nFix path names of any image files to point to the ‘img’ folder.\nSave and Render. Done!"
  }
]